{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "import gensim.downloader as api\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as f \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.downloader import load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Dataset Generation (5 points)\n",
    "We will use the Amazon reviews dataset used in HW1. Load the dataset\n",
    "and build a balanced dataset of 250K reviews along with their ratings (50K\n",
    "instances per each rating score) through random selection. Create ternary\n",
    "labels using the ratings. We assume that ratings more than 3 denote positive\n",
    "1\n",
    "sentiment (class 1) and rating less than 3 denote negative sentiment (class\n",
    "2). Reviews with rating 3 are considered to have neutral sentiment (class 3).\n",
    "You can store your dataset after generation and reuse it to reduce the computational load. For your experiments consider a 80%/20% training/testing\n",
    "split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review_body', 'star_rating'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/amazon_reviews_us_Office_Products_v1_00.tsv\", sep='\\t', on_bad_lines='skip')#, usecols=['review_body','star_rating']) #lineterminator='\\r'\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "df = df[['review_body', 'star_rating']]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess/Cleaning take 15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to punkt...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "contraction_mapping = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "\n",
    "pattern_contractions = re.compile('(%s)' % '|'.join(contraction_mapping.keys()))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('stopwords', 'punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_map=contraction_mapping):\n",
    "    return pattern_contractions.sub(lambda occurrence: contraction_map[occurrence.group(0)], text)\n",
    "\n",
    "\n",
    "def rem_stopwords(review,stp):\n",
    "    words = review.split()\n",
    "    filtered_words = [word for word in words if word not in stp]\n",
    "    filtered_sentence = ' '.join(filtered_words)\n",
    "    return filtered_sentence\n",
    "\n",
    "\n",
    "def lemmazation(review):\n",
    "    words = review.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    lemmatized_review = ' '.join(lemmatized_words)\n",
    "    return lemmatized_review\n",
    "\n",
    "\n",
    "def clean_preproc_reviews(reviews, stp):\n",
    "    ### CLEANING\n",
    "    reviews = reviews.str.lower()\n",
    "    # reviews = reviews.apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "    # reviews = reviews.replace(r'http\\S+', '', regex=True)\n",
    "    # reviews = reviews.replace(\"[^a-zA-Z]\", \" \", regex=True)\n",
    "    # reviews = reviews.replace('\\s+', ' ', regex=True).str.strip()\n",
    "    # reviews = reviews.apply(lambda x: expand_contractions(x))\n",
    "\n",
    "    # ### PREPROCESSING\n",
    "    # reviews = reviews.apply(lambda x : rem_stopwords(x, stp))\n",
    "    # reviews = reviews.apply(lemmazation)\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Clean the reviews\n",
    "df['review_body'] =df['review_body'].astype(str)\n",
    "df.dropna(subset=['review_body'], inplace=True)\n",
    "df['review_body'] = clean_preproc_reviews(df['review_body'], stop_words)\n",
    "df.dropna(subset=['review_body'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['star_rating'].apply(lambda x: 0 if x in [4, 5] else (1 if x in [1, 2] else 2))\n",
    "\n",
    "star_ratings = [5, 4, 3, 2, 1]\n",
    "samples = [ df[df['star_rating'] == rating].sample(n = 50000, random_state = 42) for rating in star_ratings]\n",
    "merged_dataset = pd.concat(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample review and label after creating merged dataset:\n",
      "                                               review_body star_rating  label\n",
      "1333654  i had this for about eight months when it fail...           2      1\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample review and label after creating merged dataset:\")\n",
    "print(merged_dataset[['review_body', 'star_rating', 'label']].sample(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Word Embedding (30 points)\n",
    "In this part the of the assignment, you will learn how to generate two sets\n",
    "of Word2Vec features for the dataset you generated. You can use Gensim\n",
    "library for this purpose. A helpful tutorial is available in the following link:\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.\n",
    "html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) (10 points)\n",
    "Load the pretrained “word2vec-google-news-300” Word2Vec model and learn\n",
    "how to extract word embeddings for your dataset. Try to check semantic\n",
    "similarities of the generated vectors using two examples of your own, e.g.,\n",
    "King − M an + W oman = Queen or excellent ∼ outstanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wv['buger'] - wv['fries'] + wv['fish'] ?= wv['chips']\n",
    "# test_relationship = wv['burger'] - wv['fries'] + wv['fish']\n",
    "# print(test_relationship, wv['chips']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman = queen\n",
      "Similarity between 'excellent' and 'outstanding': 0.5567486\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "result = pretrained_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(\"King - Man + Woman =\", result[0][0])\n",
    "\n",
    "similarity = pretrained_model.similarity('excellent', 'outstanding')\n",
    "print(\"Similarity between 'excellent' and 'outstanding':\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman = king\n",
      "Similarity between 'excellent' and 'outstanding': 0.556748628616333\n"
     ]
    }
   ],
   "source": [
    "# Function to find the most similar word\n",
    "def most_similar_vector(vector):\n",
    "    return pretrained_model.similar_by_vector(vector, topn=1)[0][0]\n",
    "\n",
    "# Vector arithmetic: \"King - Man + Woman\"\n",
    "result_vector = pretrained_model['king'] - pretrained_model['man'] + pretrained_model['woman']\n",
    "analogy_result = most_similar_vector(result_vector)\n",
    "print(f\"King - Man + Woman = {analogy_result}\")\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Compute similarity between 'excellent' and 'outstanding'\n",
    "similarity_score = cosine_similarity(pretrained_model['excellent'], pretrained_model['outstanding'])\n",
    "print(f\"Similarity between 'excellent' and 'outstanding': {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) (20 points)\n",
    "Train a Word2Vec model using your own dataset. Set the embedding size\n",
    "to be 300 and the window size to be 11. You can also consider a minimum\n",
    "word count of 10. Check the semantic similarities for the same two examples\n",
    "in part (a). What do you conclude from comparing vectors generated by\n",
    "yourself and the pretrained model? Which of the Word2Vec models seems\n",
    "to encode semantic similarities between words better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    def __init__(self, df, col):\n",
    "        self.df = df\n",
    "        self.col = col\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in self.df[self.col]:\n",
    "            yield utils.simple_preprocess(line)\n",
    "\n",
    "\n",
    "sentences = MyCorpus(merged_dataset, 'review_body')\n",
    "my_model = gensim.models.Word2Vec(sentences=sentences, vector_size=300, window=11, min_count=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman = jumbo\n",
      "Similarity between 'excellent' and 'outstanding': 0.8390566\n",
      "KeyedVectors<vector_size=300, 16476 keys>\n"
     ]
    }
   ],
   "source": [
    "### NEW Dataframe\n",
    "\n",
    "word_vectors = my_model.wv\n",
    "\n",
    "result = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(\"King - Man + Woman =\", result[0][0])\n",
    "similarity = word_vectors.similarity('excellent', 'outstanding')\n",
    "print(\"Similarity between 'excellent' and 'outstanding':\", similarity)\n",
    "print( my_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(word2vec_model, doc):\n",
    "    # Filter out words in the document that are not in the word2vec model's vocabulary.\n",
    "    doc = [word for word in doc if word in word2vec_model.key_to_index]\n",
    "\n",
    "    # If the document contains no words present in the model's vocabulary, return a zero vector.\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "        \n",
    "    # Otherwise, return the average of the word vectors for the words in the document.\n",
    "    return np.mean(word2vec_model[doc], axis=0)\n",
    "\n",
    "merged_dataset['processed_text'] = merged_dataset['review_body'].apply(gensim.utils.simple_preprocess)\n",
    "merged_dataset['pretrained_vector'] = merged_dataset['processed_text'].apply(lambda doc: document_vector(pretrained_model, doc))\n",
    "merged_dataset['doc_vector'] = merged_dataset['processed_text'].apply(lambda doc: document_vector(my_model.wv, doc))\n",
    "\n",
    "Y = merged_dataset['label']\n",
    "X = merged_dataset['doc_vector']\n",
    "X_pre = merged_dataset['pretrained_vector']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Simple models (20 points)\n",
    "Using the Word2Vec features that you can generate using the two models\n",
    "you prepared in the Word Embedding section, train a perceptron and an\n",
    "SVM model similar to HW1 for class 1 and class 2 (binary models). For this\n",
    "purpose, you can just use the average Word2Vec vectors for each review as\n",
    "the input feature (x = 1N PNi=1 Wi for a review with N words). To improve 2\n",
    "your performance, use the data cleaning and preprocessing steps of HW1\n",
    "to include only important words from each review when you compute the\n",
    "average x = 1 N PN i=1 Wi.\n",
    "Report your accuracy values on the testing split for\n",
    "these models for each feature type along with values you reported in your\n",
    "HW1 submission, i.e., for each of perceptron and SVM, you need to report\n",
    "three accuracy values for “word2vec-google-news-300”, your own Word2Vec,\n",
    "and TF-IDF features.\n",
    "What do you conclude from comparing performances for the models\n",
    "trained using the three different feature types (TF-IDF, pretrained Word2Vec,\n",
    "your trained Word2Vec)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Accuracy: 0.6006, Precision: 0.6505, Recall: 0.6006, F1-Score: 0.6172\n",
      " Testing: Accuracy: 0.5998, Precision: 0.6509, Recall: 0.5998, F1-Score: 0.6167\n"
     ]
    }
   ],
   "source": [
    "#### PERCEPTRON \n",
    "### USE AVERAGE WORD VECTOR\n",
    "\n",
    "def evaulate(y_label, y_predicted):\n",
    "    accuracy = accuracy_score(y_label, y_predicted)\n",
    "    precision = precision_score(y_label, y_predicted, average='weighted')\n",
    "    recall = recall_score(y_label, y_predicted, average='weighted')\n",
    "    f1 = f1_score(y_label, y_predicted, average='weighted')\n",
    "    return accuracy, precision, recall,f1\n",
    "\n",
    "# def evaluate(y_label, y_predicted):\n",
    "#     precision = precision_score(y_label, y_predicted, average=None)\n",
    "#     cm = confusion_matrix(y_label, y_predicted)\n",
    "#     class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "#     return class_accuracy, precision\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = np.vstack(X_train), np.vstack(X_test), np.vstack(y_train), np.vstack(y_test)\n",
    "\n",
    "clf = Perceptron(tol=1e-5, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "tr_acc, tr_prec, tr_rec, tr_f1 = evaulate(y_train, y_pred_train)\n",
    "te_acc, te_prec, te_rec, te_f1 = evaulate(y_test, y_pred_test)\n",
    "\n",
    "print(\"Training: Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-Score: {:.4f}\".format(tr_acc, tr_prec, tr_rec, tr_f1))\n",
    "print(\" Testing: Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-Score: {:.4f}\".format(te_acc, te_prec, te_rec, te_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Accuracy: 0.5898, Precision: 0.6162, Recall: 0.5898, F1-Score: 0.5344\n",
      " Testing: Accuracy: 0.5861, Precision: 0.6103, Recall: 0.5861, F1-Score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pre, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = np.vstack(X_train), np.vstack(X_test), np.vstack(y_train), np.vstack(y_test)\n",
    "\n",
    "clf = Perceptron(tol=1e-5, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "tr_acc, tr_prec, tr_rec, tr_f1 = evaulate(y_train, y_pred_train)\n",
    "te_acc, te_prec, te_rec, te_f1 = evaulate(y_test, y_pred_test)\n",
    "\n",
    "print(\"Training: Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-Score: {:.4f}\".format(tr_acc, tr_prec, tr_rec, tr_f1))\n",
    "print(\" Testing: Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-Score: {:.4f}\".format(te_acc, te_prec, te_rec, te_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Feedforward Neural Networks (25 points)\n",
    "Using the features that you can generate using the models you prepared in\n",
    "the Word “Embedding section”, train a feedforward multilayer perceptron\n",
    "network for sentiment analysis classification. Consider a network with two\n",
    "hidden layers, each with 50 and 10 nodes, respectively. You can use cross\n",
    "entropy loss and your own choice for other hyperparamters, e.g., nonlinearity,\n",
    "number of epochs, etc. Part of getting good results is to select good values\n",
    "for these hyperparamters.\n",
    "You can also refer to the following tutorial to familiarize yourself:\n",
    "https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
    "Although the above tutorial is for image data but the concept of training\n",
    "an MLP is very similar to what we want to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) (10 points)\n",
    "To generate the input features, use the average Word2Vec vectors similar to\n",
    "the “Simple models” section and train the neural network. Train a network\n",
    "for binary classification using class 1 and class 2 and also a ternary model for\n",
    "the three classes. Report accuracy values on the testing split for your MLP\n",
    "model for each of the binary and ternary classification cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TERNARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module): \n",
    "    def __init__(self, n_classes):\n",
    "        super(Net, self).__init__()\n",
    "        n_dim = 300\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.fc1 = nn.Linear(n_dim, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, n_classes)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = f.gelu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = f.gelu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = f.softmax(self.fc3(x))\n",
    "        return x \n",
    "\n",
    "\n",
    "ternary_model = Net(n_classes=3)\n",
    "binary_model = Net(n_classes=2)\n",
    "print(ternary_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features and labels before creating datasets:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures and labels before creating datasets:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):  \n\u001b[1;32m----> 3\u001b[0m     vector_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mX_train_tensor\u001b[49m[i], p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# L2 norm \u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#vector_summary = \", \".join(f\"{x:.2f}\" for x in X_train_tensor[i][:3]) + \"...\"  # Summarize vector \u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Norm = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvector_norm\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train_tensor[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Features and labels before creating datasets:\")\n",
    "for i in range(5):  \n",
    "    vector_norm = torch.norm(X_train_tensor[i], p=2)  # L2 norm \n",
    "    #vector_summary = \", \".join(f\"{x:.2f}\" for x in X_train_tensor[i][:3]) + \"...\"  # Summarize vector \n",
    "    print(f\"Sample {i+1}: Norm = {vector_norm:.2f}, Label: {y_train_tensor[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TERNARY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.890880 \tTraining Accuracy: 64.87% \tValidation Loss: 0.872804 \tValidation Accuracy: 66.46%\n",
      "Validation loss decreased (inf --> 0.872804).  Saving model ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m correct_valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     26\u001b[0m ternary_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 27\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mternary_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\envs\\pytorch_dl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\envs\\pytorch_dl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\envs\\pytorch_dl\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\envs\\pytorch_dl\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\envs\\pytorch_dl\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\envs\\pytorch_dl\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\envs\\pytorch_dl\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TextDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TextDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64 \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ternary_model.parameters(), lr=0.01)\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "n_epochs = 50 \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    correct_train = 0\n",
    "    correct_valid = 0\n",
    "    \n",
    "    ternary_model.train()\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = ternary_model(data)\n",
    "        target = target.squeeze()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        correct_train += (predicted == target).sum().item()\n",
    "\n",
    "    ternary_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = ternary_model(data)\n",
    "            target = target.squeeze()\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct_valid += (predicted == target).sum().item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    valid_loss = valid_loss / len(test_loader.dataset)\n",
    "    \n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    valid_accuracy = correct_valid / len(test_loader.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Accuracy: {:.2f}% \\tValidation Loss: {:.6f} \\tValidation Accuracy: {:.2f}%'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        train_accuracy * 100,\n",
    "        valid_loss,\n",
    "        valid_accuracy * 100\n",
    "    ))\n",
    "\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(ternary_model.state_dict(), 'ternary_model.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BINARY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset['binary_label'] = merged_dataset['label'].apply(lambda x: 1 if x == 2 else (0 if x == 0 else 1))\n",
    "merged_dataset['binary_label'].sample(100)\n",
    "\n",
    "X_binary = merged_dataset['doc_vector'].values\n",
    "Y_binary = merged_dataset['binary_label'].values\n",
    "print(X_binary.shape, type(X_binary) )\n",
    "\n",
    "x_bin_train, x_bin_test, y_bin_train, y_bin_test = train_test_split(X_binary, Y_binary, test_size=0.2)\n",
    "\n",
    "print(x_bin_train.shape, type(x_bin_train))\n",
    "\n",
    "x_bin_train = np.array(x_bin_train.tolist(), dtype=np.float32)  # Convert to float32 numpy array if not already\n",
    "y_bin_train = np.array(y_bin_train)\n",
    "\n",
    "x_bin_test = np.array(x_bin_test.tolist(), dtype=np.float32)  # Convert to float32 numpy array if not already\n",
    "y_bin_test = np.array(y_bin_test)\n",
    "\n",
    "# Now convert to torch tensors\n",
    "x_bin_train = torch.tensor(x_bin_train, dtype=torch.float32)\n",
    "y_bin_train = torch.tensor(y_bin_train, dtype=torch.long)\n",
    "\n",
    "x_bin_test = torch.tensor(x_bin_test, dtype=torch.float32)\n",
    "y_bin_test = torch.tensor(y_bin_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_dataset_bin = TextDataset(x_bin_train, y_bin_train)\n",
    "test_dataset_bin = TextDataset(x_bin_test, y_bin_test)\n",
    "\n",
    "batch_size = 64 \n",
    "train_loader_bin = DataLoader(train_dataset_bin, batch_size=batch_size, shuffle=True)\n",
    "test_loader_bin = DataLoader(test_dataset_bin, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "criterion_bin = nn.CrossEntropyLoss()\n",
    "optimizer_bin = torch.optim.Adam(binary_model.parameters(), lr=0.01)\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "n_epochs = 50 \n",
    "\n",
    "# TRAIN LOOP\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    correct_train = 0\n",
    "    correct_valid = 0\n",
    "    \n",
    "    binary_model.train()\n",
    "    for data, target in train_loader_bin:\n",
    "        optimizer_bin.zero_grad()\n",
    "        output = binary_model(data)\n",
    "        target = target.squeeze()\n",
    "        loss = criterion_bin(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_bin.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        correct_train += (predicted == target).sum().item()\n",
    "\n",
    "    binary_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader_bin:\n",
    "            output = binary_model(data)\n",
    "            target = target.squeeze()\n",
    "            loss = criterion_bin(output, target)\n",
    "            valid_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct_valid += (predicted == target).sum().item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    valid_loss = valid_loss / len(test_loader.dataset)\n",
    "    \n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    valid_accuracy = correct_valid / len(test_loader.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Accuracy: {:.2f}% \\tValidation Loss: {:.6f} \\tValidation Accuracy: {:.2f}%'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        train_accuracy * 100,\n",
    "        valid_loss,\n",
    "        valid_accuracy * 100\n",
    "    ))\n",
    "\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(binary_model.state_dict(), 'binary_model.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) (15 points)\n",
    "To generate the input features, concatenate the first 10 Word2Vec vectors\n",
    "for each review as the input feature (x = [WT\n",
    "1, ..., WT [10]) and train the neural 3\n",
    "network. Report the accuracy value on the testing split for your MLP model\n",
    "for each of the binary and ternary classification cases.\n",
    "What do you conclude by comparing accuracy values you obtain with\n",
    "those obtained in the “’Simple Models” section (note you can compare the\n",
    "accuracy values for binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['star_rating'].apply(lambda x: 0 if x in [4, 5] else (1 if x in [1, 2] else 2))\n",
    "merged_dataset['label'] = merged_dataset['label'].apply(lambda x: 1 if x is 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Convolutional Neural Networks (20 points)\n",
    "Using the vectors you prepared in the “Word Embedding” section, train a\n",
    "convolutional neural network (CNN) for sentiment analysis classification.\n",
    "Train a simple CNN for sentiment analysis. You can consider an two-layer\n",
    "CNN with the output channel sizes of 50 and 10. To feed your data into the\n",
    "CNN, limit the maximum review length to 50 by truncating longer reviews\n",
    "and padding shorter reviews with a null value (0). You can use cross entropy\n",
    "loss and your own choice for other hyperparamters, e.g., nonlinearity, number\n",
    "of epochs, etc. Train the CNN network for binary classification using class 1\n",
    "and class 2 and also a ternary model for the three classes. Report accuracy\n",
    "values on the testing split for your CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,) <class 'numpy.ndarray'>\n",
      "(200000,) <class 'numpy.ndarray'>\n",
      "Epoch: 1 \tTraining Loss: 0.512096 \tTraining Accuracy: 78.83% \tValidation Loss: 0.493364 \tValidation Accuracy: 80.53%\n",
      "Validation loss decreased (inf --> 0.493364).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.503597 \tTraining Accuracy: 79.71% \tValidation Loss: 0.490508 \tValidation Accuracy: 80.76%\n",
      "Validation loss decreased (0.493364 --> 0.490508).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.499485 \tTraining Accuracy: 80.18% \tValidation Loss: 0.494315 \tValidation Accuracy: 80.74%\n",
      "Epoch: 4 \tTraining Loss: 0.502700 \tTraining Accuracy: 79.89% \tValidation Loss: 0.486292 \tValidation Accuracy: 81.54%\n",
      "Validation loss decreased (0.490508 --> 0.486292).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.501991 \tTraining Accuracy: 79.77% \tValidation Loss: 0.488278 \tValidation Accuracy: 81.60%\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'label' column to a binary format\n",
    "merged_dataset['binary_label'] = merged_dataset['label'].apply(lambda x: 1 if x == 2 else (0 if x == 0 else 1))\n",
    "merged_dataset['binary_label'].sample(100)\n",
    "\n",
    "# Extracting features and labels for binary classification\n",
    "X_binary = merged_dataset['doc_vector'].values\n",
    "Y_binary = merged_dataset['binary_label'].values\n",
    "print(X_binary.shape, type(X_binary))\n",
    "\n",
    "# Splitting the dataset into training and testing sets for binary classification\n",
    "x_bin_train, x_bin_test, y_bin_train, y_bin_test = train_test_split(X_binary, Y_binary, test_size=0.2)\n",
    "print(x_bin_train.shape, type(x_bin_train))\n",
    "\n",
    "# Converting lists to numpy arrays of type float32\n",
    "x_bin_train = np.array(x_bin_train.tolist(), dtype=np.float32)\n",
    "y_bin_train = np.array(y_bin_train)\n",
    "\n",
    "x_bin_test = np.array(x_bin_test.tolist(), dtype=np.float32)\n",
    "y_bin_test = np.array(y_bin_test)\n",
    "\n",
    "# Converting numpy arrays to PyTorch tensors\n",
    "x_bin_train = torch.tensor(x_bin_train, dtype=torch.float32)\n",
    "y_bin_train = torch.tensor(y_bin_train, dtype=torch.long)\n",
    "\n",
    "x_bin_test = torch.tensor(x_bin_test, dtype=torch.float32)\n",
    "y_bin_test = torch.tensor(y_bin_test, dtype=torch.long)\n",
    "\n",
    "# Creating datasets and dataloaders for binary classification\n",
    "train_dataset_bin = TextDataset(x_bin_train, y_bin_train)\n",
    "test_dataset_bin = TextDataset(x_bin_test, y_bin_test)\n",
    "\n",
    "batch_size_bin = 64\n",
    "train_loader_bin = DataLoader(train_dataset_bin, batch_size=batch_size_bin, shuffle=True)\n",
    "test_loader_bin = DataLoader(test_dataset_bin, batch_size=batch_size_bin, shuffle=False)\n",
    "\n",
    "# Setting up the loss function and optimizer for binary classification\n",
    "criterion_bin = nn.CrossEntropyLoss()\n",
    "optimizer_bin = torch.optim.Adam(binary_model.parameters(), lr=0.01)\n",
    "valid_loss_min_bin = np.Inf\n",
    "\n",
    "n_epochs_bin = 5\n",
    "\n",
    "# Training loop for binary classification\n",
    "for epoch in range(n_epochs_bin):\n",
    "    train_loss_bin = 0.0\n",
    "    valid_loss_bin = 0.0\n",
    "    correct_train_bin = 0\n",
    "    correct_valid_bin = 0\n",
    "    \n",
    "    binary_model.train()\n",
    "    for data, target in train_loader_bin:\n",
    "        optimizer_bin.zero_grad()\n",
    "        output = binary_model(data)\n",
    "        target = target.squeeze()\n",
    "        loss = criterion_bin(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_bin.step()\n",
    "        train_loss_bin += loss.item() * data.size(0)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        correct_train_bin += (predicted == target).sum().item()\n",
    "\n",
    "    binary_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader_bin:\n",
    "            output = binary_model(data)\n",
    "            target = target.squeeze()\n",
    "            loss = criterion_bin(output, target)\n",
    "            valid_loss_bin += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct_valid_bin += (predicted == target).sum().item()\n",
    "\n",
    "    train_loss_bin = train_loss_bin / len(train_loader_bin.dataset)\n",
    "    valid_loss_bin = valid_loss_bin / len(test_loader_bin.dataset)\n",
    "    \n",
    "    train_accuracy_bin = correct_train_bin / len(train_loader_bin.dataset)\n",
    "    valid_accuracy_bin = correct_valid_bin / len(test_loader_bin.dataset)\n",
    "\n",
    "    print(f'Epoch: {epoch+1} \\tTraining Loss: {train_loss_bin:.6f} \\tTraining Accuracy: {train_accuracy_bin * 100:.2f}% \\tValidation Loss: {valid_loss_bin:.6f} \\tValidation Accuracy: {valid_accuracy_bin * 100:.2f}%')\n",
    "\n",
    "    if valid_loss_bin <= valid_loss_min_bin:\n",
    "        print(f'Validation loss decreased ({valid_loss_min_bin:.6f} --> {valid_loss_bin:.6f}).  Saving model ...')\n",
    "        torch.save(binary_model.state_dict(), 'binary_model.pt')\n",
    "        valid_loss_min_bin = valid_loss_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.877268 \tTraining Accuracy: 66.05% \tValidation Loss: 0.861012 \tValidation Accuracy: 67.76%\n",
      "Validation loss decreased (inf --> 0.861012).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.875181 \tTraining Accuracy: 66.32% \tValidation Loss: 0.864552 \tValidation Accuracy: 67.54%\n",
      "Epoch: 3 \tTraining Loss: 0.871953 \tTraining Accuracy: 66.57% \tValidation Loss: 0.862998 \tValidation Accuracy: 67.82%\n",
      "Epoch: 4 \tTraining Loss: 0.871334 \tTraining Accuracy: 66.75% \tValidation Loss: 0.858151 \tValidation Accuracy: 67.94%\n",
      "Validation loss decreased (0.861012 --> 0.858151).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.869552 \tTraining Accuracy: 66.95% \tValidation Loss: 0.853986 \tValidation Accuracy: 68.42%\n",
      "Validation loss decreased (0.858151 --> 0.853986).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.868843 \tTraining Accuracy: 67.02% \tValidation Loss: 0.862916 \tValidation Accuracy: 67.73%\n",
      "Epoch: 7 \tTraining Loss: 0.868962 \tTraining Accuracy: 66.98% \tValidation Loss: 0.867549 \tValidation Accuracy: 67.59%\n",
      "Epoch: 8 \tTraining Loss: 0.868220 \tTraining Accuracy: 67.08% \tValidation Loss: 0.852217 \tValidation Accuracy: 68.85%\n",
      "Validation loss decreased (0.853986 --> 0.852217).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.866993 \tTraining Accuracy: 67.17% \tValidation Loss: 0.863106 \tValidation Accuracy: 68.02%\n",
      "Epoch: 10 \tTraining Loss: 0.867457 \tTraining Accuracy: 67.13% \tValidation Loss: 0.861191 \tValidation Accuracy: 68.21%\n",
      "Epoch: 11 \tTraining Loss: 0.867062 \tTraining Accuracy: 67.06% \tValidation Loss: 0.855119 \tValidation Accuracy: 68.47%\n",
      "Epoch: 12 \tTraining Loss: 0.867102 \tTraining Accuracy: 67.23% \tValidation Loss: 0.856580 \tValidation Accuracy: 68.34%\n",
      "Epoch: 13 \tTraining Loss: 0.866337 \tTraining Accuracy: 67.12% \tValidation Loss: 0.854625 \tValidation Accuracy: 68.31%\n",
      "Epoch: 14 \tTraining Loss: 0.866362 \tTraining Accuracy: 67.25% \tValidation Loss: 0.857807 \tValidation Accuracy: 68.07%\n",
      "Epoch: 15 \tTraining Loss: 0.865698 \tTraining Accuracy: 67.32% \tValidation Loss: 0.853470 \tValidation Accuracy: 68.46%\n",
      "Epoch: 16 \tTraining Loss: 0.866958 \tTraining Accuracy: 67.13% \tValidation Loss: 0.883647 \tValidation Accuracy: 66.20%\n",
      "Epoch: 17 \tTraining Loss: 0.867467 \tTraining Accuracy: 67.09% \tValidation Loss: 0.857215 \tValidation Accuracy: 68.42%\n",
      "Epoch: 18 \tTraining Loss: 0.866294 \tTraining Accuracy: 67.19% \tValidation Loss: 0.857831 \tValidation Accuracy: 68.57%\n",
      "Epoch: 19 \tTraining Loss: 0.865783 \tTraining Accuracy: 67.24% \tValidation Loss: 0.855976 \tValidation Accuracy: 67.93%\n",
      "Epoch: 20 \tTraining Loss: 0.864897 \tTraining Accuracy: 67.39% \tValidation Loss: 0.856354 \tValidation Accuracy: 68.46%\n",
      "Epoch: 21 \tTraining Loss: 0.865995 \tTraining Accuracy: 67.26% \tValidation Loss: 0.852919 \tValidation Accuracy: 68.91%\n",
      "Epoch: 22 \tTraining Loss: 0.865502 \tTraining Accuracy: 67.36% \tValidation Loss: 0.860612 \tValidation Accuracy: 68.05%\n",
      "Epoch: 23 \tTraining Loss: 0.866482 \tTraining Accuracy: 67.37% \tValidation Loss: 0.856873 \tValidation Accuracy: 68.37%\n",
      "Epoch: 24 \tTraining Loss: 0.866583 \tTraining Accuracy: 67.29% \tValidation Loss: 0.859130 \tValidation Accuracy: 68.38%\n",
      "Epoch: 25 \tTraining Loss: 0.866556 \tTraining Accuracy: 67.27% \tValidation Loss: 0.855205 \tValidation Accuracy: 68.33%\n",
      "Epoch: 26 \tTraining Loss: 0.864937 \tTraining Accuracy: 67.52% \tValidation Loss: 0.854923 \tValidation Accuracy: 68.67%\n",
      "Epoch: 27 \tTraining Loss: 0.865366 \tTraining Accuracy: 67.36% \tValidation Loss: 0.860300 \tValidation Accuracy: 68.08%\n",
      "Epoch: 28 \tTraining Loss: 0.865180 \tTraining Accuracy: 67.32% \tValidation Loss: 0.856792 \tValidation Accuracy: 68.14%\n",
      "Epoch: 29 \tTraining Loss: 0.865264 \tTraining Accuracy: 67.45% \tValidation Loss: 0.855520 \tValidation Accuracy: 68.21%\n",
      "Epoch: 30 \tTraining Loss: 0.864407 \tTraining Accuracy: 67.51% \tValidation Loss: 0.855815 \tValidation Accuracy: 68.47%\n",
      "Epoch: 31 \tTraining Loss: 0.864758 \tTraining Accuracy: 67.38% \tValidation Loss: 0.855168 \tValidation Accuracy: 68.23%\n",
      "Epoch: 32 \tTraining Loss: 0.864494 \tTraining Accuracy: 67.44% \tValidation Loss: 0.856016 \tValidation Accuracy: 68.25%\n",
      "Epoch: 33 \tTraining Loss: 0.864418 \tTraining Accuracy: 67.39% \tValidation Loss: 0.858973 \tValidation Accuracy: 67.76%\n",
      "Epoch: 34 \tTraining Loss: 0.864296 \tTraining Accuracy: 67.51% \tValidation Loss: 0.856720 \tValidation Accuracy: 68.63%\n",
      "Epoch: 35 \tTraining Loss: 0.864976 \tTraining Accuracy: 67.45% \tValidation Loss: 0.854097 \tValidation Accuracy: 68.76%\n",
      "Epoch: 36 \tTraining Loss: 0.865641 \tTraining Accuracy: 67.37% \tValidation Loss: 0.858341 \tValidation Accuracy: 68.37%\n",
      "Epoch: 37 \tTraining Loss: 0.864424 \tTraining Accuracy: 67.55% \tValidation Loss: 0.853373 \tValidation Accuracy: 68.85%\n",
      "Epoch: 38 \tTraining Loss: 0.864676 \tTraining Accuracy: 67.39% \tValidation Loss: 0.853723 \tValidation Accuracy: 68.68%\n",
      "Epoch: 39 \tTraining Loss: 0.865632 \tTraining Accuracy: 67.35% \tValidation Loss: 0.857539 \tValidation Accuracy: 68.24%\n",
      "Epoch: 40 \tTraining Loss: 0.865915 \tTraining Accuracy: 67.32% \tValidation Loss: 0.866099 \tValidation Accuracy: 68.07%\n",
      "Epoch: 41 \tTraining Loss: 0.866915 \tTraining Accuracy: 67.23% \tValidation Loss: 0.860064 \tValidation Accuracy: 68.02%\n",
      "Epoch: 42 \tTraining Loss: 0.866911 \tTraining Accuracy: 67.22% \tValidation Loss: 0.858142 \tValidation Accuracy: 68.40%\n",
      "Epoch: 43 \tTraining Loss: 0.865130 \tTraining Accuracy: 67.46% \tValidation Loss: 0.857731 \tValidation Accuracy: 68.51%\n",
      "Epoch: 44 \tTraining Loss: 0.865624 \tTraining Accuracy: 67.53% \tValidation Loss: 0.857521 \tValidation Accuracy: 68.30%\n",
      "Epoch: 45 \tTraining Loss: 0.864822 \tTraining Accuracy: 67.53% \tValidation Loss: 0.857489 \tValidation Accuracy: 68.02%\n",
      "Epoch: 46 \tTraining Loss: 0.864708 \tTraining Accuracy: 67.51% \tValidation Loss: 0.857191 \tValidation Accuracy: 68.42%\n",
      "Epoch: 47 \tTraining Loss: 0.864364 \tTraining Accuracy: 67.53% \tValidation Loss: 0.855096 \tValidation Accuracy: 68.46%\n",
      "Epoch: 48 \tTraining Loss: 0.865595 \tTraining Accuracy: 67.35% \tValidation Loss: 0.858397 \tValidation Accuracy: 68.20%\n",
      "Epoch: 49 \tTraining Loss: 0.865718 \tTraining Accuracy: 67.34% \tValidation Loss: 0.862897 \tValidation Accuracy: 68.21%\n",
      "Epoch: 50 \tTraining Loss: 0.864272 \tTraining Accuracy: 67.56% \tValidation Loss: 0.854599 \tValidation Accuracy: 68.67%\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train, X_test, y_train, y_test are already defined for the ternary classification task\n",
    "\n",
    "X_train_ternary_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_ternary_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_ternary_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_ternary_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset_ternary = TextDataset(X_train_ternary_tensor, y_train_ternary_tensor)\n",
    "test_dataset_ternary = TextDataset(X_test_ternary_tensor, y_test_ternary_tensor)\n",
    "\n",
    "batch_size_ternary = 64 \n",
    "train_loader_ternary = DataLoader(train_dataset_ternary, batch_size=batch_size_ternary, shuffle=True)\n",
    "test_loader_ternary = DataLoader(test_dataset_ternary, batch_size=batch_size_ternary, shuffle=False)\n",
    "\n",
    "criterion_ternary = nn.CrossEntropyLoss()\n",
    "optimizer_ternary = torch.optim.Adam(ternary_model.parameters(), lr=0.01)\n",
    "valid_loss_min_ternary = np.Inf\n",
    "\n",
    "n_epochs_ternary = 50 \n",
    "\n",
    "for epoch in range(n_epochs_ternary):\n",
    "    train_loss_ternary = 0.0\n",
    "    valid_loss_ternary = 0.0\n",
    "    correct_train_ternary = 0\n",
    "    correct_valid_ternary = 0\n",
    "    \n",
    "    ternary_model.train()\n",
    "    for data, target in train_loader_ternary:\n",
    "        optimizer_ternary.zero_grad()\n",
    "        output = ternary_model(data)\n",
    "        target = target.squeeze()\n",
    "        loss = criterion_ternary(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_ternary.step()\n",
    "        train_loss_ternary += loss.item() * data.size(0)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        correct_train_ternary += (predicted == target).sum().item()\n",
    "\n",
    "    ternary_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader_ternary:\n",
    "            output = ternary_model(data)\n",
    "            target = target.squeeze()\n",
    "            loss = criterion_ternary(output, target)\n",
    "            valid_loss_ternary += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct_valid_ternary += (predicted == target).sum().item()\n",
    "\n",
    "    train_loss_ternary = train_loss_ternary / len(train_loader_ternary.dataset)\n",
    "    valid_loss_ternary = valid_loss_ternary / len(test_loader_ternary.dataset)\n",
    "    \n",
    "    train_accuracy_ternary = correct_train_ternary / len(train_loader_ternary.dataset)\n",
    "    valid_accuracy_ternary = correct_valid_ternary / len(test_loader_ternary.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Accuracy: {:.2f}% \\tValidation Loss: {:.6f} \\tValidation Accuracy: {:.2f}%'.format(\n",
    "        epoch+1, \n",
    "        train_loss_ternary,\n",
    "        train_accuracy_ternary * 100,\n",
    "        valid_loss_ternary,\n",
    "        valid_accuracy_ternary * 100\n",
    "    ))\n",
    "\n",
    "    if valid_loss_ternary <= valid_loss_min_ternary:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min_ternary,\n",
    "        valid_loss_ternary))\n",
    "        torch.save(ternary_model.state_dict(), 'ternary_model.pt')\n",
    "        valid_loss_min_ternary = valid_loss_ternary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
