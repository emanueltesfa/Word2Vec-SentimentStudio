{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Dataset Generation (5 points)\n",
    "We will use the Amazon reviews dataset used in HW1. Load the dataset\n",
    "and build a balanced dataset of 250K reviews along with their ratings (50K\n",
    "instances per each rating score) through random selection. Create ternary\n",
    "labels using the ratings. We assume that ratings more than 3 denote positive\n",
    "1\n",
    "sentiment (class 1) and rating less than 3 denote negative sentiment (class\n",
    "2). Reviews with rating 3 are considered to have neutral sentiment (class 3).\n",
    "You can store your dataset after generation and reuse it to reduce the computational load. For your experiments consider a 80%/20% training/testing\n",
    "split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Word Embedding (30 points)\n",
    "In this part the of the assignment, you will learn how to generate two sets\n",
    "of Word2Vec features for the dataset you generated. You can use Gensim\n",
    "library for this purpose. A helpful tutorial is available in the following link:\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.\n",
    "html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) (10 points)\n",
    "Load the pretrained “word2vec-google-news-300” Word2Vec model and learn\n",
    "how to extract word embeddings for your dataset. Try to check semantic\n",
    "similarities of the generated vectors using two examples of your own, e.g.,\n",
    "King − M an + W oman = Queen or excellent ∼ outstanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) (20 points)\n",
    "Train a Word2Vec model using your own dataset. Set the embedding size\n",
    "to be 300 and the window size to be 11. You can also consider a minimum\n",
    "word count of 10. Check the semantic similarities for the same two examples\n",
    "in part (a). What do you conclude from comparing vectors generated by\n",
    "yourself and the pretrained model? Which of the Word2Vec models seems\n",
    "to encode semantic similarities between words better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Simple models (20 points)\n",
    "Using the Word2Vec features that you can generate using the two models\n",
    "you prepared in the Word Embedding section, train a perceptron and an\n",
    "SVM model similar to HW1 for class 1 and class 2 (binary models). For this\n",
    "purpose, you can just use the average Word2Vec vectors for each review as\n",
    "the input feature (x = 1N PNi=1 Wi for a review with N words). To improve 2\n",
    "your performance, use the data cleaning and preprocessing steps of HW1\n",
    "to include only important words from each review when you compute the\n",
    "average x = 1 N PN i=1 Wi.\n",
    "Report your accuracy values on the testing split for\n",
    "these models for each feature type along with values you reported in your\n",
    "HW1 submission, i.e., for each of perceptron and SVM, you need to report\n",
    "three accuracy values for “word2vec-google-news-300”, your own Word2Vec,\n",
    "and TF-IDF features.\n",
    "What do you conclude from comparing performances for the models\n",
    "trained using the three different feature types (TF-IDF, pretrained Word2Vec,\n",
    "your trained Word2Vec)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Feedforward Neural Networks (25 points)\n",
    "Using the features that you can generate using the models you prepared in\n",
    "the Word “Embedding section”, train a feedforward multilayer perceptron\n",
    "network for sentiment analysis classification. Consider a network with two\n",
    "hidden layers, each with 50 and 10 nodes, respectively. You can use cross\n",
    "entropy loss and your own choice for other hyperparamters, e.g., nonlinearity,\n",
    "number of epochs, etc. Part of getting good results is to select good values\n",
    "for these hyperparamters.\n",
    "You can also refer to the following tutorial to familiarize yourself:\n",
    "https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
    "Although the above tutorial is for image data but the concept of training\n",
    "an MLP is very similar to what we want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) (10 points)\n",
    "To generate the input features, use the average Word2Vec vectors similar to\n",
    "the “Simple models” section and train the neural network. Train a network\n",
    "for binary classification using class 1 and class 2 and also a ternary model for\n",
    "the three classes. Report accuracy values on the testing split for your MLP\n",
    "model for each of the binary and ternary classification cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) (15 points)\n",
    "To generate the input features, concatenate the first 10 Word2Vec vectors\n",
    "for each review as the input feature (x = [WT\n",
    "1, ..., WT [10]) and train the neural 3\n",
    "network. Report the accuracy value on the testing split for your MLP model\n",
    "for each of the binary and ternary classification cases.\n",
    "What do you conclude by comparing accuracy values you obtain with\n",
    "those obtained in the “’Simple Models” section (note you can compare the\n",
    "accuracy values for binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Convolutional Neural Networks (20 points)\n",
    "Using the vectors you prepared in the “Word Embedding” section, train a\n",
    "convolutional neural network (CNN) for sentiment analysis classification.\n",
    "Train a simple CNN for sentiment analysis. You can consider an two-layer\n",
    "CNN with the output channel sizes of 50 and 10. To feed your data into the\n",
    "CNN, limit the maximum review length to 50 by truncating longer reviews\n",
    "and padding shorter reviews with a null value (0). You can use cross entropy\n",
    "loss and your own choice for other hyperparamters, e.g., nonlinearity, number\n",
    "of epochs, etc. Train the CNN network for binary classification using class 1\n",
    "and class 2 and also a ternary model for the three classes. Report accuracy\n",
    "values on the testing split for your CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
