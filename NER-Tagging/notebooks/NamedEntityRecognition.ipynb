{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Tuple, List , Optional, Dict\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import os \n",
    "\n",
    "from itertools import product\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data_files: list[str]) -> tuple[dict[str, int], dict[str, int]]:\n",
    "\n",
    "    \"\"\"Builds word and tag vocabularies from the given list of data files.\n",
    "\n",
    "    Args:\n",
    "        data_files (list): A list of file paths containing the data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two dictionaries: `word_vocab` and `tag_vocab`.\n",
    "            - `word_vocab` (dict): A dictionary mapping words to their indices.\n",
    "            - `tag_vocab` (dict): A dictionary mapping tags to their indices.\n",
    "    \"\"\"\n",
    "\n",
    "    word_vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_vocab = {}\n",
    "    word_idx, tag_idx = 2, 0  \n",
    "\n",
    "    for file_path in data_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    _, word, tag = line.split()\n",
    "                    if word not in word_vocab:\n",
    "                        word_vocab[word] = word_idx\n",
    "                        word_idx += 1\n",
    "                    if tag not in tag_vocab:\n",
    "                        tag_vocab[tag] = tag_idx\n",
    "                        tag_idx += 1\n",
    "    return word_vocab, tag_vocab\n",
    "\n",
    "\n",
    "def get_class_weights(file_paths: list[str], tag_vocab: dict[str, int]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    \"\"\"Calculate class weights for a set of tags based on their frequency in the dataset.\n",
    "\n",
    "    Args:\n",
    "        file_paths (list[str]): A list of file paths containing the data.\n",
    "        tag_vocab (dict[str, int]): A dictionary mapping tags to their indices.\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: A tuple containing two tensors:\n",
    "            - `regular_weights_tensor` (torch.Tensor): A tensor containing the regular class weights.\n",
    "            - `inv_weights_tensor` (torch.Tensor): A tensor containing the inverse class weights.\n",
    "    \"\"\"\n",
    "\n",
    "    tag_counts = Counter()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 3:\n",
    "                    _, _, tag = parts\n",
    "                    if tag in tag_vocab:  \n",
    "                        tag_counts[tag] += 1\n",
    "\n",
    "    total_tags = sum(tag_counts.values())\n",
    "    regular_weights = {tag: (count / total_tags) for tag, count in tag_counts.items()}\n",
    "    inv_weights = {tag: (total_tags / (count + 1e-9)) for tag, count in tag_counts.items()} # Calculate inverse class weights (inversely proportional to frequency)\n",
    "\n",
    "    num_tags = len(tag_vocab)\n",
    "    regular_weights_tensor = torch.zeros(num_tags, dtype=torch.float) # Initialize weights tensors based on tag_vocab ordering\n",
    "    inv_weights_tensor = torch.zeros(num_tags, dtype=torch.float)\n",
    "\n",
    "    # Populate the tensors from tag_vocab indices\n",
    "    for tag, idx in tag_vocab.items():\n",
    "        regular_weights_tensor[idx] = regular_weights.get(tag, 0)\n",
    "        inv_weights_tensor[idx] = inv_weights.get(tag, 0)\n",
    "\n",
    "    return regular_weights_tensor, inv_weights_tensor\n",
    "\n",
    "\n",
    "class BILSTMForNER(nn.Module):\n",
    "    \"\"\"A Bidirectional LSTM model for Named Entity Recognition.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): The dimensionality of the input data.\n",
    "        embedding_dim (int): The dimensionality of the word embeddings.\n",
    "        hidden_dim (int): The dimensionality of the hidden state of the LSTM.\n",
    "        output_dim (int): The dimensionality of the output.\n",
    "        num_layers (int, optional): Number of LSTM layers. Defaults to 1.\n",
    "        dropout (float, optional): Dropout probability. Defaults to 0.33.\n",
    "\n",
    "    Returns:\n",
    "        BILSTMForNER: A Bidirectional LSTM model for Named Entity Recognition.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, glove_embeddings = None, num_layers = 1, dropout = 0.33):\n",
    "        super(BILSTMForNER, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        if glove_embeddings is not None:\n",
    "            self.embedding.weight = nn.Parameter(glove_embeddings)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first = True, bidirectional = True, dropout = dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 128)\n",
    "        self.classifier = nn.Linear(128, output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.elu(self.fc(lstm_out))  \n",
    "        logits = self.classifier(out)  \n",
    "        return logits  # batch, seq, embeddim\n",
    "\n",
    "\n",
    "class IndexedNERDataset(Dataset):\n",
    "\n",
    "    \"\"\"Dataset class for Indexed Named Entity Recognition.\n",
    "\n",
    "    This class prepares data for Named Entity Recognition tasks by indexing words and tags.\n",
    "\n",
    "    Attributes:\n",
    "        word_vocab (dict[str, int]): A dictionary mapping words to their indices.\n",
    "        tag_vocab (Optional[dict[str, int]]): A dictionary mapping tags to their indices if `use_tags` is True, otherwise None.\n",
    "        use_tags (bool): Whether to include tags.\n",
    "        data (list): A list to store the processed data.\n",
    "\n",
    "    Methods:\n",
    "        __init__: Initialize the dataset.\n",
    "        _load_data: Load data from the dataset file.\n",
    "        __len__: Return the number of data instances in the dataset.\n",
    "        __getitem__: Retrieve a specific data instance from the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, file_path, word_vocab, tag_vocab = None, use_tags = True):\n",
    "        self.word_vocab = word_vocab\n",
    "        self.tag_vocab = tag_vocab if use_tags else None\n",
    "        self.use_tags = use_tags\n",
    "        self.data = []\n",
    "        self._load_data(file_path)\n",
    "        \n",
    "    def _load_data(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            sentence = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    if self.use_tags:\n",
    "                        _, word, tag = line.split()\n",
    "                        tag_idx = self.tag_vocab.get(tag, -1)  \n",
    "                    else:\n",
    "                        word = line\n",
    "                        tag_idx = -1 \n",
    "                    sentence.append((self.word_vocab.get(word, self.word_vocab['<UNK>']), tag_idx))\n",
    "                else:\n",
    "                    self.data.append(sentence)\n",
    "                    sentence = []\n",
    "            if sentence:  # Handle the case where the file doesn't end with a newline\n",
    "                self.data.append(sentence)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, tags = zip(*self.data[idx])\n",
    "        return torch.tensor(sentence, dtype=torch.long), torch.tensor(tags, dtype=torch.long)\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "    sentences, tags = zip(*batch)\n",
    "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value = word_vocab['<PAD>'])\n",
    "    tags_padded = pad_sequence(tags, batch_first=True, padding_value = -1)  # Use -1 or another unique index for padding in tags\n",
    "    return sentences_padded, tags_padded\n",
    "\n",
    "\n",
    "def compute_metrics(preds: List[int], labels: List[int]) -> Tuple[float, float, float]:\n",
    "    \"\"\"Compute F1 scores for the given predictions and labels.\n",
    "\n",
    "    Args:\n",
    "        preds (List[int]): Predicted labels.\n",
    "        labels (List[int]): True labels.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: A tuple containing F1 scores for macro, micro, and weighted averages.\n",
    "    \"\"\"\n",
    "\n",
    "    f1_mac = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    f1_mic = f1_score(labels, preds, average='micro', zero_division=0)\n",
    "    f1_weight = f1_score(labels, preds, average='weighted', zero_division=0)\n",
    "\n",
    "    return f1_mac, f1_mic, f1_weight\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, dataloader: DataLoader, criterion: nn.Module, device: torch.device) -> Tuple[float, float, float, float, float]:\n",
    "    \"\"\"Evaluate the model on the given dataloader using the specified criterion.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to evaluate.\n",
    "        dataloader (DataLoader): DataLoader providing the evaluation data.\n",
    "        criterion (nn.Module): The loss criterion.\n",
    "        device (torch.device): The device (CPU or GPU) to perform evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float, float, float]: A tuple containing the average loss, accuracy, and F1 scores for macro, micro, and weighted averages.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            # (batch sz, seq len) \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # print(torch.max(outputs, dim=2))\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, dim=2)\n",
    "            all_preds.extend(preds.view(-1).cpu().numpy())\n",
    "            all_labels.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "    valid_indices = [i for i, label in enumerate(all_labels) if label != -1]\n",
    "    valid_preds = [all_preds[i] for i in valid_indices]\n",
    "    valid_labels = [all_labels[i] for i in valid_indices]\n",
    "\n",
    "    accuracy = np.mean(np.array(valid_preds) == np.array(valid_labels))\n",
    "    f1_mac, f1_mic, f1_weight = compute_metrics(valid_preds, valid_labels)\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy, f1_mac, f1_mic, f1_weight\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, dataloaders: dict[str, DataLoader], optimizer: optim.Optimizer, criterion: nn.Module, device: torch.device, num_epochs: int = 50, patience: int = 10, scheduler: Optional[optim.lr_scheduler._LRScheduler] = None, ckpt_name: str = 'best_model.pth') -> Tuple[float, float, float]:\n",
    "    \"\"\"Train the model using the provided dataloaders, optimizer, and criterion.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloaders (dict[str, DataLoader]): A dictionary containing DataLoader objects for training and validation datasets.\n",
    "        optimizer (optim.Optimizer): The optimizer for updating the model's parameters.\n",
    "        criterion (nn.Module): The loss criterion.\n",
    "        device (torch.device): The device (CPU or GPU) to perform training on.\n",
    "        num_epochs (int, optional): Number of epochs for training. Defaults to 50.\n",
    "        patience (int, optional): Number of epochs to wait for improvement in validation F1 score before early stopping. Defaults to 10.\n",
    "        scheduler (Optional[optim.lr_scheduler._LRScheduler], optional): Learning rate scheduler. Defaults to None.\n",
    "        ckpt_name (str, optional): File name to save the best model checkpoint. Defaults to 'best_model.pth'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: A tuple containing the accuracy on the validation set, accuracy on the training set, and the best validation F1 score achieved during training.\n",
    "    \"\"\"\n",
    "\n",
    "    best_val_f1 = -float('inf')\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in tqdm( range(num_epochs) ):\n",
    "        model.train()\n",
    "        for inputs, labels in dataloaders['train']:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            a_max, a_indx = torch.max(outputs, dim = 2)\n",
    "            # print(torch.max(outputs, dim=2))\n",
    "            # print('predition shape ',outputs.shape, 'label shape', labels.shape)\n",
    "            # print('predition',outputs, 'label', labels) \n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_loss, train_acc, train_f1_mac, train_f1_mic, train_f1_weighted = evaluate_model(model, dataloaders['train'], criterion, device)\n",
    "        val_loss, val_acc, val_f1_mac, val_f1_mic, val_f1_weighted = evaluate_model(model, dataloaders['dev'], criterion, device)\n",
    "\n",
    "        if epoch % 5 == 0: \n",
    "            print(f'Epoch {epoch+1}:')\n",
    "            print(f'Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f},  F1Mac: {train_f1_mac:.4f}')\n",
    "            print(f'Val - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1Mac: {val_f1_mac:.4f}')\n",
    "\n",
    "            \n",
    "        # Early stopping based on validation F1 score\n",
    "        if val_f1_mac > best_val_f1:\n",
    "            best_val_f1 = val_f1_mac\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), ckpt_name)\n",
    "\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    ### after saving location of model in ckpt_name\n",
    "        #checkpoint = torch.load(ckpt_name)\n",
    "        # model.load_state_dict(checkpoint)\n",
    "    ### and run lower eval, retrieve predictions => return dev prediction file path custom/glove,\n",
    "        # file_paths = ['../../data/lstm-data/dev']#, '../../data/lstm-data/dev']\n",
    "        # word_vocab, _ = build_vocab(data_files)\n",
    "        # output_paths = get_eval_preds(model, file_paths, word_vocab)\n",
    "    ###  the run   # evaluate_fb1_model(preds_file_path, gold_file_path)\n",
    "        # evaluate_fb1_model(output_paths, gold_file_path = '../../data/lstm-data/dev')\n",
    "    return val_acc, train_acc, best_val_f1\n",
    "\n",
    "data_files = ['../../data/lstm-data/train', '../../data/lstm-data/dev']\n",
    "word_vocab, tag_vocab = build_vocab(data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAN IGNORE FOR NOW UNTIL HYPERPARAM GRID SEARCH DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def call_eval_script(preds_file_path, gold_file_path):\n",
    "#     # Ensure preds_file_path is a single string\n",
    "#     # If preds_file_path can be a list but you only need to handle one file at a time, adjust accordingly\n",
    "#     if isinstance(preds_file_path, list):\n",
    "#         preds_file_path = preds_file_path[0]  # Assuming you're handling one file at a time\n",
    "\n",
    "#     print('preds_file_path: ', preds_file_path, \"gold_file_path: \", gold_file_path)\n",
    "\n",
    "#     # Correctly formatted command\n",
    "#     cmd = f\"python ../eval.py -p {preds_file_path} -g {gold_file_path}\"\n",
    "    \n",
    "#     # Running the subprocess and capturing output\n",
    "#     result = subprocess.run(cmd, capture_output=True, text=True, shell=True)\n",
    "#     return result\n",
    "\n",
    "# def parse_eval_output(output):\n",
    "#     # Parse the output string from your script to extract F1 and potentially other metrics\n",
    "#     # This is highly dependent on your script's output format\n",
    "#     print('outputs from eval script: ', output)\n",
    "#     lines = output.strip().split('\\n')\n",
    "#     summary_line = lines[0]  # Assuming the first line contains the overall metrics\n",
    "#     metrics = summary_line.split(';')\n",
    "#     # Extract precision, recall, and F1 - adjust parsing as per your actual output\n",
    "#     precision = float(metrics[1].split(':')[1].strip().replace('%', ''))\n",
    "#     recall = float(metrics[2].split(':')[1].strip().replace('%', ''))\n",
    "#     f1 = float(metrics[3].split(':')[1].strip().replace('%', ''))\n",
    "#     print('f1: ',f1, '\\n Line: ', lines)\n",
    "#     return precision, recall, f1\n",
    "\n",
    "# def evaluate_fb1_model(preds_file_path, gold_file_path):\n",
    "#     \"\"\"\n",
    "#     Evaluates the model on the given dataloader and calls an external script for metrics.\n",
    "#     \"\"\"\n",
    "#     # Your code to generate predictions and save them to preds_file_path\n",
    "#     # Ensure predictions are in the correct format for your eval script\n",
    "\n",
    "#     # After saving predictions, call the eval script\n",
    "#     eval_output = call_eval_script(preds_file_path, gold_file_path)\n",
    "#     precision, recall, f1 = parse_eval_output(eval_output)\n",
    "\n",
    "#     return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Embedding Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:19<02:55, 19.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train - Loss: 0.7575, Acc: 0.7889,  F1Mac: 0.5193\n",
      "Val - Loss: 1.0084, Acc: 0.7755, F1Mac: 0.4669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:54<01:16, 19.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "Train - Loss: 0.1401, Acc: 0.9161,  F1Mac: 0.7726\n",
      "Val - Loss: 0.9824, Acc: 0.8732, F1Mac: 0.6337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:52<00:00, 17.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8711078366745512, 0.9168194283535468, 0.6671650354068519)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(word_vocab)  \n",
    "output_dim = len(tag_vocab)\n",
    "\n",
    "dataset_dict = {\n",
    "    'train': IndexedNERDataset('../../data/lstm-data/train', word_vocab, tag_vocab),\n",
    "    'dev': IndexedNERDataset('../../data/lstm-data/dev', word_vocab, tag_vocab)\n",
    "}\n",
    "\n",
    "# print('input dim: ', input_dim, 'output dim: ', output_dim)\n",
    "\n",
    "regular_class_weight, inv_class_weight = get_class_weights(data_files, tag_vocab)\n",
    "regular_class_weight, inv_class_weight = regular_class_weight.to(device),  inv_class_weight.to(device)\n",
    "\n",
    "model = BILSTMForNER(input_dim = input_dim, embedding_dim = 100, hidden_dim = 256, dropout = 0.33, output_dim = output_dim)\n",
    "criterion = nn.CrossEntropyLoss(weight = inv_class_weight, ignore_index = tag_vocab.get('<PAD>', -1))  \n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1, momentum = 0.9, weight_decay = 0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 50, gamma = 0.1)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(IndexedNERDataset('../../data/lstm-data/train', word_vocab, tag_vocab), batch_size = 16, shuffle = True, collate_fn = pad_collate),\n",
    "    'dev': DataLoader(IndexedNERDataset('../../data/lstm-data/dev', word_vocab, tag_vocab), batch_size = 16, shuffle = False, collate_fn = pad_collate),\n",
    "    # 'test': DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate)\n",
    "}\n",
    "\n",
    "train_model(model, dataloaders, optimizer, criterion, device, ckpt_name = './ckpts/custom_BiLSTM.pth', patience = 20, num_epochs = 10) # MODEL SAVED AT '/NER-Tagging/notebooks/ckpts/custom_BiLSTM.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOVE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path: str, word_vocab: dict[str, int], embedding_dim: int) -> torch.Tensor:\n",
    "    \"\"\"Load pre-trained GloVe embeddings from the specified path and create an embedding matrix.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the GloVe embeddings file.\n",
    "        word_vocab (dict[str, int]): A dictionary mapping words to their indices.\n",
    "        embedding_dim (int): The dimensionality of the word embeddings.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor representing the embedding matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_dict = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]  \n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embedding_dict[word] = vector\n",
    "    \n",
    "    vocab_size = len(word_vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    for word, idx in word_vocab.items():\n",
    "        embedding_vector = embedding_dict.get(word, embedding_dict.get(word.lower()))\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.randn(embedding_dim) \n",
    "    \n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = '../../data/lstm-data/glove.6B.100d/glove.6B.100d.txt' \n",
    "glove_embeddings = load_glove_embeddings(glove_path, word_vocab, embedding_dim = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOVE Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/150 [00:11<29:04, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train - Loss: 0.5651, Acc: 0.7875,  F1Mac: 0.5387\n",
      "Val - Loss: 0.6399, Acc: 0.7913, F1Mac: 0.5325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 6/150 [01:04<25:03, 10.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "Train - Loss: 0.1285, Acc: 0.9134,  F1Mac: 0.7666\n",
      "Val - Loss: 0.3761, Acc: 0.9037, F1Mac: 0.7123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 11/150 [01:58<25:31, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:\n",
      "Train - Loss: 0.0589, Acc: 0.9683,  F1Mac: 0.8933\n",
      "Val - Loss: 0.4414, Acc: 0.9527, F1Mac: 0.8173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 16/150 [02:50<23:16, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:\n",
      "Train - Loss: 0.0241, Acc: 0.9841,  F1Mac: 0.9486\n",
      "Val - Loss: 0.6143, Acc: 0.9653, F1Mac: 0.8523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 21/150 [03:41<22:06, 10.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:\n",
      "Train - Loss: 0.0243, Acc: 0.9633,  F1Mac: 0.8938\n",
      "Val - Loss: 0.5233, Acc: 0.9464, F1Mac: 0.8070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 26/150 [04:33<21:11, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26:\n",
      "Train - Loss: 0.0102, Acc: 0.9880,  F1Mac: 0.9574\n",
      "Val - Loss: 0.5701, Acc: 0.9676, F1Mac: 0.8581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 31/150 [05:30<22:19, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31:\n",
      "Train - Loss: 0.0118, Acc: 0.9865,  F1Mac: 0.9547\n",
      "Val - Loss: 0.5461, Acc: 0.9667, F1Mac: 0.8590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 36/150 [06:24<20:29, 10.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36:\n",
      "Train - Loss: 0.0166, Acc: 0.9858,  F1Mac: 0.9587\n",
      "Val - Loss: 0.5441, Acc: 0.9664, F1Mac: 0.8607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 41/150 [07:21<20:28, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41:\n",
      "Train - Loss: 0.0073, Acc: 0.9928,  F1Mac: 0.9775\n",
      "Val - Loss: 0.5671, Acc: 0.9717, F1Mac: 0.8745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 46/150 [08:17<19:54, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46:\n",
      "Train - Loss: 0.0076, Acc: 0.9945,  F1Mac: 0.9814\n",
      "Val - Loss: 0.6009, Acc: 0.9723, F1Mac: 0.8742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 51/150 [09:20<21:14, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51:\n",
      "Train - Loss: 0.0180, Acc: 0.9869,  F1Mac: 0.9411\n",
      "Val - Loss: 0.5012, Acc: 0.9632, F1Mac: 0.8410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 56/150 [10:29<21:25, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56:\n",
      "Train - Loss: 0.0068, Acc: 0.9958,  F1Mac: 0.9849\n",
      "Val - Loss: 0.6045, Acc: 0.9729, F1Mac: 0.8766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 61/150 [11:38<19:51, 13.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61:\n",
      "Train - Loss: 0.0057, Acc: 0.9961,  F1Mac: 0.9877\n",
      "Val - Loss: 0.6032, Acc: 0.9731, F1Mac: 0.8808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 66/150 [12:37<16:35, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66:\n",
      "Train - Loss: 0.0078, Acc: 0.9926,  F1Mac: 0.9789\n",
      "Val - Loss: 0.5620, Acc: 0.9690, F1Mac: 0.8699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 71/150 [13:31<14:30, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71:\n",
      "Train - Loss: 0.0047, Acc: 0.9972,  F1Mac: 0.9901\n",
      "Val - Loss: 0.6237, Acc: 0.9744, F1Mac: 0.8874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 76/150 [14:25<12:22, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76:\n",
      "Train - Loss: 0.0035, Acc: 0.9980,  F1Mac: 0.9926\n",
      "Val - Loss: 0.6180, Acc: 0.9745, F1Mac: 0.8865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 81/150 [15:05<09:38,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81:\n",
      "Train - Loss: 0.0263, Acc: 0.9920,  F1Mac: 0.9742\n",
      "Val - Loss: 0.5642, Acc: 0.9667, F1Mac: 0.8620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 86/150 [15:45<08:32,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86:\n",
      "Train - Loss: 0.0039, Acc: 0.9978,  F1Mac: 0.9908\n",
      "Val - Loss: 0.5979, Acc: 0.9739, F1Mac: 0.8840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 91/150 [16:31<09:17,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91:\n",
      "Train - Loss: 0.0030, Acc: 0.9984,  F1Mac: 0.9947\n",
      "Val - Loss: 0.5987, Acc: 0.9734, F1Mac: 0.8849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 96/150 [17:32<10:18, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96:\n",
      "Train - Loss: 0.0470, Acc: 0.9894,  F1Mac: 0.9627\n",
      "Val - Loss: 0.6376, Acc: 0.9649, F1Mac: 0.8501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 101/150 [18:24<08:39, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101:\n",
      "Train - Loss: 0.0035, Acc: 0.9980,  F1Mac: 0.9931\n",
      "Val - Loss: 0.6408, Acc: 0.9733, F1Mac: 0.8834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 106/150 [19:26<08:50, 12.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106:\n",
      "Train - Loss: 0.0027, Acc: 0.9989,  F1Mac: 0.9945\n",
      "Val - Loss: 0.5659, Acc: 0.9734, F1Mac: 0.8823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 111/150 [20:14<06:21,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111:\n",
      "Train - Loss: 0.0154, Acc: 0.9897,  F1Mac: 0.9639\n",
      "Val - Loss: 0.5586, Acc: 0.9611, F1Mac: 0.8466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 116/150 [21:05<05:55, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116:\n",
      "Train - Loss: 0.0034, Acc: 0.9984,  F1Mac: 0.9927\n",
      "Val - Loss: 0.6539, Acc: 0.9724, F1Mac: 0.8816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 121/150 [21:59<05:22, 11.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121:\n",
      "Train - Loss: 0.0021, Acc: 0.9992,  F1Mac: 0.9971\n",
      "Val - Loss: 0.6069, Acc: 0.9728, F1Mac: 0.8840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 126/150 [22:49<03:55,  9.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126:\n",
      "Train - Loss: 0.0064, Acc: 0.9969,  F1Mac: 0.9861\n",
      "Val - Loss: 0.6293, Acc: 0.9707, F1Mac: 0.8760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 131/150 [23:34<02:53,  9.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131:\n",
      "Train - Loss: 0.0028, Acc: 0.9991,  F1Mac: 0.9960\n",
      "Val - Loss: 0.6131, Acc: 0.9729, F1Mac: 0.8830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 136/150 [24:21<02:09,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136:\n",
      "Train - Loss: 0.0025, Acc: 0.9991,  F1Mac: 0.9957\n",
      "Val - Loss: 0.6058, Acc: 0.9719, F1Mac: 0.8800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 141/150 [25:06<01:21,  9.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141:\n",
      "Train - Loss: 0.0925, Acc: 0.9934,  F1Mac: 0.9598\n",
      "Val - Loss: 0.8607, Acc: 0.9641, F1Mac: 0.8264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 146/150 [25:52<00:36,  9.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146:\n",
      "Train - Loss: 0.0038, Acc: 0.9985,  F1Mac: 0.9941\n",
      "Val - Loss: 0.6661, Acc: 0.9717, F1Mac: 0.8804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [26:29<00:00, 10.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9718290744115708, 0.9989636647162055, 0.8906130444694725)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders = {\n",
    "    'train': DataLoader(IndexedNERDataset('../../data/lstm-data/train', word_vocab, tag_vocab), batch_size = 32, shuffle = True, collate_fn = pad_collate),\n",
    "    'dev': DataLoader(IndexedNERDataset('../../data/lstm-data/dev', word_vocab, tag_vocab), batch_size = 32, shuffle = False, collate_fn = pad_collate),\n",
    "    # 'test': DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate)\n",
    "}\n",
    "\n",
    "glove_model = BILSTMForNER(input_dim = input_dim, embedding_dim = 100, hidden_dim = 256, glove_embeddings = glove_embeddings, dropout = 0.33, output_dim = output_dim)\n",
    "criterion = nn.CrossEntropyLoss(weight = inv_class_weight, ignore_index = tag_vocab.get('<PAD>', -1))  \n",
    "# step_optimizer = optim.SGD(glove_model_2.parameters(), lr = 0.1, momentum = 0.9, weight_decay = 0.0001)\n",
    "# step_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 50, gamma = 0.1)\n",
    "\n",
    "train_model(glove_model, dataloaders, optim.SGD(glove_model.parameters(), lr = 0.05, momentum = 0.9, weight_decay = 0.0001), criterion, device, ckpt_name = './ckpts/glove_BiLSTM.pth', patience = 150, num_epochs = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKE PREDS AND RUN EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_preds(model, file_paths, word_vocab, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), idx_to_tag={idx: tag for tag, idx in tag_vocab.items()}, output_dir='../../data/lstm-data/preds/', output_postfix = None):   \n",
    "    \"\"\"Generate predictions for evaluation using the provided model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        file_paths (list[str]): A list of file paths containing evaluation data.\n",
    "        word_vocab (dict[str, int]): A dictionary mapping words to their indices.\n",
    "        device (torch.device, optional): The device (CPU or GPU) to perform evaluation on. Defaults to GPU if available, otherwise CPU.\n",
    "        idx_to_tag (dict[int, str], optional): A dictionary mapping tag indices to tag names. Defaults to None.\n",
    "        output_dir (str, optional): The directory to save prediction files. Defaults to '../../data/lstm-data/preds/'.\n",
    "        output_postfix (str, optional): A postfix to append to the output file names. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of file paths where the predictions are saved.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    output_paths = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        print(file_path)\n",
    "        output_file_name = os.path.basename(file_path) + \"_preds_\" + output_postfix\n",
    "        output_path = os.path.join(output_dir, 'new', output_file_name)\n",
    "        output_paths.append(output_path)\n",
    "        # print(\"error may be: \", file_path, 'Or: ', output_path)\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f, open(output_path, 'w', encoding='utf-8') as out_f:     \n",
    "            sentences = []\n",
    "            current_sentence = []\n",
    "            for line in f:\n",
    "                if line.strip():  # if line contains stripable parts \n",
    "                    parts = line.strip().split()\n",
    "                    original_word = parts[1]  \n",
    "                    current_sentence.append(word_vocab.get(original_word, word_vocab['<UNK>']))\n",
    "                elif current_sentence:  # Empty line and current sentence is not empty\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "\n",
    "            # Add the last sentence if the file doesn't end with a newline\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "\n",
    "            # Predict and write to file\n",
    "            for sentence in sentences: \n",
    "                sentence_tensor = torch.tensor([sentence], dtype = torch.long, device = device)\n",
    "                outputs = model(sentence_tensor)\n",
    "                _, preds = torch.max(outputs, dim = 2)\n",
    "                pred_tags = [idx_to_tag[pred.item()] for pred in preds[0]]  # Convert indices to tags\n",
    "\n",
    "\n",
    "                # Prediction writing \n",
    "                for i, word_idx in enumerate(sentence):\n",
    "                    word = list(word_vocab.keys())[list(word_vocab.values()).index(word_idx)]  # Inverse lookup\n",
    "                    tag = pred_tags[i]\n",
    "                    out_f.write(f\"{i+1}\\t{word}\\t{tag}\\n\")\n",
    "                out_f.write(\"\\n\")  # New line after each sentence\n",
    "    return output_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction files generated in next two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/lstm-data/test\n",
      "../../data/lstm-data/dev\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../../data/lstm-data/preds/new\\\\test_preds_custom_BiLSTM',\n",
       " '../../data/lstm-data/preds/new\\\\dev_preds_custom_BiLSTM']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = ['../../data/lstm-data/test', '../../data/lstm-data/dev']\n",
    "\n",
    "checkpoint = torch.load('./ckpts/custom_BiLSTM.pth') \n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "get_eval_preds(model, file_paths, word_vocab, output_postfix = 'custom_BiLSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/lstm-data/test\n",
      "../../data/lstm-data/dev\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../../data/lstm-data/preds/new\\\\test_preds_glove_BiLSTM',\n",
       " '../../data/lstm-data/preds/new\\\\dev_preds_glove_BiLSTM']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('./ckpts/glove_BiLSTM.pth') \n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "get_eval_preds(model, file_paths, word_vocab, output_postfix = 'glove_BiLSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim:  (26886, 9)\n",
      "class weight: torch.Size([9]),\n",
      " INV: torch.Size([9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [16:45<00:00, 10.05s/it]\n",
      "  4%|▍         | 1/24 [16:45<6:25:20, 1005.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0191, Acc: 0.9694,  F1Mac: 0.8971, F1Mic: 0.9694,  F1W: 0.9714\n",
      "Val -   Loss: 0.3298, Acc: 0.9536, F1Mac: 0.8165, F1Mic: 0.9536,  F1W: 0.9571\n",
      "LR: 0.01, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9536, Scheduler Params: 0.1, 25, None, Train_Acc: 0.9694, Val_F1: 0.8467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:28<00:00, 10.48s/it]\n",
      "  8%|▊         | 2/24 [34:13<6:17:51, 1030.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0856, Acc: 0.9388,  F1Mac: 0.8147, F1Mic: 0.9388,  F1W: 0.9450\n",
      "Val -   Loss: 0.2947, Acc: 0.9295, F1Mac: 0.7627, F1Mic: 0.9295,  F1W: 0.9372\n",
      "LR: 0.01, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9295, Scheduler Params: 0.1, 25, StepLR, Train_Acc: 0.9388, Val_F1: 0.7643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [16:44<00:00, 10.05s/it]\n",
      " 12%|█▎        | 3/24 [50:58<5:56:35, 1018.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0213, Acc: 0.9707,  F1Mac: 0.9056, F1Mic: 0.9707,  F1W: 0.9724\n",
      "Val -   Loss: 0.4010, Acc: 0.9535, F1Mac: 0.8193, F1Mic: 0.9535,  F1W: 0.9564\n",
      "LR: 0.01, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9535, Scheduler Params: 0.1, 50, None, Train_Acc: 0.9707, Val_F1: 0.8527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [16:43<00:00, 10.03s/it]\n",
      " 17%|█▋        | 4/24 [1:07:41<5:37:32, 1012.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0306, Acc: 0.9648,  F1Mac: 0.8804, F1Mic: 0.9648,  F1W: 0.9672\n",
      "Val -   Loss: 0.3087, Acc: 0.9503, F1Mac: 0.8111, F1Mic: 0.9503,  F1W: 0.9544\n",
      "LR: 0.01, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9503, Scheduler Params: 0.1, 50, StepLR, Train_Acc: 0.9648, Val_F1: 0.8187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:22<00:00, 10.43s/it]\n",
      " 21%|██        | 5/24 [1:25:04<5:24:06, 1023.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0166, Acc: 0.9756,  F1Mac: 0.9197, F1Mic: 0.9756,  F1W: 0.9769\n",
      "Val -   Loss: 0.3395, Acc: 0.9603, F1Mac: 0.8401, F1Mic: 0.9603,  F1W: 0.9626\n",
      "LR: 0.01, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9603, Scheduler Params: 0.01, 25, None, Train_Acc: 0.9756, Val_F1: 0.8529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:23<00:00, 10.44s/it]\n",
      " 25%|██▌       | 6/24 [1:42:28<5:09:07, 1030.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.1079, Acc: 0.9290,  F1Mac: 0.7903, F1Mic: 0.9290,  F1W: 0.9371\n",
      "Val -   Loss: 0.2935, Acc: 0.9203, F1Mac: 0.7447, F1Mic: 0.9203,  F1W: 0.9300\n",
      "LR: 0.01, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9203, Scheduler Params: 0.01, 25, StepLR, Train_Acc: 0.9290, Val_F1: 0.7464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:20<00:00, 10.41s/it]\n",
      " 29%|██▉       | 7/24 [1:59:49<4:52:55, 1033.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0183, Acc: 0.9712,  F1Mac: 0.9035, F1Mic: 0.9712,  F1W: 0.9729\n",
      "Val -   Loss: 0.3415, Acc: 0.9577, F1Mac: 0.8329, F1Mic: 0.9577,  F1W: 0.9603\n",
      "LR: 0.01, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9577, Scheduler Params: 0.01, 50, None, Train_Acc: 0.9712, Val_F1: 0.8494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:22<00:00, 10.43s/it]\n",
      " 33%|███▎      | 8/24 [2:17:11<4:36:27, 1036.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0378, Acc: 0.9599,  F1Mac: 0.8708, F1Mic: 0.9599,  F1W: 0.9627\n",
      "Val -   Loss: 0.3049, Acc: 0.9481, F1Mac: 0.8057, F1Mic: 0.9481,  F1W: 0.9521\n",
      "LR: 0.01, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9481, Scheduler Params: 0.01, 50, StepLR, Train_Acc: 0.9599, Val_F1: 0.8088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:23<00:00, 10.43s/it]\n",
      " 38%|███▊      | 9/24 [2:34:35<4:19:40, 1038.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0050, Acc: 0.9938,  F1Mac: 0.9733, F1Mic: 0.9938,  F1W: 0.9939\n",
      "Val -   Loss: 0.4870, Acc: 0.9705, F1Mac: 0.8657, F1Mic: 0.9705,  F1W: 0.9711\n",
      "LR: 0.05, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9705, Scheduler Params: 0.1, 25, None, Train_Acc: 0.9938, Val_F1: 0.8862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:21<00:00, 10.41s/it]\n",
      " 42%|████▏     | 10/24 [2:51:56<4:02:34, 1039.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0162, Acc: 0.9782,  F1Mac: 0.9256, F1Mic: 0.9782,  F1W: 0.9790\n",
      "Val -   Loss: 0.3430, Acc: 0.9639, F1Mac: 0.8447, F1Mic: 0.9639,  F1W: 0.9656\n",
      "LR: 0.05, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9639, Scheduler Params: 0.1, 25, StepLR, Train_Acc: 0.9782, Val_F1: 0.8458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:19<00:00, 10.40s/it]\n",
      " 46%|████▌     | 11/24 [3:09:16<3:45:16, 1039.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0040, Acc: 0.9963,  F1Mac: 0.9859, F1Mic: 0.9963,  F1W: 0.9963\n",
      "Val -   Loss: 0.5621, Acc: 0.9748, F1Mac: 0.8858, F1Mic: 0.9748,  F1W: 0.9748\n",
      "LR: 0.05, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9748, Scheduler Params: 0.1, 50, None, Train_Acc: 0.9963, Val_F1: 0.8922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:20<00:00, 10.41s/it]\n",
      " 50%|█████     | 12/24 [3:26:37<3:28:00, 1040.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0067, Acc: 0.9894,  F1Mac: 0.9625, F1Mic: 0.9894,  F1W: 0.9896\n",
      "Val -   Loss: 0.4000, Acc: 0.9713, F1Mac: 0.8685, F1Mic: 0.9713,  F1W: 0.9720\n",
      "LR: 0.05, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9713, Scheduler Params: 0.1, 50, StepLR, Train_Acc: 0.9894, Val_F1: 0.8765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:18<00:00, 10.39s/it]\n",
      " 54%|█████▍    | 13/24 [3:43:56<3:10:37, 1039.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0065, Acc: 0.9926,  F1Mac: 0.9718, F1Mic: 0.9926,  F1W: 0.9928\n",
      "Val -   Loss: 0.4920, Acc: 0.9682, F1Mac: 0.8555, F1Mic: 0.9682,  F1W: 0.9691\n",
      "LR: 0.05, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9682, Scheduler Params: 0.01, 25, None, Train_Acc: 0.9926, Val_F1: 0.8859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:16<00:00, 10.37s/it]\n",
      " 58%|█████▊    | 14/24 [4:01:13<2:53:08, 1038.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0228, Acc: 0.9734,  F1Mac: 0.9089, F1Mic: 0.9734,  F1W: 0.9747\n",
      "Val -   Loss: 0.3456, Acc: 0.9583, F1Mac: 0.8280, F1Mic: 0.9583,  F1W: 0.9608\n",
      "LR: 0.05, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9583, Scheduler Params: 0.01, 25, StepLR, Train_Acc: 0.9734, Val_F1: 0.8289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:17<00:00, 10.37s/it]\n",
      " 62%|██████▎   | 15/24 [4:18:30<2:35:45, 1038.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0040, Acc: 0.9970,  F1Mac: 0.9874, F1Mic: 0.9970,  F1W: 0.9970\n",
      "Val -   Loss: 0.5438, Acc: 0.9758, F1Mac: 0.8892, F1Mic: 0.9758,  F1W: 0.9756\n",
      "LR: 0.05, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9758, Scheduler Params: 0.01, 50, None, Train_Acc: 0.9970, Val_F1: 0.8930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:17<00:00, 10.38s/it]\n",
      " 67%|██████▋   | 16/24 [4:35:48<2:18:25, 1038.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0075, Acc: 0.9885,  F1Mac: 0.9590, F1Mic: 0.9885,  F1W: 0.9887\n",
      "Val -   Loss: 0.3820, Acc: 0.9723, F1Mac: 0.8777, F1Mic: 0.9723,  F1W: 0.9729\n",
      "LR: 0.05, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9723, Scheduler Params: 0.01, 50, StepLR, Train_Acc: 0.9885, Val_F1: 0.8786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:13<00:00, 10.33s/it]\n",
      " 71%|███████   | 17/24 [4:53:01<2:00:57, 1036.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0175, Acc: 0.9856,  F1Mac: 0.9265, F1Mic: 0.9856,  F1W: 0.9875\n",
      "Val -   Loss: 0.7835, Acc: 0.9496, F1Mac: 0.8044, F1Mic: 0.9496,  F1W: 0.9524\n",
      "LR: 0.1, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9496, Scheduler Params: 0.1, 25, None, Train_Acc: 0.9856, Val_F1: 0.8841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:13<00:00, 10.34s/it]\n",
      " 75%|███████▌  | 18/24 [5:10:15<1:43:35, 1035.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0071, Acc: 0.9894,  F1Mac: 0.9635, F1Mic: 0.9894,  F1W: 0.9896\n",
      "Val -   Loss: 0.4321, Acc: 0.9715, F1Mac: 0.8731, F1Mic: 0.9715,  F1W: 0.9721\n",
      "LR: 0.1, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9715, Scheduler Params: 0.1, 25, StepLR, Train_Acc: 0.9894, Val_F1: 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:18<00:00, 10.38s/it]\n",
      " 79%|███████▉  | 19/24 [5:27:33<1:26:23, 1036.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0071, Acc: 0.9964,  F1Mac: 0.9830, F1Mic: 0.9964,  F1W: 0.9965\n",
      "Val -   Loss: 0.7935, Acc: 0.9638, F1Mac: 0.8465, F1Mic: 0.9638,  F1W: 0.9634\n",
      "LR: 0.1, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9638, Scheduler Params: 0.1, 50, None, Train_Acc: 0.9964, Val_F1: 0.8824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:15<00:00, 10.35s/it]\n",
      " 83%|████████▎ | 20/24 [5:44:49<1:09:04, 1036.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0026, Acc: 0.9977,  F1Mac: 0.9899, F1Mic: 0.9977,  F1W: 0.9977\n",
      "Val -   Loss: 0.5741, Acc: 0.9759, F1Mac: 0.8898, F1Mic: 0.9759,  F1W: 0.9757\n",
      "LR: 0.1, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9759, Scheduler Params: 0.1, 50, StepLR, Train_Acc: 0.9977, Val_F1: 0.8914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:11<00:00, 10.32s/it]\n",
      " 88%|████████▊ | 21/24 [6:02:00<51:44, 1034.84s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0059, Acc: 0.9958,  F1Mac: 0.9836, F1Mic: 0.9958,  F1W: 0.9959\n",
      "Val -   Loss: 0.7358, Acc: 0.9656, F1Mac: 0.8572, F1Mic: 0.9656,  F1W: 0.9653\n",
      "LR: 0.1, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9656, Scheduler Params: 0.01, 25, None, Train_Acc: 0.9958, Val_F1: 0.8882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:12<00:00, 10.32s/it]\n",
      " 92%|█████████▏| 22/24 [6:19:12<34:28, 1034.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0108, Acc: 0.9858,  F1Mac: 0.9497, F1Mic: 0.9858,  F1W: 0.9861\n",
      "Val -   Loss: 0.3964, Acc: 0.9688, F1Mac: 0.8606, F1Mic: 0.9688,  F1W: 0.9698\n",
      "LR: 0.1, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9688, Scheduler Params: 0.01, 25, StepLR, Train_Acc: 0.9858, Val_F1: 0.8635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:14<00:00, 10.34s/it]\n",
      " 96%|█████████▌| 23/24 [6:36:27<17:14, 1034.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0075, Acc: 0.9972,  F1Mac: 0.9898, F1Mic: 0.9972,  F1W: 0.9973\n",
      "Val -   Loss: 0.8958, Acc: 0.9640, F1Mac: 0.8569, F1Mic: 0.9640,  F1W: 0.9631\n",
      "LR: 0.1, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9640, Scheduler Params: 0.01, 50, None, Train_Acc: 0.9972, Val_F1: 0.8860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:10<00:00, 10.31s/it]\n",
      "100%|██████████| 24/24 [6:53:38<00:00, 1034.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "Train - Loss: 0.0041, Acc: 0.9958,  F1Mac: 0.9825, F1Mic: 0.9958,  F1W: 0.9958\n",
      "Val -   Loss: 0.5586, Acc: 0.9738, F1Mac: 0.8782, F1Mic: 0.9738,  F1W: 0.9738\n",
      "LR: 0.1, Batch: 32, Patience: 100, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9738, Scheduler Params: 0.01, 50, StepLR, Train_Acc: 0.9958, Val_F1: 0.8867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# def write_params(params, str):\n",
    "#     f = open(str, 'w')\n",
    "#     f.write(repr(params) + '\\n')\n",
    "#     f.close()\n",
    "\n",
    "\n",
    "# def pad_collate(batch):\n",
    "#     (xx, yy) = zip(*batch)\n",
    "#     x_lens = [len(x) for x in xx]\n",
    "    \n",
    "#     xx_pad = pad_sequence(xx, batch_first=True, padding_value=word_vocab.get('<PAD>', 0))\n",
    "#     yy_pad = pad_sequence(yy, batch_first=True, padding_value=-1)\n",
    "    \n",
    "#     return xx_pad, yy_pad, x_lens\n",
    "\n",
    "# def get_class_weights(file_paths, tag_vocab):\n",
    "#     tag_counts = Counter()\n",
    "\n",
    "#     for file_path in file_paths:\n",
    "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#             for line in f:\n",
    "#                 parts = line.strip().split()\n",
    "#                 if len(parts) == 3:\n",
    "#                     _, _, tag = parts\n",
    "#                     if tag in tag_vocab:  # Count only tags present in tag_vocab\n",
    "#                         tag_counts[tag] += 1\n",
    "\n",
    "#     # Total number of tags to normalize tag frequencies\n",
    "#     total_tags = sum(tag_counts.values())\n",
    "    \n",
    "#     # Calculate regular class weights (proportional to frequency)\n",
    "#     regular_weights = {tag: (count / total_tags) for tag, count in tag_counts.items()}\n",
    "\n",
    "#     # Calculate inverse class weights (inversely proportional to frequency)\n",
    "#     inv_weights = {tag: (total_tags / (count + 1e-9)) for tag, count in tag_counts.items()}\n",
    "\n",
    "#     # Initialize weights tensors based on tag_vocab ordering\n",
    "#     num_tags = len(tag_vocab)\n",
    "#     regular_weights_tensor = torch.zeros(num_tags, dtype=torch.float)\n",
    "#     inv_weights_tensor = torch.zeros(num_tags, dtype=torch.float)\n",
    "\n",
    "#     # Populate the tensors according to the tag_vocab indices\n",
    "#     for tag, idx in tag_vocab.items():\n",
    "#         regular_weights_tensor[idx] = regular_weights.get(tag, 0)\n",
    "#         inv_weights_tensor[idx] = inv_weights.get(tag, 0)\n",
    "\n",
    "#     return regular_weights_tensor, inv_weights_tensor\n",
    "\n",
    "\n",
    "# def evaluate_model(model, dataloader, criterion, device):\n",
    "#     \"\"\"\n",
    "#     Evaluates the model on the given dataloader.\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     all_preds, all_labels = [], []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels, _ in dataloader:\n",
    "#             # (batch sz, seq len) \n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             # print(torch.max(outputs, dim=2))\n",
    "#             loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
    "#             # print(outputs.view(-1, outputs.shape[-1]).shape,  labels.view(-1).shape)\n",
    "#             total_loss += loss.item()\n",
    "#             _, preds = torch.max(outputs, dim=2)\n",
    "#             all_preds.extend(preds.view(-1).cpu().numpy())\n",
    "#             all_labels.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "#     valid_indices = [i for i, label in enumerate(all_labels) if label != -1]\n",
    "#     valid_preds = [all_preds[i] for i in valid_indices]\n",
    "#     valid_labels = [all_labels[i] for i in valid_indices]\n",
    "\n",
    "#     accuracy = np.mean(np.array(valid_preds) == np.array(valid_labels))\n",
    "#     f1_mac, f1_mic, f1_weight = compute_metrics(valid_preds, valid_labels)\n",
    "\n",
    "#     return total_loss / len(dataloader), accuracy, f1_mac, f1_mic, f1_weight\n",
    "\n",
    "\n",
    "# def train_model(model, dataloaders, optimizer, criterion, device,  num_epochs = 50, patience = 10, scheduler = None, ckpt_name = 'best_model.pth'):\n",
    "#     best_val_f1 = -float('inf')\n",
    "#     patience_counter = 0\n",
    "#     model.to(device)\n",
    "\n",
    "#     for epoch in tqdm( range(num_epochs) ):\n",
    "#         model.train()\n",
    "#         for inputs, labels, _ in dataloaders['train']:\n",
    "\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "\n",
    "#             a_max, a_indx = torch.max(outputs, dim = 2)\n",
    "#             # print(torch.max(outputs, dim=2))\n",
    "#             # print('predition shape ',outputs.shape, 'label shape', labels.shape)\n",
    "#             # print('predition',outputs, 'label', labels) \n",
    "#             loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step()\n",
    "\n",
    "#             # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        \n",
    "#         train_loss, train_acc, train_f1_mac, train_f1_mic, train_f1_weighted = evaluate_model(model, dataloaders['train'], criterion, device)\n",
    "#         val_loss, val_acc, val_f1_mac, val_f1_mic, val_f1_weighted = evaluate_model(model, dataloaders['dev'], criterion, device)\n",
    "\n",
    "#         # if epoch % 5 == 0: \n",
    "#         #     print(f'Epoch {epoch+1}:')\n",
    "#         #     print(f'Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f},  F1Mac: {train_f1_mac:.4f}, F1Mic: {train_f1_mic:.4f},  F1W: {train_f1_weighted:.4f}')\n",
    "#         #     print(f'Val -   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1Mac: {val_f1_mac:.4f}, F1Mic: {val_f1_mic:.4f},  F1W: {val_f1_weighted:.4f}')\n",
    "\n",
    "            \n",
    "#         # Early stopping based on validation F1 score\n",
    "#         if val_f1_mac > best_val_f1:\n",
    "#             best_val_f1 = val_f1_mac\n",
    "#             patience_counter = 0\n",
    "#             torch.save(model.state_dict(), ckpt_name + str(best_val_f1))\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "#             if patience_counter >= patience:\n",
    "#                 print(\"Early stopping triggered.\")\n",
    "#                 break\n",
    "#         if epoch > 3 and best_val_f1 < 0.45:\n",
    "#             print('very poor model')\n",
    "#             break\n",
    "#     ### after saving location of model in ckpt_name\n",
    "#         #checkpoint = torch.load(ckpt_name)\n",
    "#         # model.load_state_dict(checkpoint)\n",
    "#     ### and run lower eval, retrieve predictions => return dev prediction file path custom/glove,\n",
    "#         # file_paths = ['../../data/lstm-data/dev']#, '../../data/lstm-data/dev']\n",
    "#         # word_vocab, _ = build_vocab(data_files)\n",
    "#         # output_paths = get_eval_preds(model, file_paths, word_vocab)\n",
    "#     #  the run   ### evaluate_fb1_model(preds_file_path, gold_file_path)\n",
    "#         # evaluate_fb1_model(output_paths, gold_file_path = '../../data/lstm-data/dev')\n",
    "#     print(f'Epoch {epoch+1}:')\n",
    "#     print(f'Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f},  F1Mac: {train_f1_mac:.4f}, F1Mic: {train_f1_mic:.4f},  F1W: {train_f1_weighted:.4f}')\n",
    "#     print(f'Val -   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1Mac: {val_f1_mac:.4f}, F1Mic: {val_f1_mic:.4f},  F1W: {val_f1_weighted:.4f}')\n",
    "\n",
    "#     return val_acc, train_acc, best_val_f1\n",
    "\n",
    "\n",
    "# def grid_search(tag_vocab, dim, model_init, datasets, param_grid, class_weights, device):\n",
    "#     max_f1 = -np.inf\n",
    "#     best_params_f1 = None\n",
    "#     all_combinations = list(product(*param_grid.values()))\n",
    "    \n",
    "#     for combination in tqdm(all_combinations):\n",
    "#         lr, optimizer_class, criterion_class, epochs, batch_size, patience, momentum, weight_decay, gamma, step_size, scheduler_type = combination\n",
    "#         model = model_init(input_dim = dim[0], embedding_dim = 100, hidden_dim = 256, dropout = 0.33, glove_embeddings = glove_embeddings, output_dim = dim[1]).to(device)\n",
    "        \n",
    "#         dataloader_dict = {\n",
    "#             'train': DataLoader(datasets['train'], batch_size=batch_size, shuffle=True, collate_fn=pad_collate),\n",
    "#             'dev': DataLoader(datasets['dev'], batch_size=batch_size, shuffle=False, collate_fn=pad_collate)\n",
    "#         }\n",
    "\n",
    "#         if optimizer_class == optim.SGD:\n",
    "#             optimizer = optimizer_class(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "#         else:\n",
    "#             optimizer = optimizer_class(model.parameters(), lr=lr)  # Adjust accordingly for other optimizers\n",
    "        \n",
    "#         criterion = criterion_class(weight=class_weights, ignore_index=tag_vocab.get('<PAD>', -1)).to(device)\n",
    "        \n",
    "#         if scheduler_type == 'StepLR':\n",
    "#             scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "#         elif scheduler_type == 'ExponentialLR':\n",
    "#             scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "#         else:\n",
    "#             scheduler = None  # Handle other scheduler types or lack thereof\n",
    "\n",
    "#         # Call the training function\n",
    "#         val_accuracy, train_acc, best_val_f1 = train_model(\n",
    "#             model, dataloader_dict, optimizer, criterion, device, num_epochs = epochs, patience = patience, scheduler = scheduler, ckpt_name = 'custom_embedding_best_f1.pth',\n",
    "#         )\n",
    "        \n",
    "#         print(f\"LR: {lr}, Batch: {batch_size}, Patience: {patience}, Momentum: {momentum}, Weight_decay: {weight_decay}, Val_Acc: {val_accuracy:.4f}, Scheduler Params: {gamma}, {step_size}, {scheduler_type}, Train_Acc: {train_acc:.4f}, Val_F1: {best_val_f1:.4f}\")\n",
    "\n",
    "#         # Update best parameters based on F1 score\n",
    "#         if best_val_f1 > max_f1: \n",
    "#             max_f1 = best_val_f1\n",
    "#             best_params_f1 = {\n",
    "#                 'learning_rate': lr, \n",
    "#                 'optimizer': optimizer_class.__name__, \n",
    "#                 'criterion': criterion_class.__name__,\n",
    "#                 'epochs': epochs, \n",
    "#                 'batch_size': batch_size, \n",
    "#                 'patience': patience, \n",
    "#                 'momentum': momentum, \n",
    "#                 'weight_decay': weight_decay, \n",
    "#                 'gamma': gamma, \n",
    "#                 'step_size': step_size,\n",
    "#                 'scheduler_type': scheduler_type, \n",
    "#                 'validation_accuracy': val_accuracy, \n",
    "#                 'F1': best_val_f1, \n",
    "#                 'train_acc': train_acc\n",
    "#             }\n",
    "#             write_params(best_params_f1, 'max_f1.txt')\n",
    "\n",
    "\n",
    "#     return best_params_f1\n",
    "\n",
    "\n",
    "# data_files = [ '../../data/lstm-data/train', '../../data/lstm-data/dev']\n",
    "# word_vocab, tag_vocab = build_vocab(data_files)\n",
    "# dim = len(word_vocab), len(tag_vocab)\n",
    "# print('dim: ', dim)\n",
    "\n",
    "# dataset_dict = {\n",
    "#     'train': IndexedNERDataset('../../data/lstm-data/train', word_vocab, tag_vocab),\n",
    "#     'dev': IndexedNERDataset('../../data/lstm-data/dev', word_vocab, tag_vocab)\n",
    "# }\n",
    "\n",
    "\n",
    "# regular_class_weight, inv_class_weight = get_class_weights(data_files, tag_vocab)\n",
    "# regular_class_weight, inv_class_weight = regular_class_weight.to(device),  inv_class_weight.to(device)\n",
    "\n",
    "\n",
    "# print(f\"class weight: {regular_class_weight.shape},\\n INV: {inv_class_weight.shape}\")\n",
    "\n",
    "# # param_grid = {\n",
    "# #     'learning_rate': [1, 5e-1, 1e-1, 5e-2, 1e-2],\n",
    "# #     'optimizer': [optim.SGD], \n",
    "# #     'criterion': [torch.nn.CrossEntropyLoss], \n",
    "# #     'epochs': [50, 100],\n",
    "# #     'batch_size': [8, 16, 32, 64],\n",
    "# #     'patience': [10, 30],\n",
    "# #     'momentum': [0.9, 0.95, 0.99], \n",
    "# #     'weight_decay': [0, 1e-4, 1e-3], \n",
    "# #     'gamma': [1e-1, 1e-2, 1e-3], \n",
    "# #     'step_size': [30, 50, 70],  \n",
    "# #     'scheduler_type': ['ExponentialLR', 'StepLR', None], \n",
    "# # }\n",
    "# param_grid = {\n",
    "#     'learning_rate': [1e-2, 0.05, 1e-1],\n",
    "#     'optimizer': [optim.SGD], \n",
    "#     'criterion': [torch.nn.CrossEntropyLoss], \n",
    "#     'epochs': [100],\n",
    "#     'batch_size': [32],\n",
    "#     'patience': [100],\n",
    "#     'momentum': [0.9], \n",
    "#     'weight_decay': [1e-4], \n",
    "#     'gamma': [1e-1, 1e-2], \n",
    "#     'step_size': [25, 50],  \n",
    "#     'scheduler_type': [ None, 'StepLR'], \n",
    "# }\n",
    "\n",
    "\n",
    "# best_params_f1 = grid_search(tag_vocab, dim, model_init = BILSTMForNER, datasets = dataset_dict, param_grid = param_grid, class_weights = inv_class_weight, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping triggered.\n",
    "# Epoch 65:\n",
    "# Train - Loss: 0.0191, Acc: 0.9946,  F1Mac: 0.9692, F1Mic: 0.9946,  F1W: 0.9948\n",
    "# Val -   Loss: 0.6446, Acc: 0.9683, F1Mac: 0.8465, F1Mic: 0.9683,  F1W: 0.9690\n",
    "# LR: 0.09, Batch: 32, Patience: 15, Momentum: 0.9, Weight_decay: 0.0001, Val_Acc: 0.9683, Scheduler Params: 0.1, 25, None, Train_Acc: 0.9946, Val_F1: 0.8902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bonus task is to equip the BLSTM model in Task 2 with a CNN module\n",
    "to capture character-level information (see slides page 45 in lecture 12 for the\n",
    "network architecture). The character embedding dimension is set to 30. You\n",
    "need to tune other hyper-parameters of CNN module, such as the number of\n",
    "CNN layers, the kernel size and output dimension of each CNN layer. What\n",
    "are the precision, recall and F1 score on the dev data? Predicting the NER\n",
    "tags of the sentences in the test data and output the predictions in a file\n",
    "named pred, in the same format of training data. (hint: the bonus points are\n",
    "assigned based on the ranking of your model F1 score on the test data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
