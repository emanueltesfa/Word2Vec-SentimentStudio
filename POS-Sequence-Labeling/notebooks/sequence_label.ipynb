{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Vocabulary Creation (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the selected threshold for unknown words replacement? What is the total size of your\n",
    "vocabulary and what is the total occurrences of the special token ‘< unk >’\n",
    "after replacement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Index', 'Word', 'POS'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "n_threshold = 3\n",
    "train_vocab = defaultdict(int)\n",
    "\n",
    "vocab_df  = pd.read_csv('../../data/vocab-data/train', sep='\\t', skip_blank_lines = False, header = None)\n",
    "vocab_df.columns = ['Index', 'Word', 'POS']\n",
    "\n",
    "print(vocab_df.columns)\n",
    "\n",
    "# File importing\n",
    "with open('../../data/vocab-data/train', 'r') as tr_file:\n",
    "    Lines = tr_file.readlines()\n",
    "\n",
    "    # Create vocab\n",
    "    for line in Lines:\n",
    "        if line.strip():\n",
    "            word = re.split(r'\\t', line)[1]\n",
    "            cleaned_word = re.sub(r'\\W+', '', word)     \n",
    "\n",
    "        if word not in train_vocab:\n",
    "            train_vocab[cleaned_word] = 0\n",
    "        train_vocab[cleaned_word] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some word 'xyz' has frequency 3 and my threshold for categorizing as '<unk>' is 4. Then  we should add 3 to the frequency occurrence count of '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle <unk> tokens  \n",
    "unk_count = sum(v for k, v in train_vocab.items() if v <= n_threshold)\n",
    "new_vocab = {k: v for k, v in train_vocab.items() if v > n_threshold}\n",
    "new_vocab['<unk>'] = unk_count\n",
    "indexed_vocab = {word: (index, count) for index, (word, count) in enumerate(sorted(new_vocab.items(), key = lambda item: item[1], reverse=True), start = 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Writing\n",
    "f = open(\"../../data/outputs/train_vocab.txt\", \"a\")\n",
    "for k,v in indexed_vocab.items():\n",
    "    # word index count\n",
    "    new_line = f\"{k}\\t{v[0]}\\t{v[1]}\\n\"\n",
    "    f.write(new_line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Model with Emission & Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_counts = defaultdict(int)\n",
    "emission_counts = defaultdict(int)\n",
    "state_counts = defaultdict(int)\n",
    "\n",
    "# Open training data\n",
    "with open('../../data/vocab-data/train', 'r') as tr_file:\n",
    "    Lines = tr_file.readlines()\n",
    "    prev_state = None\n",
    "\n",
    "    # Process each line\n",
    "    for line in Lines:\n",
    "        line = line.strip()\n",
    "        parts = line.split('\\t')\n",
    "        \n",
    "        if len(parts) >= 2:\n",
    "            word, state = parts[1], parts[2]\n",
    "            cleaned_word = word # re.sub(r'\\W+', '', word)\n",
    "\n",
    "            # Emission and transition counts\n",
    "            emission_counts[(state, cleaned_word)] += 1\n",
    "            state_counts[state] += 1\n",
    "            if prev_state is not None:\n",
    "                transition_counts[(prev_state, state)] += 1\n",
    "            prev_state = state\n",
    "\n",
    "        else:\n",
    "            # Calculate emission probability of new sentence to a word type\n",
    "            word_type = '/n'\n",
    "            state = '<new_line>'\n",
    "            if prev_state is not None:\n",
    "                transition_counts[(prev_state, state)] += 1\n",
    "                state_counts[state] += 1\n",
    "\n",
    "            prev_state = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilities\n",
    "transition_probs = {k: v / state_counts[k[0]] for k, v in transition_counts.items()}\n",
    "emission_probs = {k: v / state_counts[k[0]] for k, v in emission_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM Model for JSON\n",
    "hmm_model = {\n",
    "    \"transition\": {f\"({k[0]},{k[1]})\": v for k, v in transition_probs.items()},\n",
    "    \"emission\": {f\"({k[0]},{k[1]})\": v for k, v in emission_probs.items()}\n",
    "}\n",
    "\n",
    "with open(\"../../data/outputs/hmm.json\", \"w\") as f:\n",
    "    json.dump(hmm_model, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1416\n",
      "50286\n"
     ]
    }
   ],
   "source": [
    "print(len(hmm_model['transition']))\n",
    "print(len(hmm_model['emission']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy HMM Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file_path = '../../data/outputs/greedy.out'\n",
    "# states = list(set([k.split(',')[0].strip('(') for k in hmm_model['transition'].keys()]))\n",
    "\n",
    "# with open('../../data/vocab-data/dev', 'r') as tr_file, open(output_file_path, 'w') as out_file:\n",
    "#     Lines = tr_file.readlines()\n",
    "#     max = -np.inf\n",
    "#     prev_state = '<new_line>'\n",
    "    \n",
    "#     for line in Lines:\n",
    "#         line = line.split('\\t')\n",
    "\n",
    "#         if len(line) == 3: # if not new line \n",
    "#             _, word, index = line[2].replace('\\n',''), line[1], line[0]\n",
    "#             prev_state = '<new_line>' if index == '1' else prev_state\n",
    "                \n",
    "#             for state in states:\n",
    "#                 trans_indexing = f'({prev_state},{state})'\n",
    "#                 emiss_indexing = f'({state},{word})'\n",
    "                \n",
    "#                 try:\n",
    "#                     trans = hmm_model['transition'][trans_indexing]\n",
    "#                     emiss = hmm_model['emission'][emiss_indexing]\n",
    "                    \n",
    "#                 except KeyError:\n",
    "#                     continue\n",
    "\n",
    "#                 s_prob = trans * emiss\n",
    "#                 if s_prob > max:\n",
    "#                     max = s_prob\n",
    "#                     optim_state = state\n",
    "#                     # print(f'Previous {prev_state}, State: {state}, word: {word}')\n",
    "#             out_file.write(f'{index}\\t{word}\\t{optim_state}\\n')\n",
    "#         else:\n",
    "#             out_file.write('\\n')\n",
    "\n",
    "                \n",
    "#         prev_state = state \n",
    "#         max = -np.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = '../../data/outputs/greedy.out'\n",
    "states = list(set([k.split(',')[0].strip('(') for k in hmm_model['transition'].keys()]))\n",
    "\n",
    "with open('../../data/vocab-data/dev', 'r') as tr_file, open(output_file_path, 'w') as out_file:\n",
    "    Lines = tr_file.readlines()\n",
    "    prev_state = '<new_line>'\n",
    "    \n",
    "    for line in Lines:\n",
    "        line = line.split('\\t')\n",
    "        max_prob = -np.inf  # Reset max for each new word\n",
    "\n",
    "        if len(line) == 3:  # if not new line \n",
    "            _, word, index = line[2].replace('\\n',''), line[1], line[0]\n",
    "            prev_state = '<new_line>' if index == '1' else prev_state\n",
    "                \n",
    "            for state in states:\n",
    "                trans_indexing = f'({prev_state},{state})'\n",
    "                emiss_indexing = f'({state},{word})'\n",
    "                # print(emiss_indexing)\n",
    "                try:\n",
    "                    trans = hmm_model['transition'][trans_indexing]\n",
    "                    emiss = hmm_model['emission'][emiss_indexing]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "                s_prob = trans * emiss\n",
    "                if s_prob > max_prob:\n",
    "\n",
    "                    max_prob = s_prob\n",
    "                    optim_state = state\n",
    "\n",
    "            out_file.write(f'{index}\\t{word}\\t{optim_state}\\n')\n",
    "            prev_state = optim_state  # Update prev_state correctly within the loop\n",
    "\n",
    "        else:\n",
    "            out_file.write('\\n')\n",
    "            prev_state = '<new_line>' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../data/vocab-data/dev', 'r') as tr_file, open(output_file_path, 'w') as out_file:\n",
    "#     Lines = tr_file.readlines()\n",
    "#     for line in Lines:\n",
    "#         line = line.split('\\t')\n",
    "#         if len(line) == 3:  # if not new line \n",
    "#             _, word, index = line[2].replace('\\n',''), line[1], line[0]\n",
    "#             out_file.write(f'{index}\\t{word}\\t{optim_state}\\n')\n",
    "#         else:\n",
    "#             out_file.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 131768, correct: 114991, accuracy: 87.27%\n"
     ]
    }
   ],
   "source": [
    "! python ../eval.py -p ../../data/outputs/greedy.out -g ../../data/vocab-data/dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python eval.py -p greedy.out -g dev\n",
    "# python POS-Sequence-Labeling\\eval.py -p ./data/outputs/greedy.out -g ./data/vocab-data/dev\n",
    "# There should be two greedy.out files, one for dev data, the other for test data. You need compute the accuracy on dev set and submit the greedy.out of test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #works well at first but stops working \n",
    "# def viterbi_decode(observations, states, hmm_model):\n",
    "#     num_obs = len(observations)\n",
    "#     num_states = len(states)\n",
    "#     print(f\"States: {states}\")\n",
    "    \n",
    "#     # Initialize the Viterbi table with zeros\n",
    "#     viterbi_table = [[0.0 for _ in range(num_states)] for _ in range(num_obs)]\n",
    "#     backpointer = [[0 for _ in range(num_states)] for _ in range(num_obs)]\n",
    "    \n",
    "#     default_probability = 0.00  # Small probability for missing transitions/emissions\n",
    "    \n",
    "#     # Initialize the first column of the table\n",
    "#     for s in tqdm(range(num_states), desc=\"Initializing first column\"):\n",
    "#         state = states[s]\n",
    "#         emiss_indexing = f'({state},{observations[0]})'\n",
    "#         emission_prob = hmm_model['emission'].get(emiss_indexing, default_probability)\n",
    "#         viterbi_table[0][s] = (1 / num_states) * emission_prob\n",
    "#         backpointer[0][s] = 0\n",
    "\n",
    "#     # Fill the Viterbi table\n",
    "#     for t in tqdm(range(1, num_obs), desc=\"Filling Viterbi table\"):\n",
    "#         for s in range(num_states):\n",
    "#             state = states[s]\n",
    "#             max_tr_prob = None\n",
    "#             prev_st_selected = 0\n",
    "\n",
    "#             for prev_st in range(num_states):\n",
    "#                 prev_state = states[prev_st]\n",
    "#                 trans_indexing = f'({prev_state},{state})'\n",
    "#                 trans_prob = hmm_model['transition'].get(trans_indexing, default_probability)\n",
    "\n",
    "#                 # if t == 36:  \n",
    "#                 #     print('trans indexing: ', trans_indexing, 'prob: ', trans_prob)\n",
    "\n",
    "#                 tr_prob = viterbi_table[t-1][prev_st] * trans_prob\n",
    "\n",
    "#                 if max_tr_prob is None or tr_prob > max_tr_prob:\n",
    "#                     max_tr_prob = tr_prob\n",
    "#                     prev_st_selected = prev_st\n",
    "            \n",
    "#             emiss_indexing = f'({state},{observations[t]})'\n",
    "#             emission_prob = hmm_model['emission'].get(emiss_indexing, default_probability)\n",
    "#             # if t == 36:  \n",
    "#             #     print('emiss indexing : ', emiss_indexing, 'prob: ', emission_prob)\n",
    "\n",
    "#             max_prob = max_tr_prob * emission_prob\n",
    "#             viterbi_table[t][s] = max_prob\n",
    "#             backpointer[t][s] = prev_st_selected\n",
    "   \n",
    "#     # The final most probable state sequence\n",
    "#     best_path = []\n",
    "#     max_prob = max(viterbi_table[-1])\n",
    "#     last_state = viterbi_table[-1].index(max_prob)\n",
    "#     best_path.append(states[last_state])\n",
    "\n",
    "#     # Trace back\n",
    "#     for t in tqdm(range(num_obs - 1, 0, -1), desc=\"Backtracking\"):\n",
    "#         last_state = backpointer[t][last_state]\n",
    "#         best_path.insert(0, states[last_state])\n",
    "\n",
    "#     return best_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../data/outputs/hmm.json') as model_file:\n",
    "#     hmm_model = json.load(model_file)\n",
    "\n",
    "# # Extract states and start probabilities if available; otherwise, initialize uniformly\n",
    "# states = list(set([k.split(',')[0].strip('(') for k in hmm_model['transition'].keys()]))\n",
    "# start_probabilities = {state: 1/len(states) for state in states}  # Uniform start probabilities\n",
    "\n",
    "\n",
    "# with open('../../data/vocab-data/dev', 'r') as dev_file:\n",
    "#     observations = []\n",
    "#     for line in dev_file:\n",
    "#         if line.strip():  # If the line is not empty\n",
    "#             observations.append(line.strip().split('\\t')[1])\n",
    "#         else:\n",
    "#             break\n",
    "\n",
    "# # states = ['NNP', 'CD', 'NNS', 'JJ', 'MD', 'VB'] \n",
    "# # observations = ['Pierre', 'Vinken' ] \n",
    "\n",
    "# # Since start_probabilities are not provided, assume uniform distribution\n",
    "# output_file_path = '../../data/outputs/viterbi.out'\n",
    "# predicted_tags = viterbi_decode(observations, states, hmm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makes first 100 predictions then stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setnence:  ['The', 'Arizona', 'Corporations', 'Commission', 'authorized', 'an', '11.5', '%', 'rate', 'increase', 'at', 'Tucson', 'Electric', 'Power', 'Co.', ',', 'substantially', 'lower', 'than', 'recommended', 'last', 'month', 'by', 'a', 'commission', 'hearing', 'officer', 'and', 'barely', 'half', 'the', 'rise', 'sought', 'by', 'the', 'utility', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:00, 62.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5527it [01:21, 67.47it/s]\n"
     ]
    }
   ],
   "source": [
    "def viterbi_decode(observations, states, hmm_model, output_file_path):\n",
    "    num_obs = len(observations)\n",
    "    num_states = len(states)\n",
    "    viterbi_table = [[0.0 for _ in range(num_states)] for _ in range(num_obs)]\n",
    "    backpointer = [[0 for _ in range(num_states)] for _ in range(num_obs)]\n",
    "    \n",
    "    default_probability = 0.0\n",
    "    \n",
    "    # Initialize the first column of the Viterbi table\n",
    "    for s in range(num_states):\n",
    "        state = states[s]\n",
    "        emiss_indexing = f'({state},{observations[0]})' if observations[0] != '\\n' else None\n",
    "        emission_prob = hmm_model['emission'].get(emiss_indexing, default_probability) if emiss_indexing else default_probability\n",
    "        viterbi_table[0][s] = emission_prob\n",
    "        backpointer[0][s] = 0\n",
    "    \n",
    "    # Fill the Viterbi table\n",
    "    for t in range(1, num_obs): #, desc=\"Filling Viterbi table\"):\n",
    "        for s in range(num_states):\n",
    "            state = states[s]\n",
    "            max_tr_prob = None\n",
    "            prev_st_selected = 0\n",
    "            for prev_st in range(num_states):\n",
    "                prev_state = states[prev_st]\n",
    "                trans_indexing = f'({prev_state},{state})'\n",
    "                tr_prob = viterbi_table[t-1][prev_st] * hmm_model['transition'].get(trans_indexing, default_probability)\n",
    "                if max_tr_prob is None or tr_prob > max_tr_prob:\n",
    "                    max_tr_prob = tr_prob\n",
    "                    prev_st_selected = prev_st\n",
    "                           \n",
    "\n",
    "            emiss_indexing = f'({state},{observations[t]})'\n",
    "            emission_prob = hmm_model['emission'].get(emiss_indexing, default_probability)\n",
    "            max_prob = max_tr_prob * emission_prob\n",
    "            \n",
    "            viterbi_table[t][s] = max_prob\n",
    "            backpointer[t][s] = prev_st_selected\n",
    "\n",
    "            assert max_tr_prob is not None, f\"Max transition probability not found for t={t}, state={state}\"\n",
    "            # assert emission_prob > 0, f\"Emission probability is zero or very low for t={t}, state={state}, observation={observations[t]}\"\n",
    "     \n",
    "    \n",
    "    # Decode the best path from back to front\n",
    "    best_path = []\n",
    "    max_prob = max(viterbi_table[-1])\n",
    "    last_state = viterbi_table[-1].index(max_prob)\n",
    "    best_path.append(states[last_state])\n",
    "\n",
    "    for t in range(num_obs - 2, -1, -1): #, desc=\"Backtracking\"):\n",
    "        last_state = backpointer[t+1][last_state]\n",
    "        best_path.insert(0, states[last_state])\n",
    "\n",
    "    return best_path\n",
    "\n",
    "states_list = list(set([k.split(',')[0].strip('(') for k in hmm_model['transition'].keys()]))\n",
    "\n",
    "with open('../../data/outputs/hmm.json') as model_file:\n",
    "    hmm_model = json.load(model_file)\n",
    "\n",
    "def extract_sentences_from_dev_file(file_path):\n",
    "    sentences = [] \n",
    "    current_sentence = []  \n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip():  \n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) > 0:\n",
    "                    word = parts[1]  \n",
    "                    current_sentence.append(word)\n",
    "            else: \n",
    "                if current_sentence:  \n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []  \n",
    "        \n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Usage example\n",
    "file_path = '../../data/vocab-data/dev' \n",
    "sentences = extract_sentences_from_dev_file(file_path)\n",
    "print('setnence: ', sentences[0])\n",
    "opt_seq = []\n",
    "for i, sentence in tqdm(enumerate(sentences)):\n",
    "    path = viterbi_decode(observations = sentences[i] , states = states_list, hmm_model = hmm_model, output_file_path = '../../data/outputs/viterbi.out' )\n",
    "    opt_seq.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_viterbi_output(dev_file_path, predictions, output_file_path):\n",
    "    with open(dev_file_path, 'r') as dev_file, open(output_file_path, 'w') as out_file:\n",
    "        prediction_index = 0  # separate predictions counter \n",
    "        for line in dev_file:\n",
    "            if line.strip():\n",
    "                index, word, _ = line.strip().split('\\t')\n",
    "                if prediction_index < len(predictions) and predictions[prediction_index]:\n",
    "                    tag = predictions[prediction_index].pop(0)\n",
    "                    out_file.write(f'{index}\\t{word}\\t{tag}\\n')\n",
    "            else:\n",
    "                out_file.write('\\n')\n",
    "                prediction_index += 1  \n",
    "\n",
    "\n",
    "dev_file_path = '../../data/vocab-data/dev'  # Update this to your dev file path\n",
    "output_file_path='./viterbi_new.out'\n",
    "write_viterbi_output(dev_file_path, opt_seq, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 131768, correct: 18131, accuracy: 13.76%\n"
     ]
    }
   ],
   "source": [
    "! python ../eval.py -p viterbi_new.out -g ../../data/vocab-data/dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../data/outputs/hmm.json') as model_file:\n",
    "#     hmm_model = json.load(model_file)\n",
    "\n",
    "# # Extract states and start probabilities if available; otherwise, initialize uniformly\n",
    "# states = list(set([k.split(',')[0].strip('(') for k in hmm_model['transition'].keys()]))\n",
    "# start_probabilities = {state: 1/len(states) for state in states}  # Uniform start probabilities\n",
    "\n",
    "\n",
    "# with open('../../data/vocab-data/dev', 'r') as dev_file:\n",
    "#     observations = []\n",
    "#     for line in dev_file:\n",
    "#         if line.strip():  # If the line is not empty\n",
    "#             observations.append(line.strip().split('\\t')[1])\n",
    "#         else:  # If the line is empty (sentence boundary)\n",
    "#             observations.append('\\n')  # Append a newline character to represent sentence boundaries\n",
    "\n",
    "# # states = ['NNP', 'CD', 'NNS', 'JJ', 'MD', 'VB'] \n",
    "# # observations = ['Pierre', 'Vinken' ] \n",
    "\n",
    "# # Since start_probabilities are not provided, assume uniform distribution\n",
    "# output_file_path = '../../data/outputs/viterbi.out'\n",
    "# predicted_tags = viterbi_decode(observations, states, hmm_model, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\amant\\OneDrive\\Desktop\\Projects\\NLP\\NLP-HW2\\POS-Sequence-Labeling\\eval.py\", line 36, in <module>\n",
      "    pline = pf_lines[pf_count]\n",
      "            ~~~~~~~~^^^^^^^^^^\n",
      "IndexError: list index out of range\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\amant\\OneDrive\\Desktop\\Projects\\NLP\\NLP-HW2\\POS-Sequence-Labeling\\eval.py\", line 58, in <module>\n",
      "    print(repr(gline), repr(pline), total)\n",
      "                            ^^^^^\n",
      "NameError: name 'pline' is not defined. Did you mean: 'gline'?\n"
     ]
    }
   ],
   "source": [
    "! python ../eval.py -p ../../data/outputs/viterbi.out -g ../../data/vocab-data/dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# def viterbi_decode(observations, states, hmm_model, output_file_path):\n",
    "#     # Initialize the Viterbi table with log probabilities\n",
    "#     t = hmm_model[\"transition\"]\n",
    "#     e = hmm_model[\"emission\"]\n",
    "#     start = \"<start>\"\n",
    "#     n_obs = len(observations)\n",
    "#     n_states = len(states)\n",
    "#     pi = pd.DataFrame(-np.inf, index=np.arange(n_obs), columns=states)\n",
    "#     backpointer = pd.DataFrame(None, index=np.arange(n_obs), columns=states)\n",
    "\n",
    "#     log_default_probability = np.log(1e-4)  # Slightly higher default probability\n",
    "\n",
    "#     # Initialize the first column of the Viterbi table\n",
    "#     for s in states:\n",
    "#         t_s = t.get(f\"({start},{s})\", 1e-4)\n",
    "#         e_x1_s = e.get(f\"({s},{observations[0]})\", 1e-4)\n",
    "#         pi.at[0, s] = np.log(t_s) + np.log(e_x1_s)\n",
    "\n",
    "#     # Fill the Viterbi table\n",
    "#     for j in range(1, n_obs):\n",
    "#         for s in states:\n",
    "#             max_log_prob = -np.inf\n",
    "#             max_prev_st = None\n",
    "#             for prev_st in states:\n",
    "#                 t_s_sp = t.get(f\"({prev_st},{s})\", 1e-4)\n",
    "#                 e_x_s = e.get(f\"({s},{observations[j]})\", 1e-4)\n",
    "#                 log_prob = pi.at[j-1, prev_st] + np.log(t_s_sp) + np.log(e_x_s)\n",
    "#                 if log_prob > max_log_prob:\n",
    "#                     max_log_prob = log_prob\n",
    "#                     max_prev_st = prev_st\n",
    "#             pi.at[j, s] = max_log_prob if max_log_prob != -np.inf else log_default_probability\n",
    "#             backpointer.at[j, s] = max_prev_st\n",
    "\n",
    "#     # Backtrack to find the best path\n",
    "#     best_path = []\n",
    "#     last_state = pi.idxmax(axis=1).iloc[-1]\n",
    "#     for j in range(n_obs - 1, -1, -1):\n",
    "#         best_path.insert(0, last_state)\n",
    "#         last_state = backpointer.at[j, last_state]\n",
    "\n",
    "#     # # Write the best path to the output file\n",
    "#     with open(output_file_path, 'w') as out_file:\n",
    "#         for i, (word, state) in enumerate(zip(observations, best_path)):\n",
    "#             # Check if state is None, this can happen if backpointer is not updated properly\n",
    "#             if state is '<new_line>':\n",
    "#                 state = 'UNK'  # UNK for unknown state\n",
    "#             out_file.write(f\"{i}\\t{word}\\t{state}\\n\")\n",
    "\n",
    "#     return best_path\n",
    "\n",
    "# # Example usage:\n",
    "# # states_list = list(hmm_model['transitions'].keys())\n",
    "# # observations = ['The', 'quick', 'brown', 'fox', ...]  # Your list of observations\n",
    "# # path = viterbi_decode(observations, states_list, hmm_model, 'viterbi.out')\n",
    "\n",
    "# with open('../../data/outputs/hmm.json') as model_file:\n",
    "#     hmm_model = json.load(model_file)\n",
    "\n",
    "# # Extract states and start probabilities if available; otherwise, initialize uniformly\n",
    "# states = list(set([k.split(',')[0].strip('(') for k in hmm_model['transition'].keys()]))\n",
    "# start_probabilities = {state: 1/len(states) for state in states}  # Uniform start probabilities\n",
    "\n",
    "\n",
    "# with open('../../data/vocab-data/dev', 'r') as dev_file:\n",
    "#     observations = []\n",
    "#     for line in dev_file:\n",
    "#         if line.strip():  # If the line is not empty\n",
    "#             observations.append(line.strip().split('\\t')[1])\n",
    "\n",
    "# # states = ['NNP', 'CD', 'NNS', 'JJ', 'MD', 'VB'] \n",
    "# # observations = ['Pierre', 'Vinken' ] \n",
    "\n",
    "# # Since start_probabilities are not provided, assume uniform distribution\n",
    "# output_file_path = '../../data/outputs/viterbi.out'\n",
    "# predicted_tags = viterbi_decode(observations[:100], states, hmm_model, output_file_path='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_file_path = '../../data/vocab-data/dev'  # Update this to your dev file path\n",
    "# output_file_path='./viterbi_new.out'\n",
    "\n",
    "# with open(dev_file_path, 'r') as dev_file, open(output_file_path, 'w') as out_file:\n",
    "#     prediction_index = 0  # Initialize a separate counter for predictions\n",
    "#     for line in dev_file:\n",
    "#         if line.strip():  # If the line is not empty\n",
    "#             index, word, _ = line.strip().split('\\t')\n",
    "#             print(prediction_index)\n",
    "#             try:\n",
    "#                 if predicted_tags[prediction_index] == '<new_line>':\n",
    "#                     print('enter')\n",
    "#                     prediction_index += 1\n",
    "#             except:\n",
    "#                 print('idk')\n",
    "#             if prediction_index < len(predicted_tags):  # Check to avoid index out of range\n",
    "#                 tag = predicted_tags[prediction_index]\n",
    "#                 out_file.write(f'{index}\\t{word}\\t{tag}\\n')\n",
    "#                 prediction_index += 1  # Increment only if a prediction was written\n",
    "#             else:\n",
    "#                 break\n",
    "#         else:\n",
    "#             out_file.write('\\n')  # Preserve sentence boundaries without incrementing prediction_index\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 131768, correct: 18131, accuracy: 13.76%\n"
     ]
    }
   ],
   "source": [
    "! python ../eval.py -p viterbi_new.out -g ../../data/vocab-data/dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
