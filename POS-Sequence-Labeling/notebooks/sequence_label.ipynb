{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Vocabulary Creation (20 points)\n",
    "create a vocabulary using the training data. In HMM,one important problem when creating the vocabulary is to handle unknown words. One simple solution is to replace rare words whose occurrences are less than a threshold (e.g. 3) with a special token ‘< unk >’. Task. Creating a vocabulary using the training data in the file train and\n",
    "output the vocabulary into a txt file named vocab.txt.\n",
    "\n",
    " The format of the\n",
    "vocabulary file is that each line contains a word type, its index in\n",
    "the vocabulary and its occurrences, separated by the tab symbol\n",
    "‘\\t’. The first line should be the special token ‘< unk >’ and the\n",
    "following lines should be sorted by its occurrences in descending 1 order. Note that we can only use the training data to create the vocabulary, without touching the development and test data. What is the selected\n",
    "threshold for unknown words replacement? What is the total size of your\n",
    "vocabulary and what is the total occurrences of the special token ‘< unk >’\n",
    "after replacement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_threshold = 3\n",
    "train_vocab = {}\n",
    "\n",
    "# File importing\n",
    "tr_file = open('../../data/vocab-data/train', 'r')\n",
    "Lines = tr_file.readlines()\n",
    " \n",
    "# Create vocab\n",
    "for line in Lines:\n",
    "    if line.strip():\n",
    "        # print(line)\n",
    "        word = re.split(r'\\t', line)[1]\n",
    "        cleaned_word = re.sub(r'\\W+', '', word)     \n",
    "\n",
    "    if word not in train_vocab:\n",
    "        train_vocab[cleaned_word] = 0\n",
    "\n",
    "    train_vocab[cleaned_word] += 1\n",
    "\n",
    "# Handle <unk> tokens  \n",
    "unk_count = sum(v for k, v in train_vocab.items() if v <= n_threshold)\n",
    "new_vocab = {k: v for k, v in train_vocab.items() if v > n_threshold}\n",
    "new_vocab['<unk>'] = unk_count\n",
    "indexed_vocab = {word: (index, count) for index, (word, count) in enumerate(sorted(new_vocab.items(), key = lambda item: item[1], reverse=True), start = 1)}\n",
    "\n",
    "# File Writing\n",
    "f = open(\"../../data/hmm/train_vocab.txt\", \"a\")\n",
    "for k,v in indexed_vocab.items():\n",
    "    # word index count\n",
    "    new_line = f\"{k}\\t{v[0]}\\t{v[1]}\\n\"\n",
    "    f.write(new_line)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second task is to learn an HMM from the training data. Remember that\n",
    "the solution of the emission and transition parameters in HMM are in the\n",
    "following formulation:\n",
    "\n",
    "t(s′|s) = count(s→s′)\n",
    "count(s)\n",
    "e(x|s) = count(s→x)\n",
    "count(s)\n",
    "\n",
    "where t(·|·) is the transition parameter and e(·|·) is the emission parameter.\n",
    "Task. Learning a model using the training data in the file train and output\n",
    "the learned model into a model file in json format, named hmm.json. The\n",
    "model file should contains two dictionaries for the emission and transition\n",
    "parameters, respectively. The first dictionary, named transition, contains\n",
    "items with pairs of (s, s′) as key and t(s′|s) as value. The second dictionary,\n",
    "named emission, contains items with pairs of (s, x) as key and e(x|s) as value.\n",
    "How many transition and emission parameters in your HMM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_threshold = 1\n",
    "train_vocab = defaultdict(int)\n",
    "transition_counts = defaultdict(int)\n",
    "emission_counts = defaultdict(int)\n",
    "state_counts = defaultdict(int)\n",
    "\n",
    "# Open training data\n",
    "with open('../../data/vocab-data/train', 'r') as tr_file:\n",
    "    Lines = tr_file.readlines()\n",
    "    prev_state = None\n",
    "\n",
    "    # Process each line\n",
    "    for line in Lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                word, state = parts[1], parts[2]\n",
    "                cleaned_word = re.sub(r'\\W+', '', word)\n",
    "                train_vocab[cleaned_word] += 1\n",
    "                \n",
    "                # Emission and transition counts\n",
    "                emission_counts[(state, cleaned_word)] += 1\n",
    "                state_counts[state] += 1\n",
    "                if prev_state is not None:\n",
    "                    transition_counts[(prev_state, state)] += 1\n",
    "                prev_state = state\n",
    "        else:\n",
    "            prev_state = None  # Reset at the end of a sentence\n",
    "\n",
    "# Adjust vocab for <unk>\n",
    "unk_count = sum(count for word, count in train_vocab.items() if count <= n_threshold)\n",
    "filtered_vocab = {word: count for word, count in train_vocab.items() if count > n_threshold}\n",
    "filtered_vocab['<unk>'] = unk_count\n",
    "\n",
    "# Calculate probabilities\n",
    "transition_probs = {k: v / state_counts[k[0]] for k, v in transition_counts.items()}\n",
    "emission_probs = {k: v / state_counts[k[0]] for k, v in emission_counts.items()}\n",
    "\n",
    "# HMM Model for JSON\n",
    "hmm_model = {\n",
    "    \"transition\": {f\"{k[0]},{k[1]}\": v for k, v in transition_probs.items()},\n",
    "    \"emission\": {f\"{k[0]},{k[1]}\": v for k, v in emission_probs.items()}\n",
    "}\n",
    "\n",
    "\n",
    "with open(\"../../data/hmm/hmm.json\", \"w\") as f:\n",
    "    json.dump(hmm_model, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third task is to implement the greedy decoding algorithm with HMM.\n",
    "Task. Implementing the greedy decoding algorithm and evaluate it on the\n",
    "development data. What is the accuracy on the dev data? Predicting the\n",
    "part-of-speech tags of the sentences in the test data and output the predic-\n",
    "tions in a file named greedy.out, in the same format of training data.\n",
    "We also provide an evaluation script eval.py to evaluate the results of the\n",
    "model. To use the script, you need to prepare your prediction file in the same\n",
    "format as the training data, then execute the command line:\n",
    "python eval.py −p {predicted file} −g {gold-standard file}\n",
    "2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth task is to implement the viterbi decoding algorithm with HMM.\n",
    "Task. Implementing the viterbi decoding algorithm and evaluate it on the\n",
    "development data. What is the accuracy on the dev data? Predicting the\n",
    "part-of-speech tags of the sentences in the test data and output the predic-\n",
    "tions in a file named viterbi.out, in the same format of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
