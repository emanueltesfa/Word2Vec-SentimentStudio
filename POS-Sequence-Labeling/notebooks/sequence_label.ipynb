{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "\n",
    "from __future__ import annotations\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "from typing import Any, List\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Vocabulary Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the selected threshold for unknown words replacement? What is the total size of your\n",
    "vocabulary and what is the total occurrences of the special token ‘< unk >’\n",
    "after replacement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Index', 'Word', 'POS'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "n_threshold = 3\n",
    "train_vocab = defaultdict(int)\n",
    "\n",
    "vocab_df  = pd.read_csv('../../data/vocab-data/train', sep='\\t', skip_blank_lines = False, header = None)\n",
    "vocab_df.columns = ['Index', 'Word', 'POS']\n",
    "\n",
    "print(vocab_df.columns)\n",
    "\n",
    "# File importing\n",
    "with open('../../data/vocab-data/train', 'r') as tr_file:\n",
    "    Lines = tr_file.readlines()\n",
    "\n",
    "    # Create vocab\n",
    "    for line in Lines:\n",
    "        if line.strip():\n",
    "            word = re.split(r'\\t', line)[1]\n",
    "            cleaned_word = re.sub(r'\\W+', '', word)     \n",
    "\n",
    "        if word not in train_vocab:\n",
    "            train_vocab[cleaned_word] = 0\n",
    "        train_vocab[cleaned_word] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some word 'xyz' has frequency 3 and my threshold for categorizing as '<unk>' is 4. Then  we should add 3 to the frequency occurrence count of '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle <unk> tokens  \n",
    "unk_count = sum(v for k, v in train_vocab.items() if v <= n_threshold)\n",
    "new_vocab = {k: v for k, v in train_vocab.items() if v > n_threshold}\n",
    "new_vocab['<unk>'] = unk_count\n",
    "indexed_vocab = {word: (index, count) for index, (word, count) in enumerate(sorted(new_vocab.items(), key = lambda item: item[1], reverse=True), start = 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Writing\n",
    "with open(\"../../data/outputs/train_vocab.txt\", \"w\") as f:\n",
    "    for k,v in indexed_vocab.items():\n",
    "        # word index count\n",
    "        new_line = f\"{k}\\t{v[0]}\\t{v[1]}\\n\"\n",
    "        f.write(new_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 12405\n",
      "Total occurrences of words: 76948215\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 0\n",
    "total_occurrences = 0\n",
    "\n",
    "with open('../../data/outputs/train_vocab.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('\\t')  # Or split by space if that's the delimiter\n",
    "        if len(parts) >= 2:\n",
    "            vocab_size += 1\n",
    "            total_occurrences += int(parts[1])\n",
    "\n",
    "print(f\"Total vocabulary size: {vocab_size}\")\n",
    "print(f\"Total occurrences of words: {total_occurrences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Model with Emission & Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_frequencies(vocab_counts_file):\n",
    "    word_frequencies = {}\n",
    "    with open(vocab_counts_file, 'r') as file:\n",
    "        for line in file:\n",
    "            word, _, count = line.strip().split('\\t')\n",
    "            word_frequencies[word] = int(count)\n",
    "    return word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sequences_from_file(file_path: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Extracts word and tag sequences from a file.\n",
    "\n",
    "    This function reads a file where each line represents a word followed by its tag,\n",
    "    separated by tabs, and sentences are separated by double newlines. It processes\n",
    "    the file to extract sequences of words and their corresponding tags, handling\n",
    "    and reporting lines that do not conform to the expected format.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str - The path to the file from which word and tag sequences will be extracted.\n",
    "\n",
    "    Returns:\n",
    "    - tuple(list[list[str]], list[list[str]]): A tuple containing two lists:\n",
    "        - The first list contains sequences of words.\n",
    "        - The second list contains sequences of corresponding tags for those words.\n",
    "\n",
    "    Note:\n",
    "    - The function prints an error message for each line that does not contain exactly three parts\n",
    "      separated by tabs, but it continues processing the rest of the file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read().rstrip('\\n')  # Ensure no trailing newline\n",
    "\n",
    "    sentences = content.split('\\n\\n')\n",
    "\n",
    "    word_sequences, tag_sequences = [], []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words, tags = [], []\n",
    "        lines = sentence.split('\\n')\n",
    "        for line in lines:\n",
    "            try:\n",
    "                _, word, tag = line.split('\\t')\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "            except ValueError:\n",
    "                print(f\"Error processing line: {line}\")\n",
    "                continue \n",
    "\n",
    "        word_sequences.append(words)\n",
    "        tag_sequences.append(tags)\n",
    "\n",
    "    return word_sequences, tag_sequences\n",
    "\n",
    "\n",
    "class TransformData:\n",
    "    \"\"\"\n",
    "    A base class for data transformation operations.\n",
    "\n",
    "    This class serves as a foundation for transformations that require a fitting step before\n",
    "    applying a transformation. It defines a generic `fit_transform` method that chains the\n",
    "    `fit` and `transform` methods, allowing subclasses to implement specific logic for these\n",
    "    operations.\n",
    "\n",
    "    Methods:\n",
    "    - fit_transform: Applies both the fitting and transforming operations sequentially.\n",
    "    \"\"\"\n",
    "    def fit_transform(self, *args, **kwargs):\n",
    "        self.fit(*args, **kwargs)\n",
    "        return self.transform(*args, **kwargs)\n",
    "\n",
    "@dataclass\n",
    "class UnkTransform(TransformData):\n",
    "\n",
    "    \"\"\"\n",
    "    A transformation that replaces infrequent words with a special token.\n",
    "\n",
    "    This class implements a transformation for sequences of words where words that occur\n",
    "    less frequently than a specified threshold are replaced with a predefined special token.\n",
    "    It first fits to the data to find the frequency of each word and then applies the transformation\n",
    "    based on the threshold.\n",
    "\n",
    "    Attributes:\n",
    "    - threshold (int): The minimum frequency a word must have to not be replaced.\n",
    "    - special_token (str): The token used to replace infrequent words.\n",
    "\n",
    "    Methods:\n",
    "    - fit: Calculates the frequency of each word in the provided sequences.\n",
    "    - transform: Replaces infrequent words in the sequences with the special token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threshold: int, special_token: str):\n",
    "        self.threshold = threshold\n",
    "        self.special_token = special_token\n",
    "        self.words = None\n",
    "\n",
    "    def fit(self, sequences: list[list[str]], *_):\n",
    "        frequency = Counter(chain(*sequences)) \n",
    "        self.words = {word: count for word, count in frequency.items() if count >= self.threshold}\n",
    "        return self\n",
    "\n",
    "    def transform(self, sequences: list[list[str]], *_):\n",
    "        return [\n",
    "            [\n",
    "                (word if word in self.words else self.special_token)\n",
    "                for word in sequence\n",
    "            ]\n",
    "            for sequence in sequences\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilization\n",
    "vocab_counts_file = '../../data/outputs/train_vocab.txt'  \n",
    "word_frequencies = load_word_frequencies(vocab_counts_file)\n",
    "\n",
    "\n",
    "word_sequences, tag_sequences = extract_sequences_from_file('../../data/vocab-data/train')\n",
    "transformer = UnkTransform(3, '<unk>')\n",
    "word_sequences_t = transformer.fit_transform(word_sequences)\n",
    "counts = Counter(itertools.chain(*tag_sequences))\n",
    "emission_temp = [list(zip(y, x)) for (y, x) in zip(tag_sequences, word_sequences_t)]\n",
    "emission_counts = Counter(itertools.chain(*emission_temp))\n",
    "prior_counts = Counter([sequence[0] for sequence in tag_sequences])\n",
    "bigrams = [list(zip(y, y[1:])) for y in tag_sequences]\n",
    "transition_counts = Counter(chain(*bigrams))\n",
    "\n",
    "states = set(counts.keys())\n",
    "observations = set(Counter(itertools.chain(*word_sequences_t)).keys())\n",
    "N = len(tag_sequences)\n",
    "\n",
    "\n",
    "transition_probs = defaultdict(int)\n",
    "emission_probs = defaultdict(int)\n",
    "state_probs = defaultdict(int)\n",
    "\n",
    "# Prior Probabilities\n",
    "state_probs = {s: prior_counts[s] / N for s in states}\n",
    "\n",
    "# Transition probabilities\n",
    "for s in states:\n",
    "    for s_ in states:\n",
    "        transition_probs[s, s_] = transition_counts[s, s_] / counts[s]\n",
    "\n",
    "# Emission probabilities\n",
    "for o in observations:\n",
    "    for s in states:\n",
    "        emission_probs[s, o] = emission_counts[s, o] / counts[s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM Model for JSON\n",
    "hmm_model = {\n",
    "    \"transition\": {f\"({k[0]},{k[1]})\": v for k, v in transition_probs.items()},\n",
    "    \"emission\": {f\"({k[0]},{k[1]})\": v for k, v in emission_probs.items()}\n",
    "}\n",
    "\n",
    "with open(\"../../data/outputs/hmm.json\", \"w\") as f:\n",
    "    json.dump(hmm_model, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n",
      "761400\n"
     ]
    }
   ],
   "source": [
    "print(len(hmm_model['transition']))\n",
    "print(len(hmm_model['emission']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy HMM Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_greedy(sentence: List[str], state_probs: defaultdict, transition_probs: defaultdict, emission_probs: defaultdict ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs greedy decoding on a given sentence to find the most probable state path.\n",
    "\n",
    "    This function applies a greedy decoding algorithm over a sequence of words (sentence) using\n",
    "    given Hidden Markov Model (HMM) parameters: initial state probabilities, transition probabilities,\n",
    "    and emission probabilities. It starts by selecting the state with the highest probability for the\n",
    "    first word based on state and emission probabilities. For subsequent words, it selects the state\n",
    "    that has the highest transition probability from the previous state, multiplied by the emission\n",
    "    probability of the word for that state.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence: List[str] - A list of words representing the sentence to decode.\n",
    "    - state_probs: defaultdict - A dictionary containing the initial state probabilities.\n",
    "    - transition_probs: defaultdict - A dictionary containing state-to-state transition probabilities.\n",
    "    - emission_probs: defaultdict - A dictionary containing state-to-word emission probabilities.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of states representing the most probable path through the states for the given sentence.\n",
    "\n",
    "    Note: This function assumes that 'states' is a predefined list of all possible states in the HMM.\n",
    "          It performs decoding based on a greedy strategy, potentially not always resulting in the globally optimal state sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    path = []\n",
    "    word = sentence[0]\n",
    "    max_state = None\n",
    "    max_prob = 0\n",
    "\n",
    "    for s in states:\n",
    "        prob = state_probs[s] * emission_probs[s, word]\n",
    "        # print(prob)\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            max_state = s\n",
    "    \n",
    "    path.append(max_state)\n",
    "    prev_state = max_state\n",
    "        \n",
    "\n",
    "    for i, word in enumerate(sentence[1:]):\n",
    "        max_state = None\n",
    "        max_prob = 0\n",
    "        for s in states:\n",
    "            prob = transition_probs[prev_state, s] * emission_probs[s, word]\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                max_state = s\n",
    "        path.append(max_state)\n",
    "        prev_state = max_state\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_to_file(filepath: str, word_sequences: list[list[str]], tag_sequences: list[list[str]]):\n",
    "    \n",
    "    \"\"\"\n",
    "    Writes word sequences and their corresponding tag sequences to a file.\n",
    "\n",
    "    This function takes a filepath and two lists of lists: word_sequences and tag_sequences. \n",
    "    Each inner list in word_sequences corresponds to a sequence of words, and each inner list in \n",
    "    tag_sequences contains the tags associated with the words in the corresponding word sequence. \n",
    "    The function writes these sequences to the specified file, with each word/tag pair separated by a tab \n",
    "    and each sequence separated by a newline. The function ensures that each word sequence has a \n",
    "    corresponding tag sequence of the same length and raises a ValueError if this is not the case.\n",
    "\n",
    "    Parameters:\n",
    "    - filepath: str - The path to the file where the word and tag sequences will be written.\n",
    "    - word_sequences: list[list[str]] - A list of lists, where each inner list contains a sequence of words.\n",
    "    - tag_sequences: list[list[str]] - A list of lists, where each inner list contains the tags corresponding to the words in the same position of word_sequences.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If any word sequence and its corresponding tag sequence do not have the same length.\n",
    "\n",
    "    The output file will have the following format:\n",
    "    1   word1   tag1\n",
    "    2   word2   tag2\n",
    "    ...\n",
    "\n",
    "\n",
    "    Note: The function does not return any value.\n",
    "    \"\"\"\n",
    "\n",
    "    result_str = \"\"\n",
    "    \n",
    "    for seq_index in range(len(word_sequences)):\n",
    "        word_seq = word_sequences[seq_index]\n",
    "        tag_seq = tag_sequences[seq_index]\n",
    "        \n",
    "        if len(word_seq) != len(tag_seq):\n",
    "            raise ValueError(\"Word sequence and tag sequence lengths do not match.\")\n",
    "        \n",
    "        for pair_index in range(len(word_seq)):\n",
    "            word = word_seq[pair_index]\n",
    "            tag = tag_seq[pair_index]\n",
    "            result_str += f\"{pair_index + 1}\\t{word}\\t{tag}\\n\"\n",
    "        \n",
    "        if seq_index < len(word_sequences) - 1:\n",
    "            result_str += \"\\n\"\n",
    "    \n",
    "    # Write the result string to the specified file\n",
    "    with open(filepath, 'w') as file:\n",
    "        file.write(result_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev predictions writing and greedy call\n",
    "word_sequences_dev, tag_sequences_dev = extract_sequences_from_file('../../data/vocab-data/dev')\n",
    "word_sequences_dev_t = transformer.transform(word_sequences_dev)\n",
    "pred_tagsequence = [\n",
    "    decode_greedy(word_sequences_dev_t[i], state_probs, transition_probs, emission_probs)\n",
    "    for i in range(len(word_sequences_dev_t))\n",
    "]\n",
    "prediction_to_file('../../data/outputs/greedy_dev.out', word_sequences_dev_t, pred_tagsequence)\n",
    "\n",
    "\n",
    "# test predictions writing and greedy call \n",
    "word_sequences_test, tag_sequences_test = extract_sequences_from_file('../../data/vocab-data/test')\n",
    "word_sequences_test_t = transformer.transform(word_sequences_test)\n",
    "pred_tagsequence = [\n",
    "    decode_greedy(word_sequences_test_t[i], state_probs, transition_probs, emission_probs)\n",
    "    for i in range(len(word_sequences_test_t))\n",
    "]\n",
    "prediction_to_file('../../data/outputs/greedy_test.out', word_sequences_test_t, pred_tagsequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 131768, correct: 122119, accuracy: 92.68%\n"
     ]
    }
   ],
   "source": [
    "!python ../eval.py -p ../../data/outputs/greedy_dev.out -g ../../data/vocab-data/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VITERBI HMM DECODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5527/5527 [01:30<00:00, 60.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 131768, correct: 124333, accuracy: 94.36%\n"
     ]
    }
   ],
   "source": [
    "# Citation: https://en.wikipedia.org/wiki/Viterbi_algorithm\n",
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    # Initialize the dynamic programming table\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "    \n",
    "    # Initialize base cases (t == 0)\n",
    "    for st in states:\n",
    "        V[0][st] = start_p[st] * emit_p[st, obs[0]]\n",
    "        path[st] = [st]\n",
    "    \n",
    "    # Run Viterbi for t > 0\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        newpath = {}\n",
    "        \n",
    "        for st in states:\n",
    "            (max_prob, state) = max(\n",
    "                (V[t-1][prev_st] * trans_p[prev_st, st] * emit_p[st, obs[t]], prev_st) \n",
    "                for prev_st in states)\n",
    "            V[t][st] = max_prob\n",
    "            newpath[st] = path[state] + [st]\n",
    "        \n",
    "        path = newpath\n",
    "    \n",
    "    # Build the output\n",
    "    (max_prob, state) = max((V[len(obs) - 1][st], st) for st in states)\n",
    "    return path[state], max_prob\n",
    "\n",
    "def dptable(V):\n",
    "    # Print a table of steps from dictionary\n",
    "    yield \"    \" + \" \".join(f\"{i:5d}\" for i in range(len(V)))\n",
    "    for state in V[0]:\n",
    "        yield f\"{state}: \" + \" \".join(f\"{v[state]:.5f}\" for v in V)\n",
    "\n",
    "\n",
    "\n",
    "word_sequences_dev, tag_sequences_dev = extract_sequences_from_file('../../data/vocab-data/dev')\n",
    "word_sequences_dev_t = transformer.transform(word_sequences_dev)\n",
    "pred_tagsequence = [\n",
    "    viterbi(word_sequences_dev_t[i], states, state_probs, transition_probs, emission_probs)[0]\n",
    "    for i in tqdm(range(len(word_sequences_dev_t)))\n",
    "]\n",
    "prediction_to_file('../../data/outputs/viterbi_dev.out', word_sequences_dev_t, pred_tagsequence)\n",
    "\n",
    "word_sequences_test, tag_sequences_test = extract_sequences_from_file('../../data/vocab-data/test')\n",
    "word_sequences_test_t = transformer.transform(word_sequences_test)\n",
    "pred_tagsequence = [\n",
    "    viterbi(word_sequences_test_t[i], S, state_probs, transition_probs, emission_probs)[0]\n",
    "    for i in tqdm(range(len(word_sequences_test_t)))\n",
    "]\n",
    "prediction_to_file('../../data/outputs/viterbi_test.out', word_sequences_test_t, pred_tagsequence)\n",
    "\n",
    "## ACC OF DEV FILE NOT TEST\n",
    "!python ../eval.py -p ../../data/outputs/viterbi_dev.out -g ../../data/vocab-data/dev "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
