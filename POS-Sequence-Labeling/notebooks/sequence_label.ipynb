{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Vocabulary Creation (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the selected threshold for unknown words replacement? What is the total size of your\n",
    "vocabulary and what is the total occurrences of the special token ‘< unk >’\n",
    "after replacement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Index', 'Word', 'POS'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "n_threshold = 3\n",
    "train_vocab = defaultdict(int)\n",
    "\n",
    "vocab_df  = pd.read_csv('../../data/vocab-data/train', sep='\\t', skip_blank_lines = False, header = None)\n",
    "vocab_df.columns = ['Index', 'Word', 'POS']\n",
    "\n",
    "print(vocab_df.columns)\n",
    "\n",
    "# File importing\n",
    "with open('../../data/vocab-data/train', 'r') as tr_file:\n",
    "    Lines = tr_file.readlines()\n",
    "\n",
    "    # Create vocab\n",
    "    for line in Lines:\n",
    "        if line.strip():\n",
    "            word = re.split(r'\\t', line)[1]\n",
    "            cleaned_word = re.sub(r'\\W+', '', word)     \n",
    "\n",
    "        if word not in train_vocab:\n",
    "            train_vocab[cleaned_word] = 0\n",
    "        train_vocab[cleaned_word] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some word 'xyz' has frequency 3 and my threshold for categorizing as '<unk>' is 4. Then  we should add 3 to the frequency occurrence count of '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle <unk> tokens  \n",
    "unk_count = sum(v for k, v in train_vocab.items() if v <= n_threshold)\n",
    "new_vocab = {k: v for k, v in train_vocab.items() if v > n_threshold}\n",
    "new_vocab['<unk>'] = unk_count\n",
    "indexed_vocab = {word: (index, count) for index, (word, count) in enumerate(sorted(new_vocab.items(), key = lambda item: item[1], reverse=True), start = 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Writing\n",
    "f = open(\"../../data/outputs/train_vocab.txt\", \"a\")\n",
    "for k,v in indexed_vocab.items():\n",
    "    # word index count\n",
    "    new_line = f\"{k}\\t{v[0]}\\t{v[1]}\\n\"\n",
    "    f.write(new_line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Model with Emission & Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_counts = defaultdict(int)\n",
    "emission_counts = defaultdict(int)\n",
    "state_counts = defaultdict(int)\n",
    "\n",
    "# Open training data\n",
    "with open('../../data/vocab-data/train', 'r') as tr_file:\n",
    "    Lines = tr_file.readlines()\n",
    "    prev_state = None\n",
    "\n",
    "    # Process each line\n",
    "    for line in Lines:\n",
    "        line = line.strip()\n",
    "        parts = line.split('\\t')\n",
    "        \n",
    "        if len(parts) >= 2:\n",
    "            word, state = parts[1], parts[2]\n",
    "            cleaned_word = word # re.sub(r'\\W+', '', word)\n",
    "\n",
    "            # Emission and transition counts\n",
    "            emission_counts[(state, cleaned_word)] += 1\n",
    "            state_counts[state] += 1\n",
    "            if prev_state is not None:\n",
    "                transition_counts[(prev_state, state)] += 1\n",
    "            prev_state = state\n",
    "\n",
    "        else:\n",
    "            # Calculate emission probability of new sentence to a word type\n",
    "            word_type = '/n'\n",
    "            state = '<new_line>'\n",
    "            if prev_state is not None:\n",
    "                transition_counts[(prev_state, state)] += 1\n",
    "                state_counts[state] += 1\n",
    "\n",
    "            prev_state = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilities\n",
    "transition_probs = {k: v / state_counts[k[0]] for k, v in transition_counts.items()}\n",
    "emission_probs = {k: v / state_counts[k[0]] for k, v in emission_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM Model for JSON\n",
    "hmm_model = {\n",
    "    \"transition\": {f\"({k[0]},{k[1]})\": v for k, v in transition_probs.items()},\n",
    "    \"emission\": {f\"({k[0]},{k[1]})\": v for k, v in emission_probs.items()}\n",
    "}\n",
    "\n",
    "with open(\"../../data/outputs/hmm.json\", \"w\") as f:\n",
    "    json.dump(hmm_model, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1416\n",
      "50286\n"
     ]
    }
   ],
   "source": [
    "print(len(hmm_model['transition']))\n",
    "print(len(hmm_model['emission']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy HMM Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file_path = '../../data/outputs/greedy.out'\n",
    "# states = list(set([k.split(',')[0].strip('(') for k in hmm_model['transition'].keys()]))\n",
    "\n",
    "# with open('../../data/vocab-data/dev', 'r') as tr_file, open(output_file_path, 'w') as out_file:\n",
    "#     Lines = tr_file.readlines()\n",
    "#     max = -np.inf\n",
    "#     prev_state = '<new_line>'\n",
    "    \n",
    "#     for line in Lines:\n",
    "#         line = line.split('\\t')\n",
    "\n",
    "#         if len(line) == 3: # if not new line \n",
    "#             _, word, index = line[2].replace('\\n',''), line[1], line[0]\n",
    "#             prev_state = '<new_line>' if index == '1' else prev_state\n",
    "                \n",
    "#             for state in states:\n",
    "#                 trans_indexing = f'({prev_state},{state})'\n",
    "#                 emiss_indexing = f'({state},{word})'\n",
    "                \n",
    "#                 try:\n",
    "#                     trans = hmm_model['transition'][trans_indexing]\n",
    "#                     emiss = hmm_model['emission'][emiss_indexing]\n",
    "                    \n",
    "#                 except KeyError:\n",
    "#                     continue\n",
    "\n",
    "#                 s_prob = trans * emiss\n",
    "#                 if s_prob > max:\n",
    "#                     max = s_prob\n",
    "#                     optim_state = state\n",
    "#                     # print(f'Previous {prev_state}, State: {state}, word: {word}')\n",
    "#             out_file.write(f'{index}\\t{word}\\t{optim_state}\\n')\n",
    "#         else:\n",
    "#             out_file.write('\\n')\n",
    "\n",
    "                \n",
    "#         prev_state = state \n",
    "#         max = -np.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = '../../data/outputs/greedy.out'\n",
    "states = list(set([k.split(',')[0].strip('(') for k in hmm_model['transition'].keys()]))\n",
    "\n",
    "with open('../../data/vocab-data/dev', 'r') as tr_file, open(output_file_path, 'w') as out_file:\n",
    "    Lines = tr_file.readlines()\n",
    "    prev_state = '<new_line>'\n",
    "    \n",
    "    for line in Lines:\n",
    "        line = line.split('\\t')\n",
    "        max = -np.inf  # Reset max for each new word\n",
    "\n",
    "        if len(line) == 3:  # if not new line \n",
    "            _, word, index = line[2].replace('\\n',''), line[1], line[0]\n",
    "            prev_state = '<new_line>' if index == '1' else prev_state\n",
    "                \n",
    "            for state in states:\n",
    "                trans_indexing = f'({prev_state},{state})'\n",
    "                emiss_indexing = f'({state},{word})'\n",
    "                \n",
    "                try:\n",
    "                    trans = hmm_model['transition'][trans_indexing]\n",
    "                    emiss = hmm_model['emission'][emiss_indexing]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "                s_prob = trans * emiss\n",
    "                if s_prob > max:\n",
    "                    max = s_prob\n",
    "                    optim_state = state\n",
    "\n",
    "            out_file.write(f'{index}\\t{word}\\t{optim_state}\\n')\n",
    "            prev_state = optim_state  # Update prev_state correctly within the loop\n",
    "\n",
    "        else:\n",
    "            out_file.write('\\n')\n",
    "            prev_state = '<new_line>'  # Reset prev_state for a new sentence/line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 131768, correct: 114991, accuracy: 87.27%\n"
     ]
    }
   ],
   "source": [
    "! python ../eval.py -p ../../data/outputs/greedy.out -g ../../data/vocab-data/dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python eval.py -p greedy.out -g dev\n",
    "# python POS-Sequence-Labeling\\eval.py -p ./data/outputs/greedy.out -g ./data/vocab-data/dev\n",
    "# There should be two greedy.out files, one for dev data, the other for test data. You need compute the accuracy on dev set and submit the greedy.out of test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131768 46\n",
      "{'': 0.021739130434782608, 'VB': 0.021739130434782608, 'WRB': 0.021739130434782608, 'CC': 0.021739130434782608, 'DT': 0.021739130434782608, '#': 0.021739130434782608, 'WDT': 0.021739130434782608, 'FW': 0.021739130434782608, 'MD': 0.021739130434782608, '.': 0.021739130434782608, 'VBG': 0.021739130434782608, \"''\": 0.021739130434782608, '<new_line>': 0.021739130434782608, 'PRP': 0.021739130434782608, 'SYM': 0.021739130434782608, 'EX': 0.021739130434782608, 'PDT': 0.021739130434782608, 'RB': 0.021739130434782608, 'NNS': 0.021739130434782608, ':': 0.021739130434782608, '``': 0.021739130434782608, 'VBZ': 0.021739130434782608, 'VBP': 0.021739130434782608, 'NN': 0.021739130434782608, '$': 0.021739130434782608, 'IN': 0.021739130434782608, 'NNPS': 0.021739130434782608, 'JJR': 0.021739130434782608, 'LS': 0.021739130434782608, 'JJ': 0.021739130434782608, 'RBR': 0.021739130434782608, '-LRB-': 0.021739130434782608, 'WP$': 0.021739130434782608, 'TO': 0.021739130434782608, 'RBS': 0.021739130434782608, 'NNP': 0.021739130434782608, 'VBD': 0.021739130434782608, 'CD': 0.021739130434782608, 'RP': 0.021739130434782608, '-RRB-': 0.021739130434782608, 'POS': 0.021739130434782608, 'PRP$': 0.021739130434782608, 'UH': 0.021739130434782608, 'VBN': 0.021739130434782608, 'WP': 0.021739130434782608, 'JJS': 0.021739130434782608}\n"
     ]
    }
   ],
   "source": [
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    pass\n",
    "\n",
    "\n",
    "with open('../../data/outputs/hmm.json') as model_file:\n",
    "    hmm_model = json.load(model_file)\n",
    "\n",
    "# Extract states and start probabilities if available; otherwise, initialize uniformly\n",
    "states = list(set([k.split(',')[0].strip('(') for k in hmm_model['transition'].keys()]))\n",
    "start_probabilities = {state: 1/len(states) for state in states}  # Uniform start probabilities\n",
    "\n",
    "\n",
    "with open('../../data/vocab-data/dev', 'r') as dev_file:\n",
    "    observations = [line.strip().split('\\t')[1] for line in dev_file if len(line.strip().split('\\t')) > 1]\n",
    "\n",
    "# states = ['NNP', 'CD', 'NNS', 'JJ', 'MD', 'VB']  # Update based on your model\n",
    "# observations = ['Pierre', 'Vinken'] \n",
    "\n",
    "print(len(observations), len(states))\n",
    "# Since start_probabilities are not provided, assume uniform distribution\n",
    "predicted_tags = viterbi(observations, states, start_probabilities, hmm_model['transition'], hmm_model['emission'])\n",
    "\n",
    "print(start_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
