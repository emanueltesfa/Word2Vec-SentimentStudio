{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Vocabulary Creation (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the selected threshold for unknown words replacement? What is the total size of your\n",
    "vocabulary and what is the total occurrences of the special token ‘< unk >’\n",
    "after replacement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Index', 'Word', 'POS'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "n_threshold = 2\n",
    "train_vocab = defaultdict(int)\n",
    "\n",
    "vocab_df  = pd.read_csv('../../data/vocab-data/train', sep='\\t', skip_blank_lines = False, header = None)\n",
    "vocab_df.columns = ['Index', 'Word', 'POS']\n",
    "\n",
    "print(vocab_df.columns)\n",
    "\n",
    "# File importing\n",
    "with open('../../data/vocab-data/train', 'r') as tr_file:\n",
    "    Lines = tr_file.readlines()\n",
    "\n",
    "    # Create vocab\n",
    "    for line in Lines:\n",
    "        if line.strip():\n",
    "            word = re.split(r'\\t', line)[1]\n",
    "            cleaned_word = re.sub(r'\\W+', '', word)     \n",
    "\n",
    "        if word not in train_vocab:\n",
    "            train_vocab[cleaned_word] = 0\n",
    "        train_vocab[cleaned_word] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some word 'xyz' has frequency 3 and my threshold for categorizing as '<unk>' is 4. Then  we should add 3 to the frequency occurrence count of '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle <unk> tokens  \n",
    "unk_count = sum(v for k, v in train_vocab.items() if v <= n_threshold)\n",
    "new_vocab = {k: v for k, v in train_vocab.items() if v > n_threshold}\n",
    "new_vocab['<unk>'] = unk_count\n",
    "indexed_vocab = {word: (index, count) for index, (word, count) in enumerate(sorted(new_vocab.items(), key = lambda item: item[1], reverse=True), start = 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Writing\n",
    "f = open(\"../../data/outputs/train_vocab.txt\", \"a\")\n",
    "for k,v in indexed_vocab.items():\n",
    "    # word index count\n",
    "    new_line = f\"{k}\\t{v[0]}\\t{v[1]}\\n\"\n",
    "    f.write(new_line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Model with Emission & Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_frequencies(vocab_counts_file):\n",
    "    word_frequencies = {}\n",
    "    with open(vocab_counts_file, 'r') as file:\n",
    "        for line in file:\n",
    "            word, _, count = line.strip().split('\\t')\n",
    "            word_frequencies[word] = int(count)\n",
    "    return word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_counts_file = '../../data/outputs/train_vocab.txt'  \n",
    "word_frequencies = load_word_frequencies(vocab_counts_file)\n",
    "\n",
    "transition_counts = defaultdict(int)\n",
    "emission_counts = defaultdict(int)\n",
    "state_counts = defaultdict(int)\n",
    "\n",
    "with open('../../data/vocab-data/train', 'r') as tr_file:\n",
    "    Lines = tr_file.readlines()\n",
    "    prev_state = None\n",
    "\n",
    "    # Process each line\n",
    "    for line in Lines:\n",
    "        line = line.strip()\n",
    "        parts = line.split('\\t')\n",
    "        \n",
    "        if len(parts) >= 2:\n",
    "            word, state = parts[1], parts[2]\n",
    "            # Replace the word with '<unk>' if its frequency is below the threshold\n",
    "            cleaned_word = word if word_frequencies.get(word, 0) >= n_threshold else '<unk>'\n",
    "\n",
    "            # Emission and transition counts\n",
    "            emission_counts[(state, cleaned_word)] += 1\n",
    "            state_counts[state] += 1\n",
    "            if prev_state is not None:\n",
    "                transition_counts[(prev_state, state)] += 1\n",
    "            prev_state = state\n",
    "\n",
    "        else:\n",
    "            word_type = '/n'\n",
    "            state = '<new_line>'\n",
    "            if prev_state is not None:\n",
    "                transition_counts[(prev_state, state)] += 1\n",
    "                state_counts[state] += 1\n",
    "\n",
    "            prev_state = state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilities\n",
    "transition_probs = {k: v / state_counts[k[0]] for k, v in transition_counts.items()}\n",
    "emission_probs = {k: v / state_counts[k[0]] for k, v in emission_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM Model for JSON\n",
    "hmm_model = {\n",
    "    \"transition\": {f\"({k[0]},{k[1]})\": v for k, v in transition_probs.items()},\n",
    "    \"emission\": {f\"({k[0]},{k[1]})\": v for k, v in emission_probs.items()}\n",
    "}\n",
    "\n",
    "with open(\"../../data/outputs/hmm.json\", \"w\") as f:\n",
    "    json.dump(hmm_model, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1416\n",
      "21219\n"
     ]
    }
   ],
   "source": [
    "print(len(hmm_model['transition']))\n",
    "print(len(hmm_model['emission']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy HMM Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = '../../data/outputs/greedy.out'\n",
    "states = list(set([k.split(',')[0].strip('(') for k in hmm_model['transition'].keys()]))\n",
    "\n",
    "with open('../../data/vocab-data/dev', 'r') as tr_file, open(output_file_path, 'w') as out_file:\n",
    "    Lines = tr_file.readlines()\n",
    "    prev_state = '<new_line>'\n",
    "    \n",
    "    for line in Lines:\n",
    "        line = line.split('\\t')\n",
    "        max_prob = -np.inf  # Reset max for each new word\n",
    "\n",
    "        if len(line) == 3:  # if not new line \n",
    "            _, word, index = line[2].replace('\\n',''), line[1], line[0]\n",
    "            word = word if word_frequencies.get(word, 0) >= n_threshold else '<unk>'\n",
    "\n",
    "            prev_state = '<new_line>' if index == '1' else prev_state\n",
    "                \n",
    "            for state in states:\n",
    "                trans_indexing = f'({prev_state},{state})'\n",
    "                emiss_indexing = f'({state},{word})'\n",
    "                # print(emiss_indexing)\n",
    "                try:\n",
    "                    trans = hmm_model['transition'][trans_indexing]\n",
    "                    emiss = hmm_model['emission'][emiss_indexing]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "                s_prob = trans * emiss\n",
    "                if s_prob > max_prob:\n",
    "\n",
    "                    max_prob = s_prob\n",
    "                    optim_state = state\n",
    "\n",
    "            out_file.write(f'{index}\\t{word}\\t{optim_state}\\n')\n",
    "            prev_state = optim_state  # Update prev_state correctly within the loop\n",
    "\n",
    "        else:\n",
    "            out_file.write('\\n')\n",
    "            prev_state = '<new_line>' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 131768, correct: 102741, accuracy: 77.97%\n"
     ]
    }
   ],
   "source": [
    "! python ../eval.py -p ../../data/outputs/greedy.out -g ../../data/vocab-data/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decode(observations, states, hmm_model, output_file_path):\n",
    "    num_obs = len(observations)\n",
    "    num_states = len(states)\n",
    "    viterbi_table = [[0.0 for _ in range(num_states)] for _ in range(num_obs)]\n",
    "    backpointer = [[0 for _ in range(num_states)] for _ in range(num_obs)]\n",
    "    \n",
    "    default_probability = 0.0\n",
    "    \n",
    "    # Initialize the first column of the Viterbi table\n",
    "    for s in range(num_states):\n",
    "        state = states[s]\n",
    "        emiss_indexing = f'({state},{observations[0]})' if observations[0] != '\\n' else None\n",
    "        emission_prob = hmm_model['emission'].get(emiss_indexing, default_probability) if emiss_indexing else default_probability\n",
    "        viterbi_table[0][s] = emission_prob\n",
    "        backpointer[0][s] = 0\n",
    "    \n",
    "    # Fill the Viterbi table\n",
    "    for t in range(1, num_obs): #, desc=\"Filling Viterbi table\"):\n",
    "        for s in range(num_states):\n",
    "            state = states[s]\n",
    "            max_tr_prob = None\n",
    "            prev_st_selected = 0\n",
    "            for prev_st in range(num_states):\n",
    "                prev_state = states[prev_st]\n",
    "                trans_indexing = f'({prev_state},{state})'\n",
    "                tr_prob = viterbi_table[t-1][prev_st] * hmm_model['transition'].get(trans_indexing, default_probability)\n",
    "                if max_tr_prob is None or tr_prob > max_tr_prob:\n",
    "                    max_tr_prob = tr_prob\n",
    "                    prev_st_selected = prev_st\n",
    "                           \n",
    "\n",
    "            emiss_indexing = f'({state},{observations[t]})'\n",
    "            emission_prob = hmm_model['emission'].get(emiss_indexing, default_probability)\n",
    "            max_prob = max_tr_prob * emission_prob\n",
    "            \n",
    "            viterbi_table[t][s] = max_prob\n",
    "            backpointer[t][s] = prev_st_selected\n",
    "\n",
    "            assert max_tr_prob is not None, f\"Max transition probability not found for t={t}, state={state}\"\n",
    "            # assert emission_prob > 0, f\"Emission probability is zero or very low for t={t}, state={state}, observation={observations[t]}\"\n",
    "     \n",
    "    \n",
    "    # Decode the best path from back to front\n",
    "    best_path = []\n",
    "    max_prob = max(viterbi_table[-1])\n",
    "    last_state = viterbi_table[-1].index(max_prob)\n",
    "    best_path.append(states[last_state])\n",
    "\n",
    "    for t in range(num_obs - 2, -1, -1): #, desc=\"Backtracking\"):\n",
    "        last_state = backpointer[t+1][last_state]\n",
    "        best_path.insert(0, states[last_state])\n",
    "\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_from_dev_file(file_path, word_freq, n_threshold = 2):\n",
    "    sentences = [] \n",
    "    current_sentence = []  \n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip():  \n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 2:  # Ensure we have both index and word in the line\n",
    "                    word = parts[1]\n",
    "                    # Replace the word with '<unk>' if its frequency is below the threshold\n",
    "                    cleaned_word = word if word_freq.get(word, 0) >= n_threshold else '<unk>'\n",
    "                    current_sentence.append(cleaned_word)\n",
    "            else: \n",
    "                if current_sentence:  \n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []  \n",
    "        \n",
    "        if current_sentence: # IF NOT NEW LINE\n",
    "            sentences.append(current_sentence)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5527it [01:06, 82.81it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('../../data/outputs/hmm.json') as model_file:\n",
    "    hmm_model = json.load(model_file)\n",
    "\n",
    "states_list = list(set([k.split(',')[0].strip('(') for k in hmm_model['transition'].keys()]))\n",
    "sentences = extract_sentences_from_dev_file('../../data/vocab-data/dev', word_freq = word_frequencies )\n",
    "\n",
    "opt_seq = []\n",
    "for i, sentence in tqdm(enumerate(sentences)):\n",
    "    path = viterbi_decode(observations = sentences[i] , states = states_list, hmm_model = hmm_model, output_file_path = '../../data/outputs/viterbi.out' )\n",
    "    opt_seq.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_viterbi_output(dev_file_path, predictions, output_file_path):\n",
    "    with open(dev_file_path, 'r') as dev_file, open(output_file_path, 'w') as out_file:\n",
    "        prediction_index = 0  # separate predictions counter \n",
    "        for line in dev_file:\n",
    "            if line.strip():\n",
    "                index, word, _ = line.strip().split('\\t')\n",
    "                if prediction_index < len(predictions) and predictions[prediction_index]:\n",
    "                    tag = predictions[prediction_index].pop(0)\n",
    "                    out_file.write(f'{index}\\t{word}\\t{tag}\\n')\n",
    "            else:\n",
    "                out_file.write('\\n')\n",
    "                prediction_index += 1  \n",
    "\n",
    "\n",
    "dev_file_path = '../../data/vocab-data/dev'  # Update this to your dev file path\n",
    "output_file_path='./viterbi_new.out'\n",
    "write_viterbi_output(dev_file_path, opt_seq, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 131768, correct: 107740, accuracy: 81.76%\n"
     ]
    }
   ],
   "source": [
    "! python ../eval.py -p viterbi_new.out -g ../../data/vocab-data/dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
