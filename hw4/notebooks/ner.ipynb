{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BILSTMForNER(nn.Module):\n",
    "#     def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, num_layers=1, dropout=0.33):\n",
    "#         super(BILSTMForNER, self).__init__()\n",
    "#         self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True, dropout=dropout if num_layers > 1 else 0)\n",
    "#         self.fc = nn.Linear(hidden_dim * 2, 128)  # Linear output dim is 128\n",
    "#         self.classifier = nn.Linear(128, output_dim)  # Adjusted to match the number of unique NER tags\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         embedded = self.dropout(self.embedding(x))  # Apply dropout to embeddings\n",
    "#         lstm_output, _ = self.lstm(embedded)\n",
    "#         out = self.fc(lstm_output)\n",
    "#         out = F.elu(out)  \n",
    "#         out = self.classifier(out)\n",
    "#         return out\n",
    "\n",
    "# def task1_collate(data: list[dict], use_targets: bool) -> dict[str, torch.Tensor]:\n",
    "#     word_encodings = [torch.Tensor(x['word_encodings']) for x in data]\n",
    "#     word_encodings = pad_sequence(word_encodings, batch_first=True)\n",
    "\n",
    "#     lengths = torch.IntTensor([len(x['word_encodings']) for x in data])\n",
    "\n",
    "#     # capital_mask = [torch.Tensor(x['capital_mask']) for x in data]\n",
    "#     # capital_mask = pad_sequence(capital_mask, batch_first = True)\n",
    "\n",
    "#     item: dict[str, torch.Tensor] = {\n",
    "#         'word_encodings': word_encodings,\n",
    "#         'lengths': lengths,\n",
    "#     }\n",
    "\n",
    "#     if use_targets: \n",
    "#         tag_encodings = [torch.Tensor(x['tag_encodings']) for x in data] # type: ignore\n",
    "#         tag_encodings = pad_sequence(tag_encodings, batch_first=True)\n",
    "#         item |= {'tag_encodings': tag_encodings}\n",
    "    \n",
    "#     return item\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Tuple, List \n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, labels):\n",
    "    \"\"\"\n",
    "    Computes the precision, recall, and F1 score from predictions and labels.\n",
    "    \n",
    "    Args:\n",
    "    - preds (list of int): The list of predictions.\n",
    "    - labels (list of int): The list of true labels.\n",
    "    \n",
    "    Returns:\n",
    "    - precision (float): The precision of the predictions.\n",
    "    - recall (float): The recall of the predictions.\n",
    "    - f1 (float): The F1 score of the predictions.\n",
    "    \"\"\"\n",
    "    precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def build_vocab(data_files):\n",
    "    word_vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    tag_vocab = {}\n",
    "    word_idx, tag_idx = 2, 0  \n",
    "\n",
    "    for file_path in data_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    _, word, tag = line.split()\n",
    "                    if word not in word_vocab:\n",
    "                        word_vocab[word] = word_idx\n",
    "                        word_idx += 1\n",
    "                    if tag not in tag_vocab:\n",
    "                        tag_vocab[tag] = tag_idx\n",
    "                        tag_idx += 1\n",
    "    return word_vocab, tag_vocab\n",
    "\n",
    "\n",
    "class BILSTMForNER(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, num_layers=1, dropout=0.33):\n",
    "        super(BILSTMForNER, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 128)  \n",
    "        self.classifier = nn.Linear(128, output_dim) \n",
    "        self.elu = nn.ELU() \n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.embedding(x) #batch, sequence, outdim \n",
    "        # print('post lstm: ', x.shape)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.elu(self.fc(lstm_out))  \n",
    "        logits = self.classifier(out)  \n",
    "        return logits\n",
    "\n",
    "# batc , seq, embeddim\n",
    "\n",
    "# class IndexedNERDataset(Dataset):\n",
    "#     def __init__(self, file_path, word_vocab, tag_vocab):\n",
    "#         self.word_vocab = word_vocab\n",
    "#         self.tag_vocab = tag_vocab\n",
    "#         self.data = []\n",
    "#         self._load_data(file_path)\n",
    "        \n",
    "#     def _load_data(self, file_path):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#             sentence, tags = [], []\n",
    "#             for line in f:\n",
    "#                 line = line.strip()\n",
    "#                 if line:\n",
    "#                     _, word, tag = line.split()\n",
    "#                     sentence.append(self.word_vocab.get(word, self.word_vocab['<UNK>']))\n",
    "#                     tags.append(self.tag_vocab[tag])\n",
    "#                 else:\n",
    "#                     self.data.append((sentence, tags))\n",
    "#                     sentence, tags = [], []\n",
    "#             if sentence and tags:\n",
    "#                 self.data.append((sentence, tags))\n",
    "                \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         sentence, tags = self.data[idx]\n",
    "#         return torch.tensor(sentence, dtype=torch.long), torch.tensor(tags, dtype=torch.long)\n",
    "\n",
    "class IndexedNERDataset(Dataset):\n",
    "    def __init__(self, file_path, word_vocab, tag_vocab=None, use_tags=True):\n",
    "        self.word_vocab = word_vocab\n",
    "        self.tag_vocab = tag_vocab if use_tags else None\n",
    "        self.use_tags = use_tags\n",
    "        self.data = []\n",
    "        self._load_data(file_path)\n",
    "        \n",
    "    def _load_data(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            sentence = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    if self.use_tags:\n",
    "                        _, word, tag = line.split()\n",
    "                        tag_idx = self.tag_vocab.get(tag, -1)  # or some other default index\n",
    "                    else:\n",
    "                        word = line\n",
    "                        tag_idx = -1  # Placeholder for test data without tags\n",
    "                    sentence.append((self.word_vocab.get(word, self.word_vocab['<UNK>']), tag_idx))\n",
    "                else:\n",
    "                    self.data.append(sentence)\n",
    "                    sentence = []\n",
    "            if sentence:  # Handle the case where the file doesn't end with a newline\n",
    "                self.data.append(sentence)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, tags = zip(*self.data[idx])\n",
    "        return torch.tensor(sentence, dtype=torch.long), torch.tensor(tags, dtype=torch.long)\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "    sentences, tags = zip(*batch)\n",
    "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=word_vocab['<PAD>'])\n",
    "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=-1)  # Use -1 or another unique index for padding in tags\n",
    "    return sentences_padded, tags_padded\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given dataloader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            # (batch sz, seq len) \n",
    "            # print('Val Input: ', inputs.shape)\n",
    "            # print('Val label: ', labels.shape)\n",
    "            # print('Val Input: ', inputs)\n",
    "            # print('Val label: ', labels)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # print(torch.max(outputs, dim=2))\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
    "            # print(outputs.view(-1, outputs.shape[-1]).shape,  labels.view(-1).shape)\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, dim=2)\n",
    "            all_preds.extend(preds.view(-1).cpu().numpy())\n",
    "            all_labels.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "    valid_indices = [i for i, label in enumerate(all_labels) if label != -1]\n",
    "    valid_preds = [all_preds[i] for i in valid_indices]\n",
    "    valid_labels = [all_labels[i] for i in valid_indices]\n",
    "\n",
    "    accuracy = np.mean(np.array(valid_preds) == np.array(valid_labels))\n",
    "    precision, recall, f1 = compute_metrics(valid_preds, valid_labels)\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "def train_model(model, dataloaders, optimizer, criterion, device, num_epochs=50, patience=10):\n",
    "    best_val_f1 = -float('inf')\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in dataloaders['train']:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            a_max, a_indx = torch.max(outputs, dim=2)\n",
    "            # print(torch.max(outputs, dim=2))\n",
    "            # print('predition shape ',outputs.shape, 'label shape', labels.shape)\n",
    "            # print('predition',outputs, 'label', labels)\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        if epoch % 2 == 0: \n",
    "            print('indxs predicted: ', a_indx)\n",
    "            \n",
    "        train_loss, train_acc, train_precision, train_recall, train_f1 = evaluate_model(model, dataloaders['train'], criterion, device)\n",
    "        val_loss, val_acc, val_precision, val_recall, val_f1 = evaluate_model(model, dataloaders['val'], criterion, device)\n",
    "\n",
    "        # print(f'Epoch {epoch+1}:')\n",
    "        print(f'Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}')\n",
    "        print(f'Val - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}')\n",
    "\n",
    "        # Early stopping based on validation F1 score\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim:  26886 output dim:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amant\\anaconda3\\envs\\pytorch_dl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.33 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indxs predicted:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Train - Loss: 0.6407, Acc: 0.8373, Precision: 0.7738, Recall: 0.8373, F1: 0.7684\n",
      "Val - Loss: 0.7238, Acc: 0.8362, Precision: 0.7540, Recall: 0.8362, F1: 0.7658\n",
      "Train - Loss: 0.5308, Acc: 0.8553, Precision: 0.7948, Recall: 0.8553, F1: 0.8066\n",
      "Val - Loss: 0.6027, Acc: 0.8520, Precision: 0.7911, Recall: 0.8520, F1: 0.8013\n",
      "indxs predicted:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [3, 4, 1, 5, 1, 1, 4, 4, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 0, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Train - Loss: 0.4324, Acc: 0.8752, Precision: 0.8418, Recall: 0.8752, F1: 0.8443\n",
      "Val - Loss: 0.5041, Acc: 0.8694, Precision: 0.8288, Recall: 0.8694, F1: 0.8369\n",
      "Train - Loss: 0.3784, Acc: 0.8882, Precision: 0.8649, Recall: 0.8882, F1: 0.8648\n",
      "Val - Loss: 0.4621, Acc: 0.8781, Precision: 0.8439, Recall: 0.8781, F1: 0.8519\n",
      "indxs predicted:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [5, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 5, 8, 1],\n",
      "        [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Train - Loss: 0.3315, Acc: 0.9048, Precision: 0.8995, Recall: 0.9048, F1: 0.8955\n",
      "Val - Loss: 0.4162, Acc: 0.8900, Precision: 0.8807, Recall: 0.8900, F1: 0.8786\n",
      "Train - Loss: 0.2812, Acc: 0.9166, Precision: 0.9088, Recall: 0.9166, F1: 0.9079\n",
      "Val - Loss: 0.3818, Acc: 0.8996, Precision: 0.8878, Recall: 0.8996, F1: 0.8881\n",
      "indxs predicted:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [3, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 4, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 8, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 6, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [3, 4, 1, 5, 8, 1, 1, 3, 4, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "Train - Loss: 0.2399, Acc: 0.9276, Precision: 0.9202, Recall: 0.9276, F1: 0.9209\n",
      "Val - Loss: 0.3566, Acc: 0.9077, Precision: 0.8969, Recall: 0.9077, F1: 0.8978\n",
      "Train - Loss: 0.2211, Acc: 0.9331, Precision: 0.9272, Recall: 0.9331, F1: 0.9267\n",
      "Val - Loss: 0.3521, Acc: 0.9108, Precision: 0.9011, Recall: 0.9108, F1: 0.9003\n",
      "indxs predicted:  tensor([[1, 3, 4, 1, 5, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [3, 4, 1, 5, 8, 1, 1, 1, 1, 3, 4, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [3, 4, 1, 5, 8, 1, 1, 3, 4, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [1, 0, 6, 6, 6, 6, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 2, 1, 1, 1],\n",
      "        [3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Train - Loss: 0.1737, Acc: 0.9500, Precision: 0.9495, Recall: 0.9500, F1: 0.9488\n",
      "Val - Loss: 0.3015, Acc: 0.9187, Precision: 0.9149, Recall: 0.9187, F1: 0.9154\n",
      "Train - Loss: 0.1431, Acc: 0.9583, Precision: 0.9571, Recall: 0.9583, F1: 0.9570\n",
      "Val - Loss: 0.2918, Acc: 0.9238, Precision: 0.9190, Recall: 0.9238, F1: 0.9199\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_files = ['../../data/lstm-data/train', '../../data/lstm-data/dev']\n",
    "word_vocab, tag_vocab = build_vocab(data_files)\n",
    "\n",
    "input_dim = len(word_vocab)  \n",
    "output_dim = len(tag_vocab)\n",
    "\n",
    "print('input dim: ', input_dim, 'output dim: ', output_dim)\n",
    "\n",
    "model = BILSTMForNER(input_dim=input_dim, embedding_dim=100, hidden_dim=256, dropout=0.33, output_dim=output_dim)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tag_vocab.get('<PAD>', -1))  \n",
    "# optimizer = optim.SGD(model.parameters(), lr = 0.00005)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "train_dataset = IndexedNERDataset('../../data/lstm-data/train', word_vocab, tag_vocab)\n",
    "dev_dataset = IndexedNERDataset('../../data/lstm-data/dev', word_vocab, tag_vocab)\n",
    "# test_dataset = IndexedNERDataset('../../data/lstm-data/test', word_vocab, tag_vocab)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size = 16, shuffle=True, collate_fn=pad_collate),\n",
    "    'val': DataLoader(dev_dataset, batch_size = 16, shuffle=False, collate_fn=pad_collate),\n",
    "    # 'test': DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate)\n",
    "}\n",
    "\n",
    "train_model(model, dataloaders, optimizer, criterion, device, num_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Using GloVe word embeddings\n",
    "Task is to use the GloVe word embeddings to improve the BLSTM\n",
    "in Task 1. The way we use the GloVe word embeddings is straight forward:\n",
    "we initialize the embeddings in our neural network with the corresponding\n",
    "vectors in GloVe. Note that GloVe is case-insensitive, but our NER model\n",
    "should be case-sensitive because capitalization is an important information\n",
    "for NER. You are asked to find a way to deal with this conflict. What are\n",
    "the precision, recall and F1 score on the dev data? (hint: the reasonable F1\n",
    "score on dev is 88%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path, word_vocab, embedding_dim):\n",
    "    embedding_dict = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]  \n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embedding_dict[word] = vector\n",
    "    \n",
    "    vocab_size = len(word_vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    for word, idx in word_vocab.items():\n",
    "        embedding_vector = embedding_dict.get(word, embedding_dict.get(word.lower()))\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.randn(embedding_dim) \n",
    "    \n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "\n",
    "class IndexedNERDataset(Dataset):\n",
    "    def __init__(self, file_path, word_vocab, tag_vocab=None, use_tags=True):\n",
    "        self.word_vocab = word_vocab\n",
    "        self.tag_vocab = tag_vocab\n",
    "        self.use_tags = use_tags  \n",
    "        self.data = []\n",
    "        self._load_data(file_path)\n",
    "        \n",
    "    def _load_data(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            sentence, tags = [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    parts = line.split()\n",
    "                    word = parts[1] if self.use_tags else parts[0]\n",
    "                    sentence.append(self.word_vocab.get(word, self.word_vocab['<UNK>']))\n",
    "                    if self.use_tags:\n",
    "                        tag = parts[2]\n",
    "                        tags.append(self.tag_vocab[tag])\n",
    "                else:\n",
    "                    if self.use_tags:\n",
    "                        self.data.append((sentence, tags))\n",
    "                    else:\n",
    "                        self.data.append((sentence,))  # Only add the sentence for test data\n",
    "                    sentence, tags = [], []\n",
    "            if sentence and (tags if self.use_tags else True):\n",
    "                if self.use_tags:\n",
    "                    self.data.append((sentence, tags))\n",
    "                else:\n",
    "                    self.data.append((sentence,))  # Only add the sentence for test data\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.use_tags:\n",
    "            sentence, tags = self.data[idx]\n",
    "            return torch.tensor(sentence, dtype=torch.long), torch.tensor(tags, dtype=torch.long)\n",
    "        else:\n",
    "            sentence = self.data[idx][0]\n",
    "            return torch.tensor(sentence, dtype=torch.long), None  # No tags for test data\n",
    "\n",
    "\n",
    "class BILSTMGlove(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, glove_embeddings = None, num_layers = 1, dropout = 0.33):\n",
    "        super(BILSTMGlove, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        if glove_embeddings is not None:\n",
    "            self.embedding.weight = nn.Parameter(glove_embeddings)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first = True, bidirectional = True, dropout = dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 128)\n",
    "        self.classifier = nn.Linear(128, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.elu(self.fc(lstm_out))  \n",
    "        logits = self.classifier(out)  \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = '../../data/lstm-data/glove.6B.100d/glove.6B.100d.txt' \n",
    "glove_embeddings = load_glove_embeddings(glove_path, word_vocab, embedding_dim = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amant\\anaconda3\\envs\\pytorch_dl\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.33 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indxs predicted:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [3, 1, 1, 1, 1, 1, 5, 8, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 3, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 4, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 5, 1, 3, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 2, 7, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 5, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [3, 4, 1, 5, 8, 1, 1, 3, 4, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Train - Loss: 0.0890, Acc: 0.9740, Precision: 0.9731, Recall: 0.9740, F1: 0.9730\n",
      "Val - Loss: 0.1152, Acc: 0.9685, Precision: 0.9674, Recall: 0.9685, F1: 0.9670\n",
      "Train - Loss: 0.0438, Acc: 0.9869, Precision: 0.9868, Recall: 0.9869, F1: 0.9867\n",
      "Val - Loss: 0.0809, Acc: 0.9770, Precision: 0.9766, Recall: 0.9770, F1: 0.9764\n",
      "indxs predicted:  tensor([[1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 0, 6, 6, 1, 0, 1, 1, 1, 3, 4, 1, 3, 4, 1, 1, 4, 1, 3, 4, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 1, 1, 6, 6, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [3, 1, 5, 1, 1, 3, 4, 1, 5, 1, 1, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "Train - Loss: 0.0262, Acc: 0.9919, Precision: 0.9918, Recall: 0.9919, F1: 0.9918\n",
      "Val - Loss: 0.0777, Acc: 0.9790, Precision: 0.9787, Recall: 0.9790, F1: 0.9785\n",
      "Train - Loss: 0.0162, Acc: 0.9950, Precision: 0.9950, Recall: 0.9950, F1: 0.9950\n",
      "Val - Loss: 0.0799, Acc: 0.9804, Precision: 0.9800, Recall: 0.9804, F1: 0.9799\n",
      "indxs predicted:  tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [5, 1, 1, 1, 1, 1, 3, 4, 1, 1, 1, 5, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 0, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 5, 8, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         1, 1, 1, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 7, 1, 1],\n",
      "        [0, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 0, 6, 6, 1, 1, 1, 1, 0, 6, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n",
      "Train - Loss: 0.0103, Acc: 0.9970, Precision: 0.9970, Recall: 0.9970, F1: 0.9970\n",
      "Val - Loss: 0.0790, Acc: 0.9806, Precision: 0.9804, Recall: 0.9806, F1: 0.9804\n"
     ]
    }
   ],
   "source": [
    "glove_model = BILSTMGlove(input_dim = input_dim, embedding_dim=100, hidden_dim = 256, glove_embeddings = glove_embeddings, dropout=0.33, output_dim = output_dim)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tag_vocab.get('<PAD>', -1))  \n",
    "optimizer = optim.Adam(glove_model.parameters(), lr = 0.001)\n",
    "\n",
    "train_model(glove_model, dataloaders, optimizer, criterion, device, num_epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_eval(model, file_paths, word_vocab, output_paths, device, idx_to_tag):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Loop over each file path and corresponding output path\n",
    "    for file_path, output_path in zip(file_paths, output_paths):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f, open(output_path, 'w', encoding='utf-8') as out_f:\n",
    "            sentences = []\n",
    "            current_sentence = []\n",
    "            for line in f:\n",
    "                if line.strip():  # Non-empty line\n",
    "                    parts = line.strip().split()\n",
    "                    word = parts[-1]  # Get the last element, whether it's word or tag\n",
    "                    current_sentence.append(word_vocab.get(word, word_vocab['<UNK>']))\n",
    "                elif current_sentence:  # Empty line and current sentence is not empty\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "\n",
    "            # Add the last sentence if the file doesn't end with a newline\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "\n",
    "            # Predict and write to file\n",
    "            for sentence in sentences:\n",
    "                sentence_tensor = torch.tensor([sentence], dtype=torch.long, device=device)\n",
    "                outputs = model(sentence_tensor)\n",
    "                _, preds = torch.max(outputs, dim=2)\n",
    "                pred_tags = [idx_to_tag[pred.item()] for pred in preds[0]]  # Convert indices to tags\n",
    "\n",
    "                # Write predictions to the output file\n",
    "                for i, word_idx in enumerate(sentence):\n",
    "                    word = list(word_vocab.keys())[list(word_vocab.values()).index(word_idx)]  # Inverse lookup\n",
    "                    tag = pred_tags[i]\n",
    "                    out_f.write(f\"{i+1}\\t{word}\\t{tag}\\n\")\n",
    "                out_f.write(\"\\n\")  # New line after each sentence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "idx_to_tag = {idx: tag for tag, idx in tag_vocab.items()}  # Create reverse mapping\n",
    "file_paths = ['../../data/lstm-data/test']#, '../../data/lstm-data/dev']\n",
    "output_paths = ['./test_preds']#,'./dev_preds']\n",
    "custom_eval(model, file_paths, word_vocab, output_paths, device, idx_to_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: LSTM-CNN model \n",
    "The bonus task is to equip the BLSTM model in Task 2 with a CNN module\n",
    "to capture character-level information (see slides page 45 in lecture 12 for the\n",
    "network architecture). The character embedding dimension is set to 30. You\n",
    "need to tune other hyper-parameters of CNN module, such as the number of\n",
    "CNN layers, the kernel size and output dimension of each CNN layer. What\n",
    "are the precision, recall and F1 score on the dev data? Predicting the NER\n",
    "tags of the sentences in the test data and output the predictions in a file\n",
    "named pred, in the same format of training data. (hint: the bonus points are\n",
    "assigned based on the ranking of your model F1 score on the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
