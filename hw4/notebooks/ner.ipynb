{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Tuple, List \n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Simple Bidirectional LSTM model (40\n",
    "points)\n",
    "The first task is to build a simple bidirectional LSTM model (see slides page\n",
    "43 in lecture 12 for the network architecture) for NER.\n",
    "Task. Implementing the bidirectional LSTM network with PyTorch. The\n",
    "architecture of the network is:\n",
    "Embedding →BLSTM →Linear →ELU →classifier\n",
    "The hyper-parameters of the network are listed in the following table:\n",
    "\n",
    "embedding dim 100\n",
    "number of LSTM layers 1\n",
    "LSTM hidden dim 256\n",
    "LSTM Dropout 0.33\n",
    "Linear output dim 128\n",
    "\n",
    "Train this simple BLSTM model with the training data on NER with SGD\n",
    "as the optimizer. Please tune other parameters that are not specified in the\n",
    "above table, such as batch size, learning rate and learning rate scheduling.\n",
    "What are the precision, recall and F1 score on the dev data? (hint: the\n",
    "reasonable F1 score on dev is 77%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BILSTM(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, is_bid, output_dim=1, num_layers=1, dropout=0.5):\n",
    "#         super(BinaryClassificationModelWithAttention, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.num_layers = num_layers\n",
    "#         self.is_bid = is_bid #### NEXT STEPS USE THIS\n",
    "#         self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first = True, bidirectional = is_bid, dropout = dropout if num_layers > 1 else 0)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#         if self.is_bid:\n",
    "#             self.attention = Attention(hidden_dim * 2)  \n",
    "#             self.fc = nn.Linear(hidden_dim * 2, output_dim)  \n",
    "#             self.sigmoid = nn.Sigmoid()\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         if self.is_bid:\n",
    "#             h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).to(x.device)  \n",
    "#             c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).to(x.device)  \n",
    "\n",
    "#         lstm_output, _ = self.lstm(x, (h0, c0))\n",
    "#         # context_vector, attention_weights = self.attention(lstm_output)\n",
    "        \n",
    "#         out = self.dropout(lstm_output)  # Apply dropout to the context vector\n",
    "#         out = self.fc(out)\n",
    "#         out = self.sigmoid(out)\n",
    "#         return out\n",
    "\n",
    "# # Citation https://www.kaggle.com/code/khalildmk/simple-two-layer-bidirectional-lstm-with-pytorch\n",
    "\n",
    "# class Bi_RNN(nn.Module):\n",
    "\n",
    "#     def __init__(self, input_dim, hidden_dim, batch_size, output_dim, num_layers=1, rnn_type='LSTM'):\n",
    "#         super(Bi_RNN, self).__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.dropout = nn.Dropout(0.33) \n",
    "#         self.init_linear = nn.Linear(self.input_dim, self.input_dim)\n",
    "\n",
    "#         # Define the LSTM layer\n",
    "#         self.lstm = eval('nn.' + rnn_type)(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "#         self.linear = nn.Linear(self.hidden_dim * 2, output_dim)\n",
    "\n",
    "#     def init_hidden(self):\n",
    "#         return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "#                 torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         linear_input = f.relu(self.init_linear(input))\n",
    "\n",
    "#         # Forward pass through LSTM layer\n",
    "#         # shape of lstm_out: [batch_size, input_size ,hidden_dim]\n",
    "#         # shape of self.hidden: (a, b), where a and b both\n",
    "#         # have shape (batch_size, num_layers, hidden_dim).\n",
    "#         lstm_out, self.hidden = self.lstm(linear_input)\n",
    "        \n",
    "\n",
    "#         # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "#         y_pred = self.linear(lstm_out)\n",
    "#         return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTMForNER(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, num_layers=1, dropout=0.33):\n",
    "        super(BILSTMForNER, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 128)  # Linear output dim is 128\n",
    "        self.classifier = nn.Linear(128, output_dim)  # Adjusted to match the number of unique NER tags\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))  # Apply dropout to embeddings\n",
    "        lstm_output, _ = self.lstm(embedded)\n",
    "        out = self.fc(lstm_output)\n",
    "        out = F.elu(out)  \n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BILSTMForNER = BILSTMForNER(input_dim = 100, hidden_dim = 256, embedding_dim = 1024, output_dim = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NERDataset(Dataset):\n",
    "#     def __init__(self, file_path):\n",
    "#         self.examples = []\n",
    "#         self._load_data(file_path)\n",
    "\n",
    "#     def _load_data(self, file_path):\n",
    "#         with open(file_path, encoding='utf-8') as f:\n",
    "#             current_sentence = []\n",
    "#             for line in f:\n",
    "#                 line = line.strip()\n",
    "#                 if line: \n",
    "#                     _, word, tag = line.split()\n",
    "#                     current_sentence.append((word, tag))\n",
    "#                 else: \n",
    "#                     if current_sentence:  # Avoid adding empty sentences\n",
    "#                         self.examples.append(current_sentence)\n",
    "#                         current_sentence = []\n",
    "#             # Add the last sentence if file doesn't end with a newline\n",
    "#             if current_sentence:\n",
    "#                 self.examples.append(current_sentence)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.examples)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         words, tags = zip(*self.examples[idx])\n",
    "#         return {\"tokens\": words, \"tags\": tags}\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     \"\"\"\n",
    "#     A custom collate function to pad the sequences to the same length in each batch.\n",
    "#     \"\"\"\n",
    "#     max_length = max(len(item[\"tokens\"]) for item in batch)\n",
    "#     padded_tokens, padded_tags = [], []\n",
    "#     for item in batch:\n",
    "#         padding_length = max_length - len(item[\"tokens\"])\n",
    "#         padded_tokens.append(list(item[\"tokens\"]) + ['<PAD>']*padding_length)\n",
    "#         padded_tags.append(list(item[\"tags\"]) + ['O']*padding_length)  # Assuming 'O' is the padding tag\n",
    "#     return {\"tokens\": torch.tensor(padded_tokens), \"tags\": torch.tensor(padded_tags)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def task1_collate(data: list[dict], use_targets: bool) -> dict[str, torch.Tensor]:\n",
    "#     word_encodings = [torch.Tensor(x['word_encodings']) for x in data]\n",
    "#     word_encodings = pad_sequence(word_encodings, batch_first=True)\n",
    "\n",
    "#     lengths = torch.IntTensor([len(x['word_encodings']) for x in data])\n",
    "\n",
    "#     # capital_mask = [torch.Tensor(x['capital_mask']) for x in data]\n",
    "#     # capital_mask = pad_sequence(capital_mask, batch_first = True)\n",
    "\n",
    "#     item: dict[str, torch.Tensor] = {\n",
    "#         'word_encodings': word_encodings,\n",
    "#         'lengths': lengths,\n",
    "#     }\n",
    "\n",
    "#     if use_targets: \n",
    "#         tag_encodings = [torch.Tensor(x['tag_encodings']) for x in data] # type: ignore\n",
    "#         tag_encodings = pad_sequence(tag_encodings, batch_first=True)\n",
    "#         item |= {'tag_encodings': tag_encodings}\n",
    "    \n",
    "#     return item\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, optimizer, criterion, device, num_epochs=50, patience=10):\n",
    "    \"\"\"\n",
    "    Trains and validates the model.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The PyTorch model to train.\n",
    "    - dataloaders (dict): A dictionary containing 'train' and 'val' DataLoaders.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer to use for training.\n",
    "    - criterion (torch.nn.Module): The loss function.\n",
    "    - num_epochs (int): The number of epochs to train for.\n",
    "    - patience (int): The patience for early stopping.\n",
    "    \"\"\"\n",
    "    best_val_f1 = -float('inf')  \n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    best_epoch = -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_preds, train_labels, val_preds, val_labels = [], [], [], []\n",
    "\n",
    "        model.train()\n",
    "        for batch in dataloaders['train']:\n",
    "\n",
    "            inputs, labels, _ = batch['tokens'].to(device), batch['tags'].to(device).unsqueeze(-1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            t_preds = (outputs > pred_threshold).long() \n",
    "\n",
    "            # print('outputs: ', list(outputs.detach().cpu().numpy()),'labels: ', list(labels.detach().cpu().numpy()) )\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate training acc.\n",
    "            train_preds.extend(t_preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "        train_acc = np.mean(np.array(train_preds) == np.array(train_labels))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloaders['val']:\n",
    "                inputs, labels = batch['tokens'].to(device), batch['tags'].to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                preds = (outputs > pred_threshold).long() \n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "        val_accuracy = np.mean(np.array(val_preds) == np.array(val_labels))\n",
    "        val_f1 = f1_score(val_labels, val_preds)\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1, best_epoch = val_f1, epoch \n",
    "            patience_counter = 0\n",
    "            # print(f\"Validation F1 improved. Saving model...\")\n",
    "            torch.save(model.state_dict(), f'saved_models/epoch:{epoch}_valacc:{val_accuracy}_checkpoint.pth')\n",
    "        else:\n",
    "            patience_counter += 1 \n",
    "            # print(f'Validation F1 did not improve. Patience: {patience_counter}/{patience}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break  \n",
    "\n",
    "\n",
    "    # print(f'Validation Accuracy: {val_accuracy:.4f}. Training Accuracy: {train_acc:.4f}')\n",
    "    return val_accuracy,train_acc, best_val_f1, np.array(train_preds), np.array(train_labels), np.array(val_preds), np.array(val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedNERDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        super().__init__()\n",
    "        self.sentences = []\n",
    "        self.tags = []\n",
    "        self._load_data(file_path)\n",
    "        \n",
    "    def _load_data(self, file_path):\n",
    "        with open(file_path, encoding='utf-8') as f:\n",
    "            sentence, tag_sequence = [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    idx, word_idx, tag_idx = line.split()\n",
    "                    sentence.append(word_idx)\n",
    "                    tag_sequence.append(tag_idx)\n",
    "                else:  # End of sentence\n",
    "                    self.sentences.append(sentence)\n",
    "                    self.tags.append(tag_sequence)\n",
    "                    sentence, tag_sequence = [], []\n",
    "            # Adding last sentence if file doesn't end with a newline\n",
    "            if sentence and tag_sequence:\n",
    "                self.sentences.append(sentence)\n",
    "                self.tags.append(tag_sequence)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For dynamic padding, return sentences and tags as they are\n",
    "        print(self.sentences[idx])\n",
    "        sentence = torch.tensor(self.sentences[idx], dtype=torch.long)\n",
    "        tag_sequence = torch.tensor(self.tags[idx], dtype=torch.long)\n",
    "        return sentence, tag_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    \n",
    "    # Pad sequences to the longest sequence in the batch\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)  # Assuming 0 is the padding index for tokens\n",
    "    yy_pad = pad_sequence(yy, batch_first=True, padding_value=-1) # Assuming -1 is the padding index for tags\n",
    "\n",
    "    # You can also compute a mask here, indicating padded areas\n",
    "    mask = torch.tensor([[float(i != -1) for i in seq] for seq in yy_pad], dtype=torch.float)\n",
    "\n",
    "    return xx_pad, yy_pad, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup hyperparams for training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IndexedNERDataset('../../data/lstm-data/train')\n",
    "dev_dataset = IndexedNERDataset('../../data/lstm-data/dev')\n",
    "\n",
    "dataloaders = {}\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle=True, collate_fn = collate_fn)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size = 32, shuffle=False, collate_fn = collate_fn)\n",
    "dataloaders['train'], dataloaders['val'] = train_loader, dev_loader\n",
    "\n",
    "optimizer = optim.Adam(BILSTMForNER.parameters(),lr = 0.001 )\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '-', 'Jana', 'Novotna', '(', 'Czech', 'Republic', ')', 'beat', 'Sandrine', 'Testud', '(', 'France', ')']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m val_accuracy, train_acc, best_val_f1, train_preds, train_labels, val_preds, val_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBILSTMForNER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 23\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloaders, optimizer, criterion, device, num_epochs, patience)\u001b[0m\n\u001b[0;32m     20\u001b[0m train_preds, train_labels, val_preds, val_labels \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 23\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\envs\\pytorch_dl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\envs\\pytorch_dl\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\amant\\anaconda3\\envs\\pytorch_dl\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[33], line 32\u001b[0m, in \u001b[0;36mIndexedNERDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# For dynamic padding, return sentences and tags as they are\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences[idx])\n\u001b[1;32m---> 32\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     tag_sequence \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags[idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentence, tag_sequence\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "val_accuracy, train_acc, best_val_f1, train_preds, train_labels, val_preds, val_labels = train_model(BILSTMForNER, dataloaders, optimizer, criterion, device,  num_epochs=50, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Using GloVe word embeddings\n",
    "Task is to use the GloVe word embeddings to improve the BLSTM\n",
    "in Task 1. The way we use the GloVe word embeddings is straight forward:\n",
    "we initialize the embeddings in our neural network with the corresponding\n",
    "vectors in GloVe. Note that GloVe is case-insensitive, but our NER model\n",
    "should be case-sensitive because capitalization is an important information\n",
    "for NER. You are asked to find a way to deal with this conflict. What are\n",
    "the precision, recall and F1 score on the dev data? (hint: the reasonable F1\n",
    "score on dev is 88%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: LSTM-CNN model \n",
    "The bonus task is to equip the BLSTM model in Task 2 with a CNN module\n",
    "to capture character-level information (see slides page 45 in lecture 12 for the\n",
    "network architecture). The character embedding dimension is set to 30. You\n",
    "need to tune other hyper-parameters of CNN module, such as the number of\n",
    "CNN layers, the kernel size and output dimension of each CNN layer. What\n",
    "are the precision, recall and F1 score on the dev data? Predicting the NER\n",
    "tags of the sentences in the test data and output the predictions in a file\n",
    "named pred, in the same format of training data. (hint: the bonus points are\n",
    "assigned based on the ranking of your model F1 score on the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
